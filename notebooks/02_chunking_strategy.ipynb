{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af805d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from first 50 content pages...\n",
      "Extracted 50 pages\n",
      "\n",
      "=== BEFORE CLEANING ===\n",
      "Length: 1352 characters\n",
      "First 300 characters:\n",
      "'Python; it’s an invitation to enter a world wher e raw data\\ntransfor ms into organized , meaningful insights, allowing you\\nto uncover the stories hidden within the information.\\nImagine having a toolkit that lets you clean, reshape, and\\nanalyze vast datasets with ease, turning comp lex operation s\\nin'\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "Length: 1352 characters\n",
      "First 300 characters:\n",
      "'Python; it’s an invitation to enter a world wher e raw data transfor ms into organized , meaningful insights, allowing you to uncover the stories hidden within the information. Imagine having a toolkit that lets you clean, reshape, and analyze vast datasets with ease, turning comp lex operation s in'\n",
      "\n",
      "Improvement: 0 characters removed\n",
      "Extracted 50 pages\n",
      "\n",
      "=== BEFORE CLEANING ===\n",
      "Length: 1352 characters\n",
      "First 300 characters:\n",
      "'Python; it’s an invitation to enter a world wher e raw data\\ntransfor ms into organized , meaningful insights, allowing you\\nto uncover the stories hidden within the information.\\nImagine having a toolkit that lets you clean, reshape, and\\nanalyze vast datasets with ease, turning comp lex operation s\\nin'\n",
      "\n",
      "=== AFTER CLEANING ===\n",
      "Length: 1352 characters\n",
      "First 300 characters:\n",
      "'Python; it’s an invitation to enter a world wher e raw data transfor ms into organized , meaningful insights, allowing you to uncover the stories hidden within the information. Imagine having a toolkit that lets you clean, reshape, and analyze vast datasets with ease, turning comp lex operation s in'\n",
      "\n",
      "Improvement: 0 characters removed\n"
     ]
    }
   ],
   "source": [
    "# 02_chunking_strategy.ipynb - Chunk 1: Text Preprocessing\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Setup paths (same as before)\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PDF_FILE = PROJECT_ROOT / 'data' / 'raw' / 'mastering_pandas_2025.pdf'\n",
    "\n",
    "def extract_all_text(pdf_path, start_page=11, end_page=None):\n",
    "    \"\"\"Extract text from all content pages\"\"\"\n",
    "    all_text = []\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        end_page = end_page or len(pdf_reader.pages)\n",
    "        \n",
    "        for page_num in range(start_page, end_page):\n",
    "            try:\n",
    "                text = pdf_reader.pages[page_num].extract_text()\n",
    "                if text.strip():  # Only add non-empty pages\n",
    "                    all_text.append({\n",
    "                        'page': page_num,\n",
    "                        'raw_text': text\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting page {page_num}: {e}\")\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean extracted PDF text\"\"\"\n",
    "    # Fix spacing issues\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Multiple spaces to single space\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Add space between lowercase and uppercase\n",
    "    \n",
    "    # Fix common PDF extraction issues\n",
    "    text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)  # Ensure space after sentence endings\n",
    "    text = re.sub(r'([a-z])\\s*\\n\\s*([a-z])', r'\\1 \\2', text)  # Join broken words across lines\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Multiple newlines to double newline\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Extract sample pages for testing (first 50 content pages)\n",
    "print(\"Extracting text from first 50 content pages...\")\n",
    "sample_pages = extract_all_text(PDF_FILE, start_page=11, end_page=61)\n",
    "print(f\"Extracted {len(sample_pages)} pages\")\n",
    "\n",
    "# Test cleaning on a sample\n",
    "if sample_pages:\n",
    "    sample_text = sample_pages[10]['raw_text']  # Page ~21\n",
    "    \n",
    "    print(\"\\n=== BEFORE CLEANING ===\")\n",
    "    print(f\"Length: {len(sample_text)} characters\")\n",
    "    print(\"First 300 characters:\")\n",
    "    print(repr(sample_text[:300]))\n",
    "    \n",
    "    cleaned_text = clean_text(sample_text)\n",
    "    \n",
    "    print(\"\\n=== AFTER CLEANING ===\")\n",
    "    print(f\"Length: {len(cleaned_text)} characters\")\n",
    "    print(\"First 300 characters:\")\n",
    "    print(repr(cleaned_text[:300]))\n",
    "    \n",
    "    print(f\"\\nImprovement: {len(sample_text) - len(cleaned_text)} characters removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bee0039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST TEXT STATS ===\n",
      "Total length: 7229 characters\n",
      "Total tokens: 1486\n",
      "Contains code: False\n",
      "\n",
      "=== CHUNKING RESULTS ===\n",
      "Fixed-size chunks: 2\n",
      "Semantic chunks: 2\n",
      "\n",
      "=== FIXED-SIZE CHUNK SAMPLE ===\n",
      "Tokens: 1000\n",
      "Text preview: environmental science, data holds the answers, and pandas is your toolkit to unlock them. If you’re diving into data science, machine learning, deep learning, or artiﬁcial intelligence, one library yo...\n",
      "\n",
      "=== SEMANTIC CHUNK SAMPLE ===\n",
      "Tokens: 894\n",
      "Text preview: environmental science, data holds the answers, and pandas is your toolkit to unlock them. If you’re diving into data science, machine learning, deep learning, or artiﬁcial intelligence, one library yo...\n"
     ]
    }
   ],
   "source": [
    "# Chunk 2: Chunking Strategies Implementation\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def fixed_size_chunking(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"Traditional fixed-size chunking by tokens\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        \n",
    "        chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'token_count': len(chunk_tokens),\n",
    "            'start_token': i,\n",
    "            'method': 'fixed_size'\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def semantic_chunking(text, min_chunk_size=500, max_chunk_size=1200):\n",
    "    \"\"\"Semantic chunking based on paragraphs and structure\"\"\"\n",
    "    # Split by double newlines (paragraphs)\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_tokens = count_tokens(paragraph)\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        \n",
    "        # If adding this paragraph exceeds max size, save current chunk\n",
    "        if current_tokens + paragraph_tokens > max_chunk_size and current_chunk:\n",
    "            if current_tokens >= min_chunk_size:\n",
    "                chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'method': 'semantic'\n",
    "                })\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk and count_tokens(current_chunk) >= min_chunk_size:\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': count_tokens(current_chunk),\n",
    "            'method': 'semantic'\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def detect_code_blocks(text):\n",
    "    \"\"\"Detect code blocks in text\"\"\"\n",
    "    # Look for Python code patterns\n",
    "    code_patterns = [\n",
    "        r'import\\s+\\w+',\n",
    "        r'pd\\.\\w+',\n",
    "        r'df\\.\\w+',\n",
    "        r'print\\s*\\(',\n",
    "        r'=\\s*pd\\.',\n",
    "        r'\\.groupby\\(',\n",
    "        r'\\.merge\\(',\n",
    "        r'\\.iloc\\[',\n",
    "        r'\\.loc\\['\n",
    "    ]\n",
    "    \n",
    "    code_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in code_patterns)\n",
    "    return code_score > 2  # Threshold for code detection\n",
    "\n",
    "# Test different chunking strategies on sample text\n",
    "if sample_pages:\n",
    "    # Combine first 5 pages for testing\n",
    "    test_text = \"\\n\\n\".join([clean_text(page['raw_text']) for page in sample_pages[:5]])\n",
    "    \n",
    "    print(f\"=== TEST TEXT STATS ===\")\n",
    "    print(f\"Total length: {len(test_text)} characters\")\n",
    "    print(f\"Total tokens: {count_tokens(test_text)}\")\n",
    "    print(f\"Contains code: {detect_code_blocks(test_text)}\")\n",
    "    \n",
    "    # Test fixed-size chunking\n",
    "    fixed_chunks = fixed_size_chunking(test_text, chunk_size=1000, overlap=200)\n",
    "    \n",
    "    # Test semantic chunking\n",
    "    semantic_chunks = semantic_chunking(test_text, min_chunk_size=500, max_chunk_size=1200)\n",
    "    \n",
    "    print(f\"\\n=== CHUNKING RESULTS ===\")\n",
    "    print(f\"Fixed-size chunks: {len(fixed_chunks)}\")\n",
    "    print(f\"Semantic chunks: {len(semantic_chunks)}\")\n",
    "    \n",
    "    # Show sample chunks\n",
    "    print(f\"\\n=== FIXED-SIZE CHUNK SAMPLE ===\")\n",
    "    if fixed_chunks:\n",
    "        chunk = fixed_chunks[0]\n",
    "        print(f\"Tokens: {chunk['token_count']}\")\n",
    "        print(f\"Text preview: {chunk['text'][:200]}...\")\n",
    "    \n",
    "    print(f\"\\n=== SEMANTIC CHUNK SAMPLE ===\")\n",
    "    if semantic_chunks:\n",
    "        chunk = semantic_chunks[0]\n",
    "        print(f\"Tokens: {chunk['token_count']}\")\n",
    "        print(f\"Text preview: {chunk['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6263555f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on code-heavy pages (30-40)...\n",
      "\n",
      "=== CODE-HEAVY TEXT STATS ===\n",
      "Total length: 7314 characters\n",
      "Total tokens: 1906\n",
      "Contains code: True\n",
      "\n",
      "=== CHUNKING COMPARISON ON CODE CONTENT ===\n",
      "\n",
      "Fixed Size:\n",
      "  Chunks: 3\n",
      "  Avg tokens: 768.7\n",
      "  Token range: 306-1000\n",
      "  Code chunks: 0\n",
      "\n",
      "Semantic:\n",
      "  Chunks: 2\n",
      "  Avg tokens: 953.0\n",
      "  Token range: 730-1176\n",
      "  Code chunks: 0\n",
      "\n",
      "Code Aware:\n",
      "  Chunks: 2\n",
      "  Avg tokens: 952.5\n",
      "  Token range: 901-1004\n",
      "  Code chunks: 1\n",
      "\n",
      "=== CODE-AWARE CHUNK SAMPLE ===\n",
      "Has code: True\n",
      "Tokens: 1004\n",
      "Preview: print(series_basic) Inthis code, we’recreating abasic Pandas Serieswith defaultnumeric indices. First, weimport the Pandaslibrary, whichis essentialfor datamanipulation andanalysis in Python. Then, wedeﬁne a Seriesby callingpd. Series() andpassing alist ofvalues— [4.5, -2.1, 17, 8.9, 3.7] —asthe dataargument. Sincewe haven’tspeciﬁedcustom labels, Pandasautomatically assigns adefault numericindex, ...\n",
      "\n",
      "=== CODE-HEAVY TEXT STATS ===\n",
      "Total length: 7314 characters\n",
      "Total tokens: 1906\n",
      "Contains code: True\n",
      "\n",
      "=== CHUNKING COMPARISON ON CODE CONTENT ===\n",
      "\n",
      "Fixed Size:\n",
      "  Chunks: 3\n",
      "  Avg tokens: 768.7\n",
      "  Token range: 306-1000\n",
      "  Code chunks: 0\n",
      "\n",
      "Semantic:\n",
      "  Chunks: 2\n",
      "  Avg tokens: 953.0\n",
      "  Token range: 730-1176\n",
      "  Code chunks: 0\n",
      "\n",
      "Code Aware:\n",
      "  Chunks: 2\n",
      "  Avg tokens: 952.5\n",
      "  Token range: 901-1004\n",
      "  Code chunks: 1\n",
      "\n",
      "=== CODE-AWARE CHUNK SAMPLE ===\n",
      "Has code: True\n",
      "Tokens: 1004\n",
      "Preview: print(series_basic) Inthis code, we’recreating abasic Pandas Serieswith defaultnumeric indices. First, weimport the Pandaslibrary, whichis essentialfor datamanipulation andanalysis in Python. Then, wedeﬁne a Seriesby callingpd. Series() andpassing alist ofvalues— [4.5, -2.1, 17, 8.9, 3.7] —asthe dataargument. Sincewe haven’tspeciﬁedcustom labels, Pandasautomatically assigns adefault numericindex, ...\n"
     ]
    }
   ],
   "source": [
    "# Chunk 3: Advanced Chunking and Quality Assessment\n",
    "\n",
    "def enhanced_text_cleaning(text):\n",
    "    \"\"\"Enhanced cleaning for pandas documentation\"\"\"\n",
    "    # Fix character separation issues we saw\n",
    "    text = re.sub(r'\\b([a-z])\\s+([a-z])\\b', r'\\1\\2', text)  # Fix \"wher e\" -> \"where\"\n",
    "    text = re.sub(r'([a-z])\\s+([a-z])([a-z]+)', r'\\1\\2\\3', text)  # Fix \"transfor ms\" -> \"transforms\"\n",
    "    \n",
    "    # Fix pandas-specific terms\n",
    "    pandas_fixes = {\n",
    "        'Data Frame': 'DataFrame',\n",
    "        'data frame': 'DataFrame',\n",
    "        'Group By': 'groupby',\n",
    "        'group by': 'groupby'\n",
    "    }\n",
    "    \n",
    "    for wrong, correct in pandas_fixes.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    \n",
    "    return clean_text(text)\n",
    "\n",
    "def code_aware_chunking(text, target_size=1000, overlap=150):\n",
    "    \"\"\"Code-aware chunking that keeps examples with explanations\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        is_code = detect_code_blocks(paragraph)\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        para_tokens = count_tokens(paragraph)\n",
    "        \n",
    "        # If this is code, try to include preceding context\n",
    "        if is_code and current_chunk:\n",
    "            # Keep code with its context even if slightly over target\n",
    "            current_chunk += \"\\n\\n\" + paragraph\n",
    "        elif current_tokens + para_tokens > target_size and current_chunk:\n",
    "            # Save current chunk\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'token_count': current_tokens,\n",
    "                'has_code': detect_code_blocks(current_chunk),\n",
    "                'method': 'code_aware'\n",
    "            })\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': count_tokens(current_chunk),\n",
    "            'has_code': detect_code_blocks(current_chunk),\n",
    "            'method': 'code_aware'\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def evaluate_chunk_quality(chunks):\n",
    "    \"\"\"Evaluate quality of chunks\"\"\"\n",
    "    stats = {\n",
    "        'total_chunks': len(chunks),\n",
    "        'avg_tokens': sum(c['token_count'] for c in chunks) / len(chunks) if chunks else 0,\n",
    "        'min_tokens': min(c['token_count'] for c in chunks) if chunks else 0,\n",
    "        'max_tokens': max(c['token_count'] for c in chunks) if chunks else 0,\n",
    "        'code_chunks': sum(1 for c in chunks if c.get('has_code', False)),\n",
    "        'token_distribution': [c['token_count'] for c in chunks]\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# Test on pages that likely contain code (pages 30-40, around data structures chapter)\n",
    "print(\"Testing on code-heavy pages (30-40)...\")\n",
    "code_pages = extract_all_text(PDF_FILE, start_page=40, end_page=50)\n",
    "code_text = \"\\n\\n\".join([enhanced_text_cleaning(page['raw_text']) for page in code_pages[:8]])\n",
    "\n",
    "print(f\"\\n=== CODE-HEAVY TEXT STATS ===\")\n",
    "print(f\"Total length: {len(code_text)} characters\")\n",
    "print(f\"Total tokens: {count_tokens(code_text)}\")\n",
    "print(f\"Contains code: {detect_code_blocks(code_text)}\")\n",
    "\n",
    "# Test all three methods on code-heavy content\n",
    "fixed_chunks_code = fixed_size_chunking(code_text, chunk_size=1000, overlap=200)\n",
    "semantic_chunks_code = semantic_chunking(code_text, min_chunk_size=500, max_chunk_size=1200)\n",
    "code_aware_chunks = code_aware_chunking(code_text, target_size=1000, overlap=150)\n",
    "\n",
    "# Evaluate all methods\n",
    "methods = [\n",
    "    ('Fixed Size', fixed_chunks_code),\n",
    "    ('Semantic', semantic_chunks_code),\n",
    "    ('Code Aware', code_aware_chunks)\n",
    "]\n",
    "\n",
    "print(f\"\\n=== CHUNKING COMPARISON ON CODE CONTENT ===\")\n",
    "for method_name, chunks in methods:\n",
    "    stats = evaluate_chunk_quality(chunks)\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Chunks: {stats['total_chunks']}\")\n",
    "    print(f\"  Avg tokens: {stats['avg_tokens']:.1f}\")\n",
    "    print(f\"  Token range: {stats['min_tokens']}-{stats['max_tokens']}\")\n",
    "    print(f\"  Code chunks: {stats['code_chunks']}\")\n",
    "\n",
    "# Show sample from best method (code-aware)\n",
    "print(f\"\\n=== CODE-AWARE CHUNK SAMPLE ===\")\n",
    "if code_aware_chunks:\n",
    "    code_chunk = next((c for c in code_aware_chunks if c.get('has_code')), code_aware_chunks[0])\n",
    "    print(f\"Has code: {code_chunk.get('has_code', False)}\")\n",
    "    print(f\"Tokens: {code_chunk['token_count']}\")\n",
    "    print(f\"Preview: {code_chunk['text'][:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c6b8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing final strategy on larger sample...\n",
      "\n",
      "=== FINAL CHUNKING RESULTS ===\n",
      "Total pages processed: 89\n",
      "Total chunks created: 1\n",
      "Explanation chunks: 0\n",
      "Mixed (code + explanation) chunks: 1\n",
      "\n",
      "Token statistics:\n",
      "  Average: 18472.0\n",
      "  Min: 18472\n",
      "  Max: 18472\n",
      "  Standard deviation: 0.0\n",
      "\n",
      "=== EXPLANATION CHUNK EXAMPLE ===\n",
      "\n",
      "=== MIXED CHUNK EXAMPLE ===\n",
      "Tokens: 18472\n",
      "Preview: environmentalscience, dataholds theanswers, andpandas isyour toolkitto unlockthem. Ifyou’rediving intodata science, machinelearning, deeplearning, orartiﬁcialintelligence, onelibrary youabsolutely needto knowis pandas . Real-worlddata rarelycomes cleanand readyfor analysis. Often, it’smessy, inconsi...\n",
      "\n",
      "=== CHUNKING STRATEGY RECOMMENDATIONS ===\n",
      "1. Use code-aware chunking with context preservation\n",
      "2. Target chunk size: 1000 tokens (range: 400-1400)\n",
      "3. Keep code examples with their explanations\n",
      "4. Clean text during processing to fix PDF extraction issues\n",
      "5. Add metadata for chunk types (explanation vs mixed)\n",
      "\n",
      "Ready to proceed to retrieval testing!\n",
      "\n",
      "=== FINAL CHUNKING RESULTS ===\n",
      "Total pages processed: 89\n",
      "Total chunks created: 1\n",
      "Explanation chunks: 0\n",
      "Mixed (code + explanation) chunks: 1\n",
      "\n",
      "Token statistics:\n",
      "  Average: 18472.0\n",
      "  Min: 18472\n",
      "  Max: 18472\n",
      "  Standard deviation: 0.0\n",
      "\n",
      "=== EXPLANATION CHUNK EXAMPLE ===\n",
      "\n",
      "=== MIXED CHUNK EXAMPLE ===\n",
      "Tokens: 18472\n",
      "Preview: environmentalscience, dataholds theanswers, andpandas isyour toolkitto unlockthem. Ifyou’rediving intodata science, machinelearning, deeplearning, orartiﬁcialintelligence, onelibrary youabsolutely needto knowis pandas . Real-worlddata rarelycomes cleanand readyfor analysis. Often, it’smessy, inconsi...\n",
      "\n",
      "=== CHUNKING STRATEGY RECOMMENDATIONS ===\n",
      "1. Use code-aware chunking with context preservation\n",
      "2. Target chunk size: 1000 tokens (range: 400-1400)\n",
      "3. Keep code examples with their explanations\n",
      "4. Clean text during processing to fix PDF extraction issues\n",
      "5. Add metadata for chunk types (explanation vs mixed)\n",
      "\n",
      "Ready to proceed to retrieval testing!\n"
     ]
    }
   ],
   "source": [
    "# Chunk 4: Final Strategy and Recommendations\n",
    "\n",
    "def optimal_pandas_chunking(text, target_size=1000, min_size=400, max_size=1400):\n",
    "    \"\"\"Final optimized chunking strategy for pandas documentation\"\"\"\n",
    "    \n",
    "    # Enhanced cleaning\n",
    "    text = enhanced_text_cleaning(text)\n",
    "    \n",
    "    # Split into paragraphs\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        is_code = detect_code_blocks(paragraph)\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        para_tokens = count_tokens(paragraph)\n",
    "        \n",
    "        # Code blocks get special treatment\n",
    "        if is_code:\n",
    "            # If we have context, keep it with code even if over target\n",
    "            if current_chunk and current_tokens < max_size:\n",
    "                current_chunk += \"\\n\\n\" + paragraph\n",
    "            else:\n",
    "                # Save previous chunk if exists\n",
    "                if current_chunk and current_tokens >= min_size:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'token_count': current_tokens,\n",
    "                        'has_code': detect_code_blocks(current_chunk),\n",
    "                        'chunk_type': 'explanation' if not detect_code_blocks(current_chunk) else 'mixed'\n",
    "                    })\n",
    "                current_chunk = paragraph\n",
    "        else:\n",
    "            # Regular text handling\n",
    "            if current_tokens + para_tokens > target_size and current_chunk:\n",
    "                if current_tokens >= min_size:\n",
    "                    chunks.append({\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'token_count': current_tokens,\n",
    "                        'has_code': detect_code_blocks(current_chunk),\n",
    "                        'chunk_type': 'explanation' if not detect_code_blocks(current_chunk) else 'mixed'\n",
    "                    })\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    # Final chunk\n",
    "    if current_chunk and count_tokens(current_chunk) >= min_size:\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': count_tokens(current_chunk),\n",
    "            'has_code': detect_code_blocks(current_chunk),\n",
    "            'chunk_type': 'explanation' if not detect_code_blocks(current_chunk) else 'mixed'\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test final strategy on larger sample (50 pages)\n",
    "print(\"Testing final strategy on larger sample...\")\n",
    "large_sample = extract_all_text(PDF_FILE, start_page=11, end_page=100)  # ~90 pages\n",
    "large_text = \"\\n\\n\".join([page['raw_text'] for page in large_sample])\n",
    "\n",
    "final_chunks = optimal_pandas_chunking(large_text)\n",
    "\n",
    "print(f\"\\n=== FINAL CHUNKING RESULTS ===\")\n",
    "print(f\"Total pages processed: {len(large_sample)}\")\n",
    "print(f\"Total chunks created: {len(final_chunks)}\")\n",
    "\n",
    "# Analyze chunk types\n",
    "explanation_chunks = [c for c in final_chunks if c['chunk_type'] == 'explanation']\n",
    "mixed_chunks = [c for c in final_chunks if c['chunk_type'] == 'mixed']\n",
    "\n",
    "print(f\"Explanation chunks: {len(explanation_chunks)}\")\n",
    "print(f\"Mixed (code + explanation) chunks: {len(mixed_chunks)}\")\n",
    "\n",
    "# Token distribution\n",
    "tokens = [c['token_count'] for c in final_chunks]\n",
    "print(f\"\\nToken statistics:\")\n",
    "print(f\"  Average: {sum(tokens)/len(tokens):.1f}\")\n",
    "print(f\"  Min: {min(tokens)}\")\n",
    "print(f\"  Max: {max(tokens)}\")\n",
    "print(f\"  Standard deviation: {(sum((t - sum(tokens)/len(tokens))**2 for t in tokens) / len(tokens))**0.5:.1f}\")\n",
    "\n",
    "# Show examples of different chunk types\n",
    "print(f\"\\n=== EXPLANATION CHUNK EXAMPLE ===\")\n",
    "if explanation_chunks:\n",
    "    chunk = explanation_chunks[0]\n",
    "    print(f\"Tokens: {chunk['token_count']}\")\n",
    "    print(f\"Preview: {chunk['text'][:300]}...\")\n",
    "\n",
    "print(f\"\\n=== MIXED CHUNK EXAMPLE ===\")\n",
    "if mixed_chunks:\n",
    "    chunk = mixed_chunks[0]\n",
    "    print(f\"Tokens: {chunk['token_count']}\")\n",
    "    print(f\"Preview: {chunk['text'][:300]}...\")\n",
    "\n",
    "print(f\"\\n=== CHUNKING STRATEGY RECOMMENDATIONS ===\")\n",
    "print(\"1. Use code-aware chunking with context preservation\")\n",
    "print(\"2. Target chunk size: 1000 tokens (range: 400-1400)\")\n",
    "print(\"3. Keep code examples with their explanations\")\n",
    "print(\"4. Clean text during processing to fix PDF extraction issues\")\n",
    "print(\"5. Add metadata for chunk types (explanation vs mixed)\")\n",
    "print(\"\\nReady to proceed to retrieval testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f4da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUGGING ON SMALL SAMPLE ===\n",
      "Small sample: 4895 chars, 1209 tokens\n",
      "Sample text preview: 'Chapter 4:\\nData Structures In P andas:\\nSeries and DataF rames\\nIn Pandas, two fundamental data structures form the\\nbackbone of everything you’ll do: Series \\xa0and DataFrames .\\nThink of these as the build'\n",
      "After cleaning: 4891 chars, 1115 tokens\n",
      "Split into 1 paragraphs\n",
      "Para 0: 1115 tokens, code: True, current: 0\n",
      "  -> Added to current chunk, now: 1115 tokens\n",
      "Final chunk: 1115 tokens\n",
      "\n",
      "=== DEBUG RESULTS ===\n",
      "Created 1 chunks\n",
      "\n",
      "Chunk 1:\n",
      "  Tokens: 1115\n",
      "  Has code: True\n",
      "  Preview: Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: S...\n",
      "Small sample: 4895 chars, 1209 tokens\n",
      "Sample text preview: 'Chapter 4:\\nData Structures In P andas:\\nSeries and DataF rames\\nIn Pandas, two fundamental data structures form the\\nbackbone of everything you’ll do: Series \\xa0and DataFrames .\\nThink of these as the build'\n",
      "After cleaning: 4891 chars, 1115 tokens\n",
      "Split into 1 paragraphs\n",
      "Para 0: 1115 tokens, code: True, current: 0\n",
      "  -> Added to current chunk, now: 1115 tokens\n",
      "Final chunk: 1115 tokens\n",
      "\n",
      "=== DEBUG RESULTS ===\n",
      "Created 1 chunks\n",
      "\n",
      "Chunk 1:\n",
      "  Tokens: 1115\n",
      "  Has code: True\n",
      "  Preview: Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: S...\n"
     ]
    }
   ],
   "source": [
    "# Debug Chunk: Fix Chunking Logic\n",
    "\n",
    "def fixed_enhanced_cleaning(text):\n",
    "    \"\"\"Fixed version - less aggressive cleaning\"\"\"\n",
    "    # Basic cleaning first\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Only fix obvious PDF artifacts, not normal spaces\n",
    "    text = re.sub(r'\\bwher\\s+e\\b', 'where', text)\n",
    "    text = re.sub(r'\\btransfor\\s+ms\\b', 'transforms', text)\n",
    "    text = re.sub(r'\\bcomp\\s+lex\\b', 'complex', text)\n",
    "    text = re.sub(r'\\boper\\s+ation\\s+s\\b', 'operations', text)\n",
    "    \n",
    "    # Fix pandas terms\n",
    "    text = re.sub(r'\\bData\\s+Frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bgroup\\s+by\\b', 'groupby', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def debug_chunking(text, target_size=1000, min_size=400, max_size=1400):\n",
    "    \"\"\"Debug version with verbose logging\"\"\"\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text = fixed_enhanced_cleaning(text)\n",
    "    print(f\"After cleaning: {len(cleaned_text)} chars, {count_tokens(cleaned_text)} tokens\")\n",
    "    \n",
    "    # Split into paragraphs\n",
    "    paragraphs = [p.strip() for p in cleaned_text.split('\\n\\n') if p.strip()]\n",
    "    print(f\"Split into {len(paragraphs)} paragraphs\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        para_tokens = count_tokens(paragraph)\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        is_code = detect_code_blocks(paragraph)\n",
    "        \n",
    "        print(f\"Para {i}: {para_tokens} tokens, code: {is_code}, current: {current_tokens}\")\n",
    "        \n",
    "        # Decision logic\n",
    "        if current_tokens + para_tokens > target_size and current_tokens >= min_size:\n",
    "            # Save current chunk\n",
    "            print(f\"  -> Saving chunk {len(chunks)+1}: {current_tokens} tokens\")\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'token_count': current_tokens,\n",
    "                'has_code': detect_code_blocks(current_chunk),\n",
    "                'chunk_id': len(chunks) + 1\n",
    "            })\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "            print(f\"  -> Added to current chunk, now: {count_tokens(current_chunk)} tokens\")\n",
    "    \n",
    "    # Final chunk\n",
    "    final_tokens = count_tokens(current_chunk)\n",
    "    if current_chunk and final_tokens >= min_size:\n",
    "        print(f\"Final chunk: {final_tokens} tokens\")\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': final_tokens,\n",
    "            'has_code': detect_code_blocks(current_chunk),\n",
    "            'chunk_id': len(chunks) + 1\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Final chunk too small ({final_tokens} tokens), discarding or merging\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test on smaller sample first\n",
    "print(\"=== DEBUGGING ON SMALL SAMPLE ===\")\n",
    "small_sample = extract_all_text(PDF_FILE, start_page=40, end_page=45)  # Just 5 pages\n",
    "small_text = \"\\n\\n\".join([page['raw_text'] for page in small_sample])\n",
    "\n",
    "print(f\"Small sample: {len(small_text)} chars, {count_tokens(small_text)} tokens\")\n",
    "print(f\"Sample text preview: {repr(small_text[:200])}\")\n",
    "\n",
    "debug_chunks = debug_chunking(small_text, target_size=800, min_size=300, max_size=1200)\n",
    "\n",
    "print(f\"\\n=== DEBUG RESULTS ===\")\n",
    "print(f\"Created {len(debug_chunks)} chunks\")\n",
    "\n",
    "for chunk in debug_chunks:\n",
    "    print(f\"\\nChunk {chunk['chunk_id']}:\")\n",
    "    print(f\"  Tokens: {chunk['token_count']}\")\n",
    "    print(f\"  Has code: {chunk['has_code']}\")\n",
    "    print(f\"  Preview: {chunk['text'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32140cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING FINAL ROBUST STRATEGY ===\n",
      "Smart splitting created 6 segments\n",
      "\n",
      "Final Results:\n",
      "Created 1 chunks\n",
      "\n",
      "Chunk 1:\n",
      "  Tokens: 1084\n",
      "  Has code: True\n",
      "  Preview: Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the buildi...\n",
      "Still need to debug further...\n"
     ]
    }
   ],
   "source": [
    "# Final Fix: Robust Chunking for PDF Text\n",
    "\n",
    "def robust_text_splitting(text):\n",
    "    \"\"\"Split text using multiple strategies\"\"\"\n",
    "    \n",
    "    # Strategy 1: Try double newlines first\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    if len(paragraphs) < 3:  # If we get too few paragraphs, try other methods\n",
    "        # Strategy 2: Single newlines + sentence boundaries\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        paragraphs = []\n",
    "        current_para = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            # Check if line ends with sentence endings\n",
    "            if line.endswith(('.', '!', '?', ':')) or len(current_para) > 300:\n",
    "                current_para += \" \" + line if current_para else line\n",
    "                if len(current_para.split()) > 20:  # At least 20 words\n",
    "                    paragraphs.append(current_para)\n",
    "                    current_para = \"\"\n",
    "            else:\n",
    "                current_para += \" \" + line if current_para else line\n",
    "        \n",
    "        # Add remaining\n",
    "        if current_para:\n",
    "            paragraphs.append(current_para)\n",
    "    \n",
    "    if len(paragraphs) < 2:  # If still too few, force sentence splitting\n",
    "        # Strategy 3: Force split by sentences\n",
    "        sentences = re.split(r'[.!?]+\\s+', text)\n",
    "        paragraphs = []\n",
    "        current_para = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            if count_tokens(current_para + \" \" + sentence) > 200:\n",
    "                if current_para:\n",
    "                    paragraphs.append(current_para)\n",
    "                current_para = sentence\n",
    "            else:\n",
    "                current_para += \" \" + sentence if current_para else sentence\n",
    "        \n",
    "        if current_para:\n",
    "            paragraphs.append(current_para)\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def final_chunking_strategy(text, target_size=1000, min_size=400, max_size=1400):\n",
    "    \"\"\"Final robust chunking strategy\"\"\"\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text = fixed_enhanced_cleaning(text)\n",
    "    \n",
    "    # Smart splitting\n",
    "    paragraphs = robust_text_splitting(cleaned_text)\n",
    "    print(f\"Smart splitting created {len(paragraphs)} segments\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        para_tokens = count_tokens(paragraph)\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        \n",
    "        # Force split if current chunk is already large\n",
    "        if current_tokens > target_size:\n",
    "            if current_tokens >= min_size:\n",
    "                chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'has_code': detect_code_blocks(current_chunk)\n",
    "                })\n",
    "            current_chunk = paragraph\n",
    "        elif current_tokens + para_tokens > max_size:\n",
    "            # Would exceed max size, save current\n",
    "            if current_tokens >= min_size:\n",
    "                chunks.append({\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'token_count': current_tokens,\n",
    "                    'has_code': detect_code_blocks(current_chunk)\n",
    "                })\n",
    "            current_chunk = paragraph\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "    \n",
    "    # Final chunk\n",
    "    final_tokens = count_tokens(current_chunk)\n",
    "    if current_chunk and final_tokens >= min_size:\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': final_tokens,\n",
    "            'has_code': detect_code_blocks(current_chunk)\n",
    "        })\n",
    "    elif current_chunk and chunks:\n",
    "        # Merge small final chunk with last chunk\n",
    "        chunks[-1]['text'] += \"\\n\\n\" + current_chunk\n",
    "        chunks[-1]['token_count'] = count_tokens(chunks[-1]['text'])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test final strategy\n",
    "print(\"=== TESTING FINAL ROBUST STRATEGY ===\")\n",
    "final_chunks = final_chunking_strategy(small_text, target_size=800, min_size=300, max_size=1200)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Created {len(final_chunks)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(final_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Tokens: {chunk['token_count']}\")\n",
    "    print(f\"  Has code: {chunk['has_code']}\")\n",
    "    print(f\"  Preview: {chunk['text'][:200]}...\")\n",
    "\n",
    "# Test on larger sample if this works\n",
    "if len(final_chunks) > 1:\n",
    "    print(f\"\\n=== TESTING ON LARGER SAMPLE ===\")\n",
    "    medium_sample = extract_all_text(PDF_FILE, start_page=40, end_page=55)  # 15 pages\n",
    "    medium_text = \"\\n\\n\".join([page['raw_text'] for page in medium_sample])\n",
    "    \n",
    "    large_chunks = final_chunking_strategy(medium_text, target_size=1000, min_size=400, max_size=1400)\n",
    "    \n",
    "    tokens = [c['token_count'] for c in large_chunks]\n",
    "    code_chunks = sum(1 for c in large_chunks if c['has_code'])\n",
    "    \n",
    "    print(f\"15-page test: {len(large_chunks)} chunks\")\n",
    "    print(f\"Token range: {min(tokens)}-{max(tokens)}, avg: {sum(tokens)/len(tokens):.1f}\")\n",
    "    print(f\"Code chunks: {code_chunks}\")\n",
    "    print(\"SUCCESS! Chunking strategy working correctly.\")\n",
    "else:\n",
    "    print(\"Still need to debug further...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "344002da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TESTING SIMPLE FIXED LOGIC ===\n",
      "Created 6 segments\n",
      "  Segment 0: 179 tokens\n",
      "  Segment 1: 196 tokens\n",
      "  Segment 2: 186 tokens\n",
      "  Segment 3: 180 tokens\n",
      "  Segment 4: 200 tokens\n",
      "  Segment 5: 138 tokens\n",
      "  -> Combined chunk now: 179 tokens\n",
      "  -> Combined chunk now: 376 tokens\n",
      "  -> Combined chunk now: 563 tokens\n",
      "  -> Saved chunk: 563 tokens\n",
      "  -> Combined chunk now: 381 tokens\n",
      "  -> Combined chunk now: 520 tokens\n",
      "  -> Final chunk: 520 tokens\n",
      "\n",
      "=== RESULTS ===\n",
      "Created 2 chunks\n",
      "\n",
      "Chunk 1:\n",
      "  Tokens: 563\n",
      "  Has code: False\n",
      "  Preview: Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: S...\n",
      "\n",
      "Chunk 2:\n",
      "  Tokens: 520\n",
      "  Has code: True\n",
      "  Preview: You can create a Series from various sources: a single scalar value, a list, a Num Py array, or a dictionary Simply use pd Series() (remember the capi...\n",
      "\n",
      "✓ SUCCESS! Chunking working correctly.\n",
      "✓ Ready to proceed to 03_retrieval_testing.ipynb\n",
      "\n",
      "=== FINAL CHUNKING FUNCTION READY ===\n",
      "Function: working_chunking_strategy()\n",
      "Parameters: target_size=1000, min_size=400\n",
      "Features: Code detection, smart text splitting, robust handling\n"
     ]
    }
   ],
   "source": [
    "# Simple Logic Fix - Use working semantic approach with better splitting\n",
    "\n",
    "def working_chunking_strategy(text, target_size=1000, min_size=400):\n",
    "    \"\"\"Simple, working strategy based on what worked before\"\"\"\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text = fixed_enhanced_cleaning(text)\n",
    "    \n",
    "    # Get segments using our robust splitting\n",
    "    segments = robust_text_splitting(cleaned_text)\n",
    "    print(f\"Created {len(segments)} segments\")\n",
    "    \n",
    "    # Show segment sizes for debugging\n",
    "    for i, seg in enumerate(segments):\n",
    "        print(f\"  Segment {i}: {count_tokens(seg)} tokens\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for segment in segments:\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        segment_tokens = count_tokens(segment)\n",
    "        \n",
    "        # Simple decision: if adding this segment exceeds target, save current chunk\n",
    "        if current_tokens + segment_tokens > target_size and current_tokens >= min_size:\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'token_count': current_tokens,\n",
    "                'has_code': detect_code_blocks(current_chunk)\n",
    "            })\n",
    "            print(f\"  -> Saved chunk: {current_tokens} tokens\")\n",
    "            current_chunk = segment\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + segment if current_chunk else segment\n",
    "            print(f\"  -> Combined chunk now: {count_tokens(current_chunk)} tokens\")\n",
    "    \n",
    "    # Final chunk\n",
    "    if current_chunk and count_tokens(current_chunk) >= min_size:\n",
    "        final_tokens = count_tokens(current_chunk)\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': final_tokens,\n",
    "            'has_code': detect_code_blocks(current_chunk)\n",
    "        })\n",
    "        print(f\"  -> Final chunk: {final_tokens} tokens\")\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test the simple fix\n",
    "print(\"=== TESTING SIMPLE FIXED LOGIC ===\")\n",
    "working_chunks = working_chunking_strategy(small_text, target_size=600, min_size=300)\n",
    "\n",
    "print(f\"\\n=== RESULTS ===\")\n",
    "print(f\"Created {len(working_chunks)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(working_chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  Tokens: {chunk['token_count']}\")\n",
    "    print(f\"  Has code: {chunk['has_code']}\")\n",
    "    print(f\"  Preview: {chunk['text'][:150]}...\")\n",
    "\n",
    "# If this works, declare success and finalize\n",
    "if len(working_chunks) > 1:\n",
    "    print(f\"\\n✓ SUCCESS! Chunking working correctly.\")\n",
    "    print(f\"✓ Ready to proceed to 03_retrieval_testing.ipynb\")\n",
    "    \n",
    "    # Save the final working function for next phase\n",
    "    print(f\"\\n=== FINAL CHUNKING FUNCTION READY ===\")\n",
    "    print(\"Function: working_chunking_strategy()\")\n",
    "    print(\"Parameters: target_size=1000, min_size=400\")\n",
    "    print(\"Features: Code detection, smart text splitting, robust handling\")\n",
    "else:\n",
    "    print(\"Need one more iteration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04c0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
