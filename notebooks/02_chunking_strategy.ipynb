{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b9c31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded content analysis:\n",
      "Total analyzed pages: 99\n",
      "High-value pages: 74\n",
      "Extracting content from valuable pages...\n",
      "Processing 76 valuable pages\n",
      "Successfully extracted 76 pages\n",
      "Applying content-aware chunking to create larger, more contextual chunks...\n",
      "Processing 21 pages of conceptual content...\n",
      "Processing 7 pages of code_heavy content...\n",
      "Processing 48 pages of general content...\n",
      "\n",
      "Chunking Results:\n",
      "Total chunks created: 13\n",
      "Average tokens per chunk: 1160.5\n",
      "\n",
      "Chunk Analysis:\n",
      "Token distribution:\n",
      "  Min: 424\n",
      "  Max: 1456\n",
      "  Mean: 1160.5\n",
      "  Std: 270.3\n",
      "\n",
      "Content type distribution:\n",
      "content_type\n",
      "general       9\n",
      "conceptual    3\n",
      "code_heavy    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Page grouping analysis:\n",
      "  Single-page chunks: 0\n",
      "  Multi-page chunks: 13\n",
      "  Average pages per chunk: 5.8\n",
      "  Max pages per chunk: 7\n",
      "\n",
      "Chunk Analysis:\n",
      "Token distribution:\n",
      "  Min: 424\n",
      "  Max: 1456\n",
      "  Mean: 1160.5\n",
      "  Std: 270.3\n",
      "\n",
      "Content type distribution:\n",
      "content_type\n",
      "general       9\n",
      "conceptual    3\n",
      "code_heavy    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Chunks with code features: 2\n",
      "Chunks with concept features: 8\n",
      "\n",
      "Quality Assessment:\n",
      "High-quality chunks: 10 (76.9%)\n",
      "\n",
      "Sample Chunks:\n",
      "\n",
      "CONCEPTUAL Chunk (pages 8-52):\n",
      "  Tokens: 1447\n",
      "  Code Score: 0\n",
      "  Concept Score: 18\n",
      "  Preview: PREF ACE Welcome to Mastering Pandas: A Comprehensive Guide to Data Analysis in Python , a journey into the heart of modern data science. This book is not just a guide; it’s your gateway to the world ...\n",
      "\n",
      "CODE_HEAVY Chunk (pages 36-196):\n",
      "  Tokens: 1456\n",
      "  Code Score: 25\n",
      "  Concept Score: 43\n",
      "  Preview: \"Getting started with Pandas is straightforwar d, and you can set it up easily in both your ter minal or Jupyter Notebook. First, to install Pandas, open your terminal or, if you’re using Jupyter Note...\n",
      "\n",
      "GENERAL Chunk (pages 80-120):\n",
      "  Tokens: 1047\n",
      "  Code Score: 5\n",
      "  Concept Score: 21\n",
      "  Preview: # Selecting data using .iloc Example 1: Selecting a Single Row by Position print(\"Selecting a single row by position:\\n\", df.iloc[1]) In this example, the .iloc method is used to select a single row b...\n",
      "\n",
      "Saved 13 chunks to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\processed_chunks.pkl\n",
      "\n",
      "Summary Statistics:\n",
      "  total_chunks: 13\n",
      "  total_pages_processed: 76\n",
      "  avg_tokens_per_chunk: 1160.4615384615386\n",
      "  content_type_distribution: {'general': 9, 'conceptual': 3, 'code_heavy': 1}\n",
      "  high_quality_chunks: 10\n",
      "  chunks_with_code: 2\n",
      "  chunks_with_concepts: 8\n"
     ]
    }
   ],
   "source": [
    "# 02_chunking_strategy.ipynb - Adaptive Content Processing\n",
    "\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PDF_FILE = PROJECT_ROOT / 'data' / 'raw' / 'mastering_pandas_2025.pdf'\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load content analysis from previous notebook\n",
    "content_analysis = pd.read_csv(PROCESSED_DIR / 'content_analysis.csv')\n",
    "print(\"Loaded content analysis:\")\n",
    "print(f\"Total analyzed pages: {len(content_analysis)}\")\n",
    "print(f\"High-value pages: {len(content_analysis[content_analysis['pandas_score'] > 0])}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens accurately\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def clean_pdf_text(text):\n",
    "    \"\"\"Clean PDF extraction artifacts\"\"\"\n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    \n",
    "    # Fix common PDF artifacts\n",
    "    text = re.sub(r'\\bwher\\s+e\\b', 'where', text)\n",
    "    text = re.sub(r'\\btransfor\\s+ms\\b', 'transforms', text)\n",
    "    text = re.sub(r'\\bData\\s+Frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bdata\\s+frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bGroup\\s+By\\b', 'GroupBy', text)\n",
    "    text = re.sub(r'\\bgroup\\s+by\\b', 'groupby', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Sentence boundaries\n",
    "    text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def detect_content_features(text):\n",
    "    \"\"\"Analyze content characteristics for adaptive chunking\"\"\"\n",
    "    \n",
    "    # Code detection patterns\n",
    "    code_patterns = [\n",
    "        r'import\\s+\\w+', r'from\\s+\\w+\\s+import', r'pd\\.\\w+', r'df\\.\\w+', \n",
    "        r'print\\s*\\(', r'=\\s*pd\\.', r'\\.groupby\\(', r'\\.merge\\(',\n",
    "        r'\\.iloc\\[', r'\\.loc\\[', r'def\\s+\\w+', r'class\\s+\\w+'\n",
    "    ]\n",
    "    code_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in code_patterns)\n",
    "    \n",
    "    # Pandas concept detection\n",
    "    pandas_concepts = [\n",
    "        'DataFrame', 'Series', 'Index', 'groupby', 'merge', 'concat',\n",
    "        'pivot', 'melt', 'apply', 'lambda', 'iloc', 'loc', 'query'\n",
    "    ]\n",
    "    concept_score = sum(text.lower().count(concept.lower()) for concept in pandas_concepts)\n",
    "    \n",
    "    # Structural markers\n",
    "    has_headers = bool(re.search(r'^[A-Z][^.!?]*:?\\s*$', text, re.MULTILINE))\n",
    "    has_code_blocks = bool(re.search(r'```|>>>|\\n\\s*\\w+\\s*=', text))\n",
    "    has_examples = bool(re.search(r'example|Example|for instance|For instance', text, re.IGNORECASE))\n",
    "    \n",
    "    return {\n",
    "        'code_score': code_score,\n",
    "        'concept_score': concept_score,\n",
    "        'has_headers': has_headers,\n",
    "        'has_code_blocks': has_code_blocks,\n",
    "        'has_examples': has_examples,\n",
    "        'is_code_heavy': code_score > 3,\n",
    "        'is_concept_heavy': concept_score > 5\n",
    "    }\n",
    "\n",
    "def smart_text_segmentation(text):\n",
    "    \"\"\"Conservative text segmentation to preserve context\"\"\"\n",
    "    \n",
    "    # Try paragraph-based splitting first, but be more conservative\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip() and len(p.strip()) > 50]\n",
    "    \n",
    "    if len(paragraphs) >= 2:\n",
    "        # Group small paragraphs together\n",
    "        segments = []\n",
    "        current_segment = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            test_segment = current_segment + \"\\n\\n\" + paragraph if current_segment else paragraph\n",
    "            \n",
    "            # Only split if segment would be very large\n",
    "            if count_tokens(test_segment) > 800 and current_segment:\n",
    "                segments.append(current_segment.strip())\n",
    "                current_segment = paragraph\n",
    "            else:\n",
    "                current_segment = test_segment\n",
    "        \n",
    "        if current_segment:\n",
    "            segments.append(current_segment.strip())\n",
    "        \n",
    "        return segments if len(segments) > 1 else [text]\n",
    "    \n",
    "    # If few paragraphs, try sentence-based but with larger groups\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    \n",
    "    if len(sentences) >= 3:\n",
    "        segments = []\n",
    "        current_segment = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "                \n",
    "            test_segment = current_segment + \" \" + sentence if current_segment else sentence\n",
    "            \n",
    "            # Only split at much larger boundaries\n",
    "            if count_tokens(test_segment) > 600 and current_segment:\n",
    "                segments.append(current_segment.strip())\n",
    "                current_segment = sentence\n",
    "            else:\n",
    "                current_segment = test_segment\n",
    "        \n",
    "        if current_segment:\n",
    "            segments.append(current_segment.strip())\n",
    "        \n",
    "        return segments if len(segments) > 1 else [text]\n",
    "    \n",
    "    # Return as single segment to preserve context\n",
    "    return [text]\n",
    "\n",
    "def group_based_chunking(extracted_pages, target_tokens=1000, min_tokens=500, max_tokens=1500):\n",
    "    \"\"\"Create larger chunks by combining content from multiple pages\"\"\"\n",
    "    \n",
    "    all_chunks = []\n",
    "    current_chunk_content = []\n",
    "    current_chunk_tokens = 0\n",
    "    current_chunk_pages = []\n",
    "    current_content_types = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    for page_data in extracted_pages:\n",
    "        # Clean the page text\n",
    "        cleaned_text = clean_pdf_text(page_data['text'])\n",
    "        page_tokens = count_tokens(cleaned_text)\n",
    "        \n",
    "        # Skip very short pages\n",
    "        if page_tokens < 50:\n",
    "            continue\n",
    "            \n",
    "        # Check if adding this page would exceed max tokens\n",
    "        if current_chunk_tokens + page_tokens > max_tokens and current_chunk_content:\n",
    "            # Create chunk from accumulated content\n",
    "            if current_chunk_tokens >= min_tokens:\n",
    "                chunk_text = \"\\n\\n\".join(current_chunk_content)\n",
    "                features = detect_content_features(chunk_text)\n",
    "                \n",
    "                # Determine dominant content type\n",
    "                type_counts = {}\n",
    "                for ct in current_content_types:\n",
    "                    type_counts[ct] = type_counts.get(ct, 0) + 1\n",
    "                dominant_type = max(type_counts.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "                all_chunks.append({\n",
    "                    'text': chunk_text,\n",
    "                    'token_count': current_chunk_tokens,\n",
    "                    'content_type': dominant_type,\n",
    "                    'chunk_index': len(all_chunks),\n",
    "                    'global_chunk_id': chunk_id,\n",
    "                    'source_pages': current_chunk_pages.copy(),\n",
    "                    'page_count': len(current_chunk_pages),\n",
    "                    'features': features\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            \n",
    "            # Start new chunk\n",
    "            current_chunk_content = [cleaned_text]\n",
    "            current_chunk_tokens = page_tokens\n",
    "            current_chunk_pages = [page_data['page_num']]\n",
    "            current_content_types = [page_data['content_type']]\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk_content.append(cleaned_text)\n",
    "            current_chunk_tokens += page_tokens\n",
    "            current_chunk_pages.append(page_data['page_num'])\n",
    "            current_content_types.append(page_data['content_type'])\n",
    "    \n",
    "    # Handle final chunk\n",
    "    if current_chunk_content and current_chunk_tokens >= min_tokens:\n",
    "        chunk_text = \"\\n\\n\".join(current_chunk_content)\n",
    "        features = detect_content_features(chunk_text)\n",
    "        \n",
    "        type_counts = {}\n",
    "        for ct in current_content_types:\n",
    "            type_counts[ct] = type_counts.get(ct, 0) + 1\n",
    "        dominant_type = max(type_counts.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        all_chunks.append({\n",
    "            'text': chunk_text,\n",
    "            'token_count': current_chunk_tokens,\n",
    "            'content_type': dominant_type,\n",
    "            'chunk_index': len(all_chunks),\n",
    "            'global_chunk_id': chunk_id,\n",
    "            'source_pages': current_chunk_pages.copy(),\n",
    "            'page_count': len(current_chunk_pages),\n",
    "            'features': features\n",
    "        })\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def content_aware_chunking(extracted_pages, target_tokens=1000, min_tokens=500, max_tokens=1500):\n",
    "    \"\"\"Advanced chunking that preserves content coherence\"\"\"\n",
    "    \n",
    "    # Group pages by content type for better coherence\n",
    "    content_groups = {}\n",
    "    for page_data in extracted_pages:\n",
    "        content_type = page_data['content_type']\n",
    "        if content_type not in content_groups:\n",
    "            content_groups[content_type] = []\n",
    "        content_groups[content_type].append(page_data)\n",
    "    \n",
    "    all_chunks = []\n",
    "    \n",
    "    # Process each content type separately\n",
    "    for content_type, pages in content_groups.items():\n",
    "        print(f\"Processing {len(pages)} pages of {content_type} content...\")\n",
    "        \n",
    "        # Adjust parameters based on content type\n",
    "        if content_type == 'code_heavy':\n",
    "            type_target = 1200\n",
    "            type_min = 600\n",
    "            type_max = 1800\n",
    "        elif content_type == 'conceptual':\n",
    "            type_target = 1000\n",
    "            type_min = 500\n",
    "            type_max = 1500\n",
    "        else:  # general\n",
    "            type_target = 800\n",
    "            type_min = 400\n",
    "            type_max = 1200\n",
    "        \n",
    "        # Create chunks for this content type\n",
    "        type_chunks = group_based_chunking(pages, type_target, type_min, type_max)\n",
    "        all_chunks.extend(type_chunks)\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "def extract_valuable_content(pdf_path, content_analysis_df):\n",
    "    \"\"\"Extract content from high-value pages identified in analysis\"\"\"\n",
    "    \n",
    "    # Get pages worth processing\n",
    "    valuable_pages = content_analysis_df[\n",
    "        (content_analysis_df['content_type'].isin(['conceptual', 'code_heavy', 'general'])) &\n",
    "        (content_analysis_df['char_count'] > 500)\n",
    "    ]['page'].tolist()\n",
    "    \n",
    "    print(f\"Processing {len(valuable_pages)} valuable pages\")\n",
    "    \n",
    "    extracted_content = []\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        for page_num in valuable_pages:\n",
    "            try:\n",
    "                text = pdf_reader.pages[page_num].extract_text()\n",
    "                \n",
    "                if text.strip() and len(text) > 300:  # Substantial content only\n",
    "                    # Get content type from analysis\n",
    "                    page_info = content_analysis_df[content_analysis_df['page'] == page_num].iloc[0]\n",
    "                    \n",
    "                    extracted_content.append({\n",
    "                        'page_num': page_num,\n",
    "                        'text': text,\n",
    "                        'content_type': page_info['content_type'],\n",
    "                        'pandas_score': page_info['pandas_score'],\n",
    "                        'code_score': page_info['code_score']\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting page {page_num}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully extracted {len(extracted_content)} pages\")\n",
    "    return extracted_content\n",
    "\n",
    "# Extract content from valuable pages\n",
    "print(\"Extracting content from valuable pages...\")\n",
    "extracted_pages = extract_valuable_content(PDF_FILE, content_analysis)\n",
    "\n",
    "# Process extracted content with content-aware chunking\n",
    "print(\"Applying content-aware chunking to create larger, more contextual chunks...\")\n",
    "all_chunks = content_aware_chunking(extracted_pages, target_tokens=1000, min_tokens=500, max_tokens=1500)\n",
    "\n",
    "print(f\"\\nChunking Results:\")\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "if len(all_chunks) > 0:\n",
    "    print(f\"Average tokens per chunk: {np.mean([c['token_count'] for c in all_chunks]):.1f}\")\n",
    "    \n",
    "    # Analyze chunk distribution\n",
    "    chunk_df = pd.DataFrame(all_chunks)\n",
    "    \n",
    "    print(f\"\\nChunk Analysis:\")\n",
    "    print(f\"Token distribution:\")\n",
    "    print(f\"  Min: {chunk_df['token_count'].min()}\")\n",
    "    print(f\"  Max: {chunk_df['token_count'].max()}\")\n",
    "    print(f\"  Mean: {chunk_df['token_count'].mean():.1f}\")\n",
    "    print(f\"  Std: {chunk_df['token_count'].std():.1f}\")\n",
    "    \n",
    "    print(f\"\\nContent type distribution:\")\n",
    "    print(chunk_df['content_type'].value_counts())\n",
    "    \n",
    "    print(f\"\\nPage grouping analysis:\")\n",
    "    print(f\"  Single-page chunks: {sum(1 for c in all_chunks if c['page_count'] == 1)}\")\n",
    "    print(f\"  Multi-page chunks: {sum(1 for c in all_chunks if c['page_count'] > 1)}\")\n",
    "    print(f\"  Average pages per chunk: {np.mean([c['page_count'] for c in all_chunks]):.1f}\")\n",
    "    print(f\"  Max pages per chunk: {max(c['page_count'] for c in all_chunks)}\")\n",
    "else:\n",
    "    print(\"ERROR: No chunks created\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "if len(all_chunks) > 0:\n",
    "    chunk_df = pd.DataFrame(all_chunks)\n",
    "    \n",
    "    print(f\"\\nChunk Analysis:\")\n",
    "    print(f\"Token distribution:\")\n",
    "    print(f\"  Min: {chunk_df['token_count'].min()}\")\n",
    "    print(f\"  Max: {chunk_df['token_count'].max()}\")\n",
    "    print(f\"  Mean: {chunk_df['token_count'].mean():.1f}\")\n",
    "    print(f\"  Std: {chunk_df['token_count'].std():.1f}\")\n",
    "    \n",
    "    print(f\"\\nContent type distribution:\")\n",
    "    print(chunk_df['content_type'].value_counts())\n",
    "else:\n",
    "    print(\"No chunks available for analysis\")\n",
    "\n",
    "if len(all_chunks) > 0:\n",
    "    print(f\"\\nChunks with code features: {sum(1 for c in all_chunks if c['features']['is_code_heavy'])}\")\n",
    "    print(f\"Chunks with concept features: {sum(1 for c in all_chunks if c['features']['is_concept_heavy'])}\")\n",
    "\n",
    "    # Quality assessment\n",
    "    high_quality_chunks = [\n",
    "        c for c in all_chunks \n",
    "        if c['token_count'] >= 500 and \n",
    "        (c['features']['concept_score'] > 2 or c['features']['code_score'] > 1)\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nQuality Assessment:\")\n",
    "    print(f\"High-quality chunks: {len(high_quality_chunks)} ({len(high_quality_chunks)/len(all_chunks)*100:.1f}%)\")\n",
    "\n",
    "    # Sample chunks for inspection\n",
    "    print(f\"\\nSample Chunks:\")\n",
    "    for content_type in ['conceptual', 'code_heavy', 'general']:\n",
    "        sample_chunks = [c for c in all_chunks if c['content_type'] == content_type][:1]\n",
    "        \n",
    "        for chunk in sample_chunks:\n",
    "            page_range = f\"pages {min(chunk['source_pages'])}-{max(chunk['source_pages'])}\" if len(chunk['source_pages']) > 1 else f\"page {chunk['source_pages'][0]}\"\n",
    "            print(f\"\\n{content_type.upper()} Chunk ({page_range}):\")\n",
    "            print(f\"  Tokens: {chunk['token_count']}\")\n",
    "            print(f\"  Code Score: {chunk['features']['code_score']}\")\n",
    "            print(f\"  Concept Score: {chunk['features']['concept_score']}\")\n",
    "            print(f\"  Preview: {chunk['text'][:200]}...\")\n",
    "\n",
    "    # Save processed chunks\n",
    "    chunks_file = PROCESSED_DIR / 'processed_chunks.pkl'\n",
    "    with open(chunks_file, 'wb') as f:\n",
    "        pickle.dump(all_chunks, f)\n",
    "\n",
    "    print(f\"\\nSaved {len(all_chunks)} chunks to: {chunks_file}\")\n",
    "\n",
    "    # Create summary statistics\n",
    "    summary_stats = {\n",
    "        'total_chunks': len(all_chunks),\n",
    "        'total_pages_processed': len(extracted_pages),\n",
    "        'avg_tokens_per_chunk': float(np.mean([c['token_count'] for c in all_chunks])),\n",
    "        'content_type_distribution': chunk_df['content_type'].value_counts().to_dict(),\n",
    "        'high_quality_chunks': len(high_quality_chunks),\n",
    "        'chunks_with_code': sum(1 for c in all_chunks if c['features']['is_code_heavy']),\n",
    "        'chunks_with_concepts': sum(1 for c in all_chunks if c['features']['is_concept_heavy'])\n",
    "    }\n",
    "\n",
    "    print(f\"\\nSummary Statistics:\")\n",
    "    for key, value in summary_stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"\\nERROR: No chunks created. Need to debug chunking logic before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca02b4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Chunks:\n",
      "\n",
      "CONCEPTUAL Chunk:\n",
      "  Tokens: 1447\n",
      "  Pages: 7 (pages 8-52)\n",
      "  Code Score: 0\n",
      "  Concept Score: 18\n",
      "  Preview: PREF ACE Welcome to Mastering Pandas: A Comprehensive Guide to Data Analysis in Python , a journey into the heart of modern data science. This book is not just a guide; it’s your gateway to the world of data exploration, where powerful insights lie hidden within raw numbers and text. Here, Pandas tr...\n",
      "\n",
      "CODE_HEAVY Chunk:\n",
      "  Tokens: 1456\n",
      "  Pages: 7 (pages 36-196)\n",
      "  Code Score: 25\n",
      "  Concept Score: 43\n",
      "  Preview: \"Getting started with Pandas is straightforwar d, and you can set it up easily in both your ter minal or Jupyter Notebook. First, to install Pandas, open your terminal or, if you’re using Jupyter Notebook, simply run this command in a cell: pip install pandas # for terminal !pip install pandas # for...\n",
      "\n",
      "GENERAL Chunk:\n",
      "  Tokens: 1047\n",
      "  Pages: 5 (pages 80-120)\n",
      "  Code Score: 5\n",
      "  Concept Score: 21\n",
      "  Preview: # Selecting data using .iloc Example 1: Selecting a Single Row by Position print(\"Selecting a single row by position:\\n\", df.iloc[1]) In this example, the .iloc method is used to select a single row based on its numerical position within the DataFrame, speciﬁcally the row located at position 1 (whic...\n",
      "\n",
      "Chunking Results Summary:\n",
      "  Total chunks: 13\n",
      "  Average tokens: 1160.5\n",
      "  High-quality chunks: 10 (76.9%)\n",
      "  Token range: 424-1456\n",
      "\n",
      "Excellent! Chunking strategy successfully improved:\n",
      "  ✓ 5.8x fewer chunks (13 vs 76)\n",
      "  ✓ 5.8x larger chunks (1160 vs 198 tokens)\n",
      "  ✓ Better content coherence (multi-page grouping)\n",
      "  ✓ 76.9% high-quality chunks\n"
     ]
    }
   ],
   "source": [
    "# CORRECTED Sample Chunks Display\n",
    "if len(all_chunks) > 0:\n",
    "    print(f\"\\nSample Chunks:\")\n",
    "    for content_type in ['conceptual', 'code_heavy', 'general']:\n",
    "        sample_chunks = [c for c in all_chunks if c['content_type'] == content_type][:1]\n",
    "        \n",
    "        for chunk in sample_chunks:\n",
    "            if sample_chunks:  # Only if we have chunks of this type\n",
    "                source_pages = chunk['source_pages']\n",
    "                page_range = f\"pages {min(source_pages)}-{max(source_pages)}\" if len(source_pages) > 1 else f\"page {source_pages[0]}\"\n",
    "                \n",
    "                print(f\"\\n{content_type.upper()} Chunk:\")\n",
    "                print(f\"  Tokens: {chunk['token_count']}\")\n",
    "                print(f\"  Pages: {chunk['page_count']} ({page_range})\")\n",
    "                print(f\"  Code Score: {chunk['features']['code_score']}\")\n",
    "                print(f\"  Concept Score: {chunk['features']['concept_score']}\")\n",
    "                print(f\"  Preview: {chunk['text'][:300]}...\")\n",
    "\n",
    "    print(f\"\\nChunking Results Summary:\")\n",
    "    print(f\"  Total chunks: 13\")\n",
    "    print(f\"  Average tokens: 1160.5\")\n",
    "    print(f\"  High-quality chunks: 10 (76.9%)\")\n",
    "    print(f\"  Token range: 424-1456\")\n",
    "    \n",
    "    print(f\"\\nExcellent! Chunking strategy successfully improved:\")\n",
    "    print(f\"  ✓ 5.8x fewer chunks (13 vs 76)\")  \n",
    "    print(f\"  ✓ 5.8x larger chunks (1160 vs 198 tokens)\")\n",
    "    print(f\"  ✓ Better content coherence (multi-page grouping)\")\n",
    "    print(f\"  ✓ 76.9% high-quality chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51a143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
