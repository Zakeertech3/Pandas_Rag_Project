{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f563d0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\mygame\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading improvement analysis...\n",
      "Baseline Performance:\n",
      "  Average score: 0.509\n",
      "  Good+ rate: 22.2%\n",
      "\n",
      "Improvement Plan:\n",
      "  Best embedding model: multi-qa-mpnet-base-dot-v1\n",
      "  Additional pages to add: 8\n",
      "  Expected improvement: +17.7%\n",
      "\n",
      "============================================================\n",
      "STEP 1: EXTRACTING ADDITIONAL HIGH-VALUE CONTENT\n",
      "============================================================\n",
      "Extracting content from 8 additional pages...\n",
      "  Extracted page 40 (navigation, pandas_score=11)\n",
      "  Extracted page 20 (navigation, pandas_score=10)\n",
      "  Extracted page 372 (navigation, pandas_score=6)\n",
      "  Extracted page 204 (conceptual, pandas_score=5)\n",
      "  Extracted page 72 (conceptual, pandas_score=3)\n",
      "  Extracted page 4 (navigation, pandas_score=3)\n",
      "  Extracted page 96 (general, pandas_score=2)\n",
      "  Extracted page 356 (general, pandas_score=2)\n",
      "Successfully extracted 8 additional pages\n",
      "\n",
      "============================================================\n",
      "STEP 2: IMPROVED CONTENT PROCESSING\n",
      "============================================================\n",
      "Existing chunks: 13\n",
      "Additional pages: 8\n",
      "\n",
      "Combined content:\n",
      "  Original chunks: 13\n",
      "  Additional chunks: 0\n",
      "  Total chunks: 13\n",
      "  Average tokens: 1160.5\n",
      "\n",
      "============================================================\n",
      "STEP 3: IMPLEMENTING BEST EMBEDDING MODEL\n",
      "============================================================\n",
      "Switching to: multi-qa-mpnet-base-dot-v1\n",
      "Model loaded. Embedding dimension: 768\n",
      "\n",
      "Generating embeddings for 13 chunks...\n",
      "  Generated 5/13 embeddings\n",
      "  Generated 10/13 embeddings\n",
      "  Generated 13/13 embeddings\n",
      "Embeddings generated in 25.52 seconds\n",
      "\n",
      "============================================================\n",
      "STEP 4: IMPLEMENTING QUERY PREPROCESSING\n",
      "============================================================\n",
      "Query preprocessing examples:\n",
      "  Original:  'What is a pandas DataFrame?'\n",
      "  Processed: 'what is a pandas DataFrame?'\n",
      "\n",
      "  Original:  'What is a pandas Series?'\n",
      "  Processed: 'what is a pandas Series?'\n",
      "\n",
      "  Original:  'Difference between Series and DataFrame'\n",
      "  Processed: 'pandas difference between Series and DataFrame'\n",
      "\n",
      "  Original:  'How to create a DataFrame?'\n",
      "  Processed: 'pandas how to create a DataFrame?'\n",
      "\n",
      "  Original:  'How to read CSV files in pandas?'\n",
      "  Processed: 'how to read csv files in pandas?'\n",
      "\n",
      "\n",
      "============================================================\n",
      "STEP 5: SETTING UP IMPROVED VECTOR DATABASE\n",
      "============================================================\n",
      "Creating optimized collection: pandas_docs_optimized\n",
      "  Deleted existing collection\n",
      "  Collection created successfully\n",
      "Preparing 13 data points...\n",
      "Inserting points into Qdrant...\n",
      "  Inserted 13 points successfully\n",
      "  Verification: 13 points in collection\n",
      "\n",
      "============================================================\n",
      "STEP 6: COMPREHENSIVE RETRIEVAL TESTING\n",
      "============================================================\n",
      "Testing retrieval with all improvements...\n",
      "   1. Good      (0.695) - What is a pandas DataFrame?\n",
      "   2. Good      (0.700) - What is a pandas Series?\n",
      "   3. Fair      (0.592) - Difference between Series and DataFrame\n",
      "   4. Good      (0.712) - How to create a DataFrame?\n",
      "   5. Good      (0.625) - How to read CSV files in pandas?\n",
      "\n",
      "IMPROVED RETRIEVAL QUALITY ANALYSIS:\n",
      "Total queries tested: 5\n",
      "Average top score: 0.665\n",
      "Median top score: 0.695\n",
      "\n",
      "Quality Distribution:\n",
      "  Excellent:  0 queries ( 0.0%)\n",
      "  Good     :  4 queries (80.0%)\n",
      "  Fair     :  1 queries (20.0%)\n",
      "  Poor     :  0 queries ( 0.0%)\n",
      "\n",
      "IMPROVEMENT ANALYSIS:\n",
      "  Baseline average score: 0.509\n",
      "  Improved average score: 0.665\n",
      "  Actual improvement: +30.6%\n",
      "  Baseline Good+ rate: 22.2%\n",
      "  Improved Good+ rate: 80.0%\n",
      "\n",
      "Detailed Analysis for Previously Poor Queries:\n",
      "\n",
      "SUCCESS: RETRIEVAL IMPROVEMENTS IMPLEMENTED\n",
      "========================================\n",
      "✓ Embedding model upgraded to: multi-qa-mpnet-base-dot-v1\n",
      "✓ Content expanded: +0 chunks\n",
      "✓ Query preprocessing implemented\n",
      "✓ +30.6% improvement achieved\n",
      "✓ Good+ rate: 22.2% → 80.0%\n",
      "\n",
      "Optimized retrieval system ready for LLM integration!\n",
      "Collection: 'pandas_docs_optimized' with 13 high-quality chunks\n",
      "\n",
      "Results saved:\n",
      "  Evaluation: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\improved_evaluation.pkl\n",
      "  Chunks: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\optimized_chunks.pkl\n"
     ]
    }
   ],
   "source": [
    "# 05_implement_improvements.ipynb - Implementing Retrieval Improvements\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "import re\n",
    "import PyPDF2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "PDF_FILE = PROJECT_ROOT / 'data' / 'raw' / 'mastering_pandas_2025.pdf'\n",
    "\n",
    "print(\"Loading improvement analysis...\")\n",
    "with open(PROCESSED_DIR / 'improvement_analysis.pkl', 'rb') as f:\n",
    "    improvement_data = pickle.load(f)\n",
    "\n",
    "with open(PROCESSED_DIR / 'retrieval_evaluation.pkl', 'rb') as f:\n",
    "    baseline_results = pickle.load(f)\n",
    "\n",
    "print(\"Baseline Performance:\")\n",
    "print(f\"  Average score: {baseline_results['metrics']['avg_score']:.3f}\")\n",
    "print(f\"  Good+ rate: {baseline_results['metrics']['good_plus_rate']:.1%}\")\n",
    "\n",
    "print(f\"\\nImprovement Plan:\")\n",
    "print(f\"  Best embedding model: {improvement_data['best_embedding_model']}\")\n",
    "print(f\"  Additional pages to add: {improvement_data['recommendations']['expansion_pages']}\")\n",
    "print(f\"  Expected improvement: +{improvement_data['recommendations']['expected_improvement']:.1f}%\")\n",
    "\n",
    "# Step 1: Extract Additional High-Value Pages\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: EXTRACTING ADDITIONAL HIGH-VALUE CONTENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_additional_pages(pdf_path, expansion_pages, max_pages=8):\n",
    "    \"\"\"Extract content from additional high-value pages\"\"\"\n",
    "    \n",
    "    print(f\"Extracting content from {min(len(expansion_pages), max_pages)} additional pages...\")\n",
    "    \n",
    "    additional_content = []\n",
    "    pages_to_process = expansion_pages[:max_pages]\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        for page_info in pages_to_process:\n",
    "            page_num = page_info['page']\n",
    "            \n",
    "            try:\n",
    "                text = pdf_reader.pages[page_num].extract_text()\n",
    "                \n",
    "                if text.strip() and len(text) > 200:\n",
    "                    additional_content.append({\n",
    "                        'page_num': page_num,\n",
    "                        'text': text,\n",
    "                        'content_type': page_info['content_type'],\n",
    "                        'pandas_score': page_info['pandas_score'],\n",
    "                        'char_count': page_info['char_count']\n",
    "                    })\n",
    "                    print(f\"  Extracted page {page_num} ({page_info['content_type']}, pandas_score={page_info['pandas_score']})\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Error extracting page {page_num}: {e}\")\n",
    "    \n",
    "    print(f\"Successfully extracted {len(additional_content)} additional pages\")\n",
    "    return additional_content\n",
    "\n",
    "# Extract additional pages\n",
    "expansion_pages = improvement_data['available_expansion_pages']\n",
    "additional_pages = extract_additional_pages(PDF_FILE, expansion_pages, max_pages=8)\n",
    "\n",
    "# Step 2: Improved Content Processing\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: IMPROVED CONTENT PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load existing chunks\n",
    "with open(PROCESSED_DIR / 'processed_chunks.pkl', 'rb') as f:\n",
    "    existing_chunks = pickle.load(f)\n",
    "\n",
    "print(f\"Existing chunks: {len(existing_chunks)}\")\n",
    "print(f\"Additional pages: {len(additional_pages)}\")\n",
    "\n",
    "# Import chunking functions from previous notebooks\n",
    "def clean_pdf_text(text):\n",
    "    \"\"\"Clean PDF extraction artifacts\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\bwher\\s+e\\b', 'where', text)\n",
    "    text = re.sub(r'\\btransfor\\s+ms\\b', 'transforms', text)\n",
    "    text = re.sub(r'\\bData\\s+Frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bdata\\s+frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bGroup\\s+By\\b', 'GroupBy', text)\n",
    "    text = re.sub(r'\\bgroup\\s+by\\b', 'groupby', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def detect_content_features(text):\n",
    "    \"\"\"Analyze content characteristics\"\"\"\n",
    "    code_patterns = [\n",
    "        r'import\\s+\\w+', r'from\\s+\\w+\\s+import', r'pd\\.\\w+', r'df\\.\\w+', \n",
    "        r'print\\s*\\(', r'=\\s*pd\\.', r'\\.groupby\\(', r'\\.merge\\(',\n",
    "        r'\\.iloc\\[', r'\\.loc\\[', r'def\\s+\\w+', r'class\\s+\\w+'\n",
    "    ]\n",
    "    code_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in code_patterns)\n",
    "    \n",
    "    pandas_concepts = [\n",
    "        'DataFrame', 'Series', 'Index', 'groupby', 'merge', 'concat',\n",
    "        'pivot', 'melt', 'apply', 'lambda', 'iloc', 'loc', 'query'\n",
    "    ]\n",
    "    concept_score = sum(text.lower().count(concept.lower()) for concept in pandas_concepts)\n",
    "    \n",
    "    has_headers = bool(re.search(r'^[A-Z][^.!?]*:?\\s*$', text, re.MULTILINE))\n",
    "    has_code_blocks = bool(re.search(r'```|>>>|\\n\\s*\\w+\\s*=', text))\n",
    "    has_examples = bool(re.search(r'example|Example|for instance|For instance', text, re.IGNORECASE))\n",
    "    \n",
    "    return {\n",
    "        'code_score': code_score,\n",
    "        'concept_score': concept_score,\n",
    "        'has_headers': has_headers,\n",
    "        'has_code_blocks': has_code_blocks,\n",
    "        'has_examples': has_examples,\n",
    "        'is_code_heavy': code_score > 3,\n",
    "        'is_concept_heavy': concept_score > 5\n",
    "    }\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def process_additional_content(additional_pages, target_tokens=1000):\n",
    "    \"\"\"Process additional pages into chunks\"\"\"\n",
    "    \n",
    "    additional_chunks = []\n",
    "    \n",
    "    for page_data in additional_pages:\n",
    "        cleaned_text = clean_pdf_text(page_data['text'])\n",
    "        total_tokens = count_tokens(cleaned_text)\n",
    "        \n",
    "        # If page is substantial enough to be its own chunk\n",
    "        if total_tokens >= 400:\n",
    "            features = detect_content_features(cleaned_text)\n",
    "            \n",
    "            additional_chunks.append({\n",
    "                'text': cleaned_text,\n",
    "                'token_count': total_tokens,\n",
    "                'content_type': page_data['content_type'],\n",
    "                'chunk_index': 0,\n",
    "                'global_chunk_id': len(existing_chunks) + len(additional_chunks),\n",
    "                'source_pages': [page_data['page_num']],\n",
    "                'page_count': 1,\n",
    "                'features': features,\n",
    "                'is_additional': True,\n",
    "                'pandas_score': page_data['pandas_score']\n",
    "            })\n",
    "            \n",
    "            print(f\"  Created chunk from page {page_data['page_num']} ({total_tokens} tokens)\")\n",
    "    \n",
    "    return additional_chunks\n",
    "\n",
    "# Process additional content\n",
    "additional_chunks = process_additional_content(additional_pages)\n",
    "\n",
    "# Combine existing and additional chunks\n",
    "all_chunks = existing_chunks + additional_chunks\n",
    "\n",
    "print(f\"\\nCombined content:\")\n",
    "print(f\"  Original chunks: {len(existing_chunks)}\")\n",
    "print(f\"  Additional chunks: {len(additional_chunks)}\")\n",
    "print(f\"  Total chunks: {len(all_chunks)}\")\n",
    "print(f\"  Average tokens: {np.mean([c['token_count'] for c in all_chunks]):.1f}\")\n",
    "\n",
    "# Step 3: Implement Best Embedding Model\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: IMPLEMENTING BEST EMBEDDING MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_model_name = improvement_data['best_embedding_model']\n",
    "print(f\"Switching to: {best_model_name}\")\n",
    "\n",
    "# Initialize improved embedding model\n",
    "embedding_model = SentenceTransformer(best_model_name)\n",
    "embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"Model loaded. Embedding dimension: {embedding_dimension}\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(all_chunks)} chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chunk_embeddings = []\n",
    "for i, chunk in enumerate(all_chunks):\n",
    "    embedding = embedding_model.encode(chunk['text'])\n",
    "    chunk_embeddings.append(embedding)\n",
    "    \n",
    "    if (i + 1) % 5 == 0 or i == len(all_chunks) - 1:\n",
    "        print(f\"  Generated {i + 1}/{len(all_chunks)} embeddings\")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"Embeddings generated in {embedding_time:.2f} seconds\")\n",
    "\n",
    "# Step 4: Implement Query Preprocessing\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: IMPLEMENTING QUERY PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def preprocess_pandas_query(query):\n",
    "    \"\"\"Advanced pandas-specific query preprocessing\"\"\"\n",
    "    \n",
    "    # Normalize pandas terminology\n",
    "    pandas_normalizations = {\n",
    "        'dataframe': 'DataFrame',\n",
    "        'data frame': 'DataFrame', \n",
    "        'data-frame': 'DataFrame',\n",
    "        'series': 'Series',\n",
    "        'groupby': 'group by aggregation',\n",
    "        'group-by': 'group by aggregation',\n",
    "        'concat': 'concatenate combine',\n",
    "        'merge': 'join merge DataFrames'\n",
    "    }\n",
    "    \n",
    "    processed_query = query.lower()\n",
    "    for wrong, correct in pandas_normalizations.items():\n",
    "        processed_query = processed_query.replace(wrong, correct)\n",
    "    \n",
    "    # Add context for function queries\n",
    "    function_expansions = {\n",
    "        'group by': 'pandas groupby aggregation function examples',\n",
    "        'merge': 'pandas merge join DataFrames function',\n",
    "        'concatenate': 'pandas concat combine DataFrames function',\n",
    "        'pivot': 'pandas pivot table reshape data function',\n",
    "        'melt': 'pandas melt reshape data function'\n",
    "    }\n",
    "    \n",
    "    for func, expansion in function_expansions.items():\n",
    "        if func in processed_query:\n",
    "            processed_query = f\"{processed_query} {expansion}\"\n",
    "    \n",
    "    # Add pandas context if missing\n",
    "    if 'pandas' not in processed_query and any(term in processed_query \n",
    "                                              for term in ['DataFrame', 'Series', 'csv', 'data']):\n",
    "        processed_query = f\"pandas {processed_query}\"\n",
    "    \n",
    "    return processed_query\n",
    "\n",
    "# Test preprocessing on sample queries\n",
    "test_queries = baseline_results['test_queries'][:5]\n",
    "print(\"Query preprocessing examples:\")\n",
    "for query in test_queries:\n",
    "    processed = preprocess_pandas_query(query)\n",
    "    if query != processed:\n",
    "        print(f\"  Original:  '{query}'\")\n",
    "        print(f\"  Processed: '{processed}'\")\n",
    "        print()\n",
    "\n",
    "# Step 5: Setup Improved Vector Database\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: SETTING UP IMPROVED VECTOR DATABASE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Connect to Qdrant\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "\n",
    "collection_name = \"pandas_docs_optimized\"\n",
    "print(f\"Creating optimized collection: {collection_name}\")\n",
    "\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    print(\"  Deleted existing collection\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE)\n",
    ")\n",
    "print(\"  Collection created successfully\")\n",
    "\n",
    "# Prepare and insert points with rich metadata\n",
    "print(f\"Preparing {len(all_chunks)} data points...\")\n",
    "points = []\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(all_chunks, chunk_embeddings)):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embedding.tolist(),\n",
    "        payload={\n",
    "            \"text\": chunk['text'],\n",
    "            \"token_count\": chunk['token_count'],\n",
    "            \"content_type\": chunk['content_type'],\n",
    "            \"chunk_index\": chunk['chunk_index'],\n",
    "            \"global_chunk_id\": chunk['global_chunk_id'],\n",
    "            \"source_pages\": chunk['source_pages'],\n",
    "            \"page_count\": chunk['page_count'],\n",
    "            \"code_score\": chunk['features']['code_score'],\n",
    "            \"concept_score\": chunk['features']['concept_score'],\n",
    "            \"is_code_heavy\": chunk['features']['is_code_heavy'],\n",
    "            \"is_concept_heavy\": chunk['features']['is_concept_heavy'],\n",
    "            \"is_additional\": chunk.get('is_additional', False),\n",
    "            \"pandas_score\": chunk.get('pandas_score', 0),\n",
    "            \"preview\": chunk['text'][:300] + \"...\" if len(chunk['text']) > 300 else chunk['text']\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "# Insert points\n",
    "print(f\"Inserting points into Qdrant...\")\n",
    "result = qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "print(f\"  Inserted {len(points)} points successfully\")\n",
    "\n",
    "# Verify\n",
    "count_result = qdrant_client.count(collection_name)\n",
    "print(f\"  Verification: {count_result.count} points in collection\")\n",
    "\n",
    "# Step 6: Comprehensive Testing\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: COMPREHENSIVE RETRIEVAL TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def improved_retrieval_test(query, collection_name, top_k=3, use_preprocessing=True):\n",
    "    \"\"\"Test retrieval with improvements\"\"\"\n",
    "    \n",
    "    # Preprocess query if enabled\n",
    "    if use_preprocessing:\n",
    "        processed_query = preprocess_pandas_query(query)\n",
    "    else:\n",
    "        processed_query = query\n",
    "    \n",
    "    # Generate embedding\n",
    "    query_embedding = embedding_model.encode(processed_query)\n",
    "    \n",
    "    # Search\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    return search_results.points, processed_query\n",
    "\n",
    "# Test all queries with improvements\n",
    "print(f\"Testing retrieval with all improvements...\")\n",
    "improved_results = {}\n",
    "improved_scores = []\n",
    "quality_distribution = {\"Excellent\": 0, \"Good\": 0, \"Fair\": 0, \"Poor\": 0}\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    results, processed_query = improved_retrieval_test(query, collection_name)\n",
    "    \n",
    "    if results:\n",
    "        top_score = results[0].score\n",
    "        avg_score = np.mean([r.score for r in results])\n",
    "        \n",
    "        # Categorize quality\n",
    "        if top_score > 0.8:\n",
    "            quality = \"Excellent\"\n",
    "        elif top_score > 0.6:\n",
    "            quality = \"Good\"\n",
    "        elif top_score > 0.4:\n",
    "            quality = \"Fair\"\n",
    "        else:\n",
    "            quality = \"Poor\"\n",
    "        \n",
    "        quality_distribution[quality] += 1\n",
    "        improved_scores.append(top_score)\n",
    "        \n",
    "        improved_results[query] = {\n",
    "            'top_score': top_score,\n",
    "            'avg_score': avg_score,\n",
    "            'quality': quality,\n",
    "            'processed_query': processed_query,\n",
    "            'results_count': len(results)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {i+1:2d}. {quality:9s} ({top_score:.3f}) - {query}\")\n",
    "\n",
    "# Calculate improvement metrics\n",
    "print(f\"\\nIMPROVED RETRIEVAL QUALITY ANALYSIS:\")\n",
    "print(f\"Total queries tested: {len(test_queries)}\")\n",
    "print(f\"Average top score: {np.mean(improved_scores):.3f}\")\n",
    "print(f\"Median top score: {np.median(improved_scores):.3f}\")\n",
    "\n",
    "print(f\"\\nQuality Distribution:\")\n",
    "for quality, count in quality_distribution.items():\n",
    "    percentage = (count / len(test_queries)) * 100\n",
    "    print(f\"  {quality:9s}: {count:2d} queries ({percentage:4.1f}%)\")\n",
    "\n",
    "# Compare with baseline\n",
    "baseline_avg = baseline_results['metrics']['avg_score']\n",
    "improved_avg = np.mean(improved_scores)\n",
    "actual_improvement = ((improved_avg - baseline_avg) / baseline_avg) * 100\n",
    "\n",
    "excellent_rate = quality_distribution[\"Excellent\"] / len(test_queries)\n",
    "good_plus_rate = (quality_distribution[\"Excellent\"] + quality_distribution[\"Good\"]) / len(test_queries)\n",
    "\n",
    "print(f\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "print(f\"  Baseline average score: {baseline_avg:.3f}\")\n",
    "print(f\"  Improved average score: {improved_avg:.3f}\")\n",
    "print(f\"  Actual improvement: {actual_improvement:+.1f}%\")\n",
    "print(f\"  Baseline Good+ rate: {baseline_results['metrics']['good_plus_rate']:.1%}\")\n",
    "print(f\"  Improved Good+ rate: {good_plus_rate:.1%}\")\n",
    "\n",
    "# Detailed analysis for problem queries\n",
    "print(f\"\\nDetailed Analysis for Previously Poor Queries:\")\n",
    "\n",
    "problem_queries = [\n",
    "    \"pandas groupby function\",\n",
    "    \"Series creation syntax\", \n",
    "    \"pandas concat function\"\n",
    "]\n",
    "\n",
    "for query in problem_queries:\n",
    "    if query in improved_results:\n",
    "        result = improved_results[query]\n",
    "        baseline_score = baseline_results['evaluation_results'].get(query, {}).get('top_score', 0)\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"  Baseline score: {baseline_score:.3f}\")\n",
    "        print(f\"  Improved score: {result['top_score']:.3f}\")\n",
    "        print(f\"  Improvement: {((result['top_score'] - baseline_score) / baseline_score * 100):+.1f}%\")\n",
    "        print(f\"  Processed query: '{result['processed_query']}'\")\n",
    "        \n",
    "        # Show top result\n",
    "        results, _ = improved_retrieval_test(query, collection_name, top_k=1)\n",
    "        if results:\n",
    "            top_result = results[0]\n",
    "            print(f\"  Top result: {top_result.payload['content_type']} chunk\")\n",
    "            print(f\"  Source pages: {top_result.payload['source_pages']}\")\n",
    "            print(f\"  Preview: {top_result.payload['preview'][:150]}...\")\n",
    "\n",
    "# Save improved results\n",
    "improved_evaluation = {\n",
    "    'improved_results': improved_results,\n",
    "    'quality_distribution': quality_distribution,\n",
    "    'metrics': {\n",
    "        'avg_score': improved_avg,\n",
    "        'median_score': np.median(improved_scores),\n",
    "        'excellent_rate': excellent_rate,\n",
    "        'good_plus_rate': good_plus_rate,\n",
    "        'actual_improvement': actual_improvement\n",
    "    },\n",
    "    'configuration': {\n",
    "        'embedding_model': best_model_name,\n",
    "        'total_chunks': len(all_chunks),\n",
    "        'additional_chunks': len(additional_chunks),\n",
    "        'query_preprocessing': True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DIR / 'improved_evaluation.pkl', 'wb') as f:\n",
    "    pickle.dump(improved_evaluation, f)\n",
    "\n",
    "with open(PROCESSED_DIR / 'optimized_chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "print(f\"\\nSUCCESS: RETRIEVAL IMPROVEMENTS IMPLEMENTED\")\n",
    "print(f\"========================================\")\n",
    "print(f\"✓ Embedding model upgraded to: {best_model_name}\")\n",
    "print(f\"✓ Content expanded: +{len(additional_chunks)} chunks\")\n",
    "print(f\"✓ Query preprocessing implemented\")\n",
    "print(f\"✓ {actual_improvement:+.1f}% improvement achieved\")\n",
    "print(f\"✓ Good+ rate: {baseline_results['metrics']['good_plus_rate']:.1%} → {good_plus_rate:.1%}\")\n",
    "\n",
    "print(f\"\\nOptimized retrieval system ready for LLM integration!\")\n",
    "print(f\"Collection: '{collection_name}' with {len(all_chunks)} high-quality chunks\")\n",
    "\n",
    "print(f\"\\nResults saved:\")\n",
    "print(f\"  Evaluation: {PROCESSED_DIR / 'improved_evaluation.pkl'}\")\n",
    "print(f\"  Chunks: {PROCESSED_DIR / 'optimized_chunks.pkl'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0faed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
