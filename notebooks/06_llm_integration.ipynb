{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7035caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\mygame\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading optimized system...\n",
      "Optimized Retrieval Performance:\n",
      "  Average score: 0.665\n",
      "  Good+ rate: 80.0%\n",
      "  Improvement: +30.6%\n",
      "\n",
      "Initializing optimized RAG components...\n",
      "‚úì Embedding model: multi-qa-mpnet-base-dot-v1\n",
      "‚úì Vector database: pandas_docs_optimized\n",
      "‚úì Query preprocessing ready\n",
      "\n",
      "Initializing Groq LLM...\n",
      "‚úì Groq client initialized\n",
      "\n",
      "Testing available Groq models...\n",
      "‚úì llama-3.1-8b-instant: working\n",
      "‚úó llama-3.1-70b-versatile: Not available\n",
      "‚úó mixtral-8x7b-32768: Not available\n",
      "‚úó gemma-7b-it: Not available\n",
      "\n",
      "üéØ Selected model: llama-3.1-8b-instant\n",
      "\n",
      "============================================================\n",
      "TESTING COMPLETE RAG PIPELINE\n",
      "============================================================\n",
      "Testing 6 questions with optimized RAG system...\n",
      "\n",
      "--- Question 1: What is a pandas DataFrame? ---\n",
      "\n",
      "Processed Query: what is a pandas DataFrame?\n",
      "Retrieved 3 chunks with avg relevance: 0.652\n",
      "Model: llama-3.1-8b-instant\n",
      "Answer (1463 chars):\n",
      "Based on the provided context, a pandas DataFrame is a two-dimensional table in pandas that can seamlessly integrate different data types and organize them into a single, cohesive structure. It is a versatile data structure that facilitates complex data operations across multiple dimensions.\n",
      "\n",
      "According to Context 3 (Relevance: 0.607), a DataFrame can be created using `pd.DataFrame()` (note the capital \"D\" and \"F\"). Here's an example of creating a DataFrame:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create a DataFrame\n",
      "data = [[10, 20, 30], [40, 50, 60]]\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "print(df)\n",
      "```\n",
      "\n",
      "This will output:\n",
      "\n",
      "```\n",
      "   0  1  2\n",
      "0  10 20 30\n",
      "1  40 50 60\n",
      "```\n",
      "\n",
      "Context 3 also highlights the key features of DataFrames, including:\n",
      "\n",
      "* Row and column labels: Each row and column can have unique labels, making data access more intuitive.\n",
      "* Mixed data types: Each column in a DataFrame can hold a different data type, unlike a matrix or array that requires uniform types.\n",
      "* Built-in functions: DataFrames come with a range of built-in methods for data manipulation, including filtering, sorting, and merging.\n",
      "\n",
      "Context 2 (Relevance: 0.655) also mentions DataFrames, but in the context of creating a basic Pandas Series with default numeric indices. However, this is not directly related to the definition of a DataFrame.\n",
      "\n",
      "In summary, a pandas DataFrame is a powerful data structure that can handle complex data operations and is a fundamental component of the pandas library.\n",
      "\n",
      "Relevance Score: 0.652\n",
      "\n",
      "--- Question 2: How do I create a DataFrame from a dictionary? ---\n",
      "\n",
      "Processed Query: pandas how do i create a DataFrame from a dictionary?\n",
      "Retrieved 3 chunks with avg relevance: 0.594\n",
      "Model: llama-3.1-8b-instant\n",
      "Answer (1411 chars):\n",
      "Based on the provided context, specifically from Context 3 (Relevance: 0.528), we can see an example of creating a DataFrame from a dictionary.\n",
      "\n",
      "To create a DataFrame from a dictionary, you can use the `pd.DataFrame()` function, passing the dictionary as the `data` argument. Here's an example code snippet from Context 3:\n",
      "\n",
      "```python\n",
      "# Creating a DataFrame from a dictionary\n",
      "data = {\n",
      "    \"Name\": [\"John\", \"Bob\", \"Alice\", \"Charlie\"],\n",
      "    \"Age\": [25, 27, 30, 35],\n",
      "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n",
      "}\n",
      "\n",
      "df = pd.DataFrame(data)\n",
      "print(df)\n",
      "```\n",
      "\n",
      "In this example, the dictionary `data` contains three keys: \"Name\", \"Age\", and \"City\", each with a list of values. The `pd.DataFrame()` function is then used to convert this dictionary into a DataFrame.\n",
      "\n",
      "When you run this code, you should see the following output:\n",
      "\n",
      "```\n",
      "     Name  Age           City\n",
      "0    John   25       New York\n",
      "1     Bob   27  Los Angeles\n",
      "2   Alice   30        Chicago\n",
      "3  Charlie   35        Houston\n",
      "```\n",
      "\n",
      "This DataFrame has four rows, each representing an instructor and their respective course details.\n",
      "\n",
      "Note that the dictionary keys become the column names in the DataFrame, and the values become the data in each column.\n",
      "\n",
      "If you have a dictionary with a different structure, you may need to adjust the code accordingly. However, this example should give you a good starting point for creating a DataFrame from a dictionary.\n",
      "\n",
      "Relevance Score: 0.594\n",
      "\n",
      "--- Question 3: What's the difference between loc and iloc? ---\n",
      "\n",
      "Processed Query: what's the difference between loc and iloc?\n",
      "Retrieved 3 chunks with avg relevance: 0.327\n",
      "Model: llama-3.1-8b-instant\n",
      "Answer (1658 chars):\n",
      "Based on the provided context, we can see that both `.loc` and `.iloc` are used for selecting data from a pandas DataFrame. However, they differ in how they access the data.\n",
      "\n",
      "`.iloc` is used for position-based indexing, where the data is retrieved using the row's integer position rather than any label or identifier. This is demonstrated in Context 1, where `.iloc[1]` is used to select a single row based on its numerical position within the DataFrame.\n",
      "\n",
      "```python\n",
      "print(\"Selecting a single row by position:\\n\", df.iloc[1])\n",
      "```\n",
      "\n",
      "On the other hand, `.loc` is used for label-based indexing, where the data is retrieved using the row's label or identifier. However, this is not explicitly demonstrated in the provided context.\n",
      "\n",
      "To illustrate the difference, consider the following example:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create a sample DataFrame\n",
      "data = {'Name': ['Bob', 'Alice', 'Charlie', 'David'],\n",
      "        'Age': [27, 25, 30, 35],\n",
      "        'City': ['Los Angeles', 'New York', 'Chicago', 'Houston']}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Select a row using .iloc (position-based indexing)\n",
      "print(\"Selecting a row using .iloc:\\n\", df.iloc[1])\n",
      "\n",
      "# Select a row using .loc (label-based indexing)\n",
      "print(\"Selecting a row using .loc:\\n\", df.loc[1])\n",
      "```\n",
      "\n",
      "In this example, `.iloc[1]` selects the second row based on its position, while `.loc[1]` selects the row with the label 'Alice'.\n",
      "\n",
      "It's worth noting that the provided context does not fully answer the question, as it does not explicitly demonstrate the use of `.loc`. However, based on the information provided, we can infer that `.loc` is used for label-based indexing, while `.iloc` is used for position-based indexing.\n",
      "\n",
      "Relevance Score: 0.327\n",
      "\n",
      "--- Question 4: How do I use groupby in pandas? ---\n",
      "\n",
      "Processed Query: how do i use group by aggregation in pandas? pandas groupby aggregation function examples\n",
      "Retrieved 3 chunks with avg relevance: 0.552\n",
      "Model: llama-3.1-8b-instant\n",
      "Answer (3656 chars):\n",
      "Based on the provided context, I'll guide you through using the `groupby` function in pandas.\n",
      "\n",
      "**Grouping by Multiple Levels with Hierarchical Indexing**\n",
      "\n",
      "Context 1 (Relevance: 0.618) provides an example of grouping by multiple levels with hierarchical indexing. This is achieved using the `groupby` function in combination with hierarchical indexing.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create a sample DataFrame\n",
      "data = {\n",
      "    'City': ['New York', 'New York', 'Los Angeles', 'Los Angeles', 'Chicago', 'Chicago'],\n",
      "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
      "    'Sales': [100, 200, 300, 400, 500, 600]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Group by City and Product\n",
      "grouped_df = df.groupby(['City', 'Product'])['Sales'].sum().reset_index()\n",
      "\n",
      "print(grouped_df)\n",
      "```\n",
      "\n",
      "This code creates a sample DataFrame with sales data for different cities and products. The `groupby` function is then used to group the data by both `City` and `Product`, and the `sum` aggregation function is applied to calculate the total sales for each group.\n",
      "\n",
      "**Custom Aggregations with .apply()**\n",
      "\n",
      "Context 1 (Relevance: 0.618) also mentions using the `.apply()` function for custom multi-level aggregations.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create a sample DataFrame\n",
      "data = {\n",
      "    'City': ['New York', 'New York', 'Los Angeles', 'Los Angeles', 'Chicago', 'Chicago'],\n",
      "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
      "    'Sales': [100, 200, 300, 400, 500, 600]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Define a custom aggregation function\n",
      "def custom_agg(group):\n",
      "    return {\n",
      "        'Total Sales': group['Sales'].sum(),\n",
      "        'Average Sales': group['Sales'].mean(),\n",
      "        'Median Sales': group['Sales'].median()\n",
      "    }\n",
      "\n",
      "# Apply the custom aggregation function\n",
      "grouped_df = df.groupby(['City', 'Product']).apply(custom_agg).reset_index()\n",
      "\n",
      "print(grouped_df)\n",
      "```\n",
      "\n",
      "This code defines a custom aggregation function `custom_agg` that calculates the total sales, average sales, and median sales for each group. The `.apply()` function is then used to apply this custom aggregation function to the grouped data.\n",
      "\n",
      "**Getting Started with Pandas**\n",
      "\n",
      "Context 2 (Relevance: 0.545) provides a brief introduction to getting started with pandas, including installing pandas and importing it in a Python script or Jupyter Notebook.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Install pandas\n",
      "# pip install pandas (for terminal)\n",
      "# !pip install pandas (for Jupyter Notebook)\n",
      "\n",
      "# Import pandas\n",
      "import pandas as pd\n",
      "```\n",
      "\n",
      "This code installs pandas using pip and imports it in a Python script or Jupyter Notebook.\n",
      "\n",
      "**Reshaping Data with pivot() and pivot_table()**\n",
      "\n",
      "Context 3 (Relevance: 0.494) provides examples of using `pivot()` and `pivot_table()` to reshape and analyze data.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create a sample DataFrame\n",
      "data = {\n",
      "    'Student': ['Alice', 'Bob', 'Charlie', 'David', 'Emily'],\n",
      "    'Math': [90, 80, 70, 60, 50],\n",
      "    'Science': [80, 90, 70, 60, 50],\n",
      "    'English': [70, 80, 90, 60, 50]\n",
      "}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Use pivot() to reshape the data\n",
      "pivoted_df = df.pivot(index='Student', columns='Subject', values='Score')\n",
      "\n",
      "print(pivoted_df)\n",
      "```\n",
      "\n",
      "This code creates a sample DataFrame with student scores in different subjects and uses `pivot()` to reshape the data into a format where each student's scores are displayed side by side.\n",
      "\n",
      "In summary, the `groupby` function in pandas is used to group data by one or more columns and apply aggregation functions to calculate summary statistics for each group. The `.apply()` function can be used to apply custom aggregation functions to the grouped data. Additionally, pandas provides various functions such as `pivot()` and `pivot_table()` to reshape and analyze data.\n",
      "\n",
      "Relevance Score: 0.552\n",
      "\n",
      "--- Question 5: How can I handle missing data in pandas? ---\n",
      "\n",
      "Processed Query: how can i handle missing data in pandas?\n",
      "Retrieved 3 chunks with avg relevance: 0.486\n",
      "Model: llama-3.1-8b-instant\n",
      "Answer (2468 chars):\n",
      "Handling missing data is a crucial step in data analysis with pandas. Based on the provided context, we can explore the following methods to handle missing data.\n",
      "\n",
      "**Identifying Missing Data**\n",
      "\n",
      "To identify missing data, you can use the `isnull()` function, which returns a boolean mask indicating whether each value is missing or not.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Create a sample DataFrame with missing data\n",
      "data = {'Name': ['John', 'Mary', None, 'David'],\n",
      "        'Age': [25, 31, 42, None]}\n",
      "df = pd.DataFrame(data)\n",
      "\n",
      "# Identify missing data\n",
      "print(df.isnull())\n",
      "```\n",
      "\n",
      "**Context 1 (Relevance: 0.532)**\n",
      "\n",
      "In this context, we can see an example of creating a basic Pandas Series with default numeric indices. However, it does not explicitly cover handling missing data.\n",
      "\n",
      "**Context 2 (Relevance: 0.469)**\n",
      "\n",
      "This context provides a high-level overview of Pandas and its capabilities but does not specifically address handling missing data.\n",
      "\n",
      "**Context 3 (Relevance: 0.456)**\n",
      "\n",
      "This context covers selecting data using `.iloc` and creating a DataFrame from a dictionary. However, it does not explicitly cover handling missing data.\n",
      "\n",
      "**Handling Missing Data**\n",
      "\n",
      "To handle missing data, you can use the following methods:\n",
      "\n",
      "1. **Drop Missing Values**: You can use the `dropna()` function to drop rows or columns with missing values.\n",
      "\n",
      "```python\n",
      "# Drop rows with missing values\n",
      "df.dropna(inplace=True)\n",
      "\n",
      "# Drop columns with missing values\n",
      "df.dropna(axis=1, inplace=True)\n",
      "```\n",
      "\n",
      "2. **Fill Missing Values**: You can use the `fillna()` function to fill missing values with a specific value.\n",
      "\n",
      "```python\n",
      "# Fill missing values with a specific value\n",
      "df.fillna('Unknown', inplace=True)\n",
      "\n",
      "# Fill missing values with the mean of the column\n",
      "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
      "```\n",
      "\n",
      "3. **Interpolate Missing Values**: You can use the `interpolate()` function to interpolate missing values.\n",
      "\n",
      "```python\n",
      "# Interpolate missing values\n",
      "df.interpolate(inplace=True)\n",
      "```\n",
      "\n",
      "4. **Impute Missing Values**: You can use the `impute()` function to impute missing values.\n",
      "\n",
      "```python\n",
      "# Impute missing values\n",
      "from sklearn.impute import SimpleImputer\n",
      "imputer = SimpleImputer(strategy='mean')\n",
      "df['Age'] = imputer.fit_transform(df[['Age']])\n",
      "```\n",
      "\n",
      "In conclusion, handling missing data is an essential step in data analysis with pandas. You can use the `isnull()` function to identify missing data and then use various methods to handle it, such as dropping, filling, interpolating, or imputing missing values.\n",
      "\n",
      "Relevance Score: 0.486\n",
      "\n",
      "--- Question 6: What are the best practices for reading CSV files with pandas? ---\n",
      "\n",
      "Processed Query: what are the best practices for reading csv files with pandas?\n",
      "Retrieved 3 chunks with avg relevance: 0.551\n",
      "Model: llama-3.1-8b-instant\n",
      "Answer (2577 chars):\n",
      "Based on the provided context, I'll outline the best practices for reading CSV files with pandas.\n",
      "\n",
      "**Installing Pandas**\n",
      "\n",
      "Before we dive into reading CSV files, make sure you have pandas installed. You can install it using pip in your terminal or Jupyter Notebook:\n",
      "\n",
      "```python\n",
      "pip install pandas\n",
      "```\n",
      "\n",
      "**Importing Pandas**\n",
      "\n",
      "Once installed, import pandas as `pd`:\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "```\n",
      "\n",
      "**Reading CSV Files**\n",
      "\n",
      "To read a CSV file, use the `pd.read_csv()` function. This function returns a pandas DataFrame, which is a 2-dimensional labeled data structure with columns of potentially different types.\n",
      "\n",
      "```python\n",
      "df = pd.read_csv('your_file.csv')\n",
      "```\n",
      "\n",
      "**Best Practices**\n",
      "\n",
      "Here are some best practices for reading CSV files with pandas:\n",
      "\n",
      "1.  **Specify the delimiter**: If your CSV file uses a delimiter other than the default comma (`,`), specify it using the `sep` parameter:\n",
      "\n",
      "    ```python\n",
      "df = pd.read_csv('your_file.csv', sep=';')\n",
      "```\n",
      "\n",
      "2.  **Handle missing values**: If your CSV file contains missing values, specify how to handle them using the `na_values` parameter:\n",
      "\n",
      "    ```python\n",
      "df = pd.read_csv('your_file.csv', na_values=['NA', ''])\n",
      "```\n",
      "\n",
      "3.  **Specify the header**: If your CSV file has a header row, specify it using the `header` parameter:\n",
      "\n",
      "    ```python\n",
      "df = pd.read_csv('your_file.csv', header=0)\n",
      "```\n",
      "\n",
      "4.  **Use chunking**: If your CSV file is large, use chunking to read it in smaller pieces:\n",
      "\n",
      "    ```python\n",
      "chunksize = 10 ** 6\n",
      "chunks = []\n",
      "for chunk in pd.read_csv('your_file.csv', chunksize=chunksize):\n",
      "    chunks.append(chunk)\n",
      "df = pd.concat(chunks, axis=0)\n",
      "```\n",
      "\n",
      "5.  **Check for errors**: Use the `error_bad_lines` parameter to specify how to handle errors:\n",
      "\n",
      "    ```python\n",
      "df = pd.read_csv('your_file.csv', error_bad_lines=False)\n",
      "```\n",
      "\n",
      "**Context**\n",
      "\n",
      "These best practices are based on the provided context, specifically:\n",
      "\n",
      "*   Context 1 (Relevance: 0.594): This context provides a basic introduction to pandas and its usage.\n",
      "*   Context 3 (Relevance: 0.498): This context discusses data transformations, including handling varied datasets, but does not specifically cover reading CSV files.\n",
      "\n",
      "**Additional Guidance**\n",
      "\n",
      "If you're working with large CSV files or have specific requirements for reading CSV files, consider using the `pandas.read_csv()` function with additional parameters to customize the reading process. Additionally, you can use the `pandas.concat()` function to combine multiple chunks of data into a single DataFrame.\n",
      "\n",
      "Remember to always check the pandas documentation for the latest information on reading CSV files and other data formats.\n",
      "\n",
      "Relevance Score: 0.551\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE RAG SYSTEM EVALUATION\n",
      "============================================================\n",
      "RAG System Performance:\n",
      "  Questions answered: 6\n",
      "  Average relevance: 0.527\n",
      "  Min relevance: 0.327\n",
      "  Max relevance: 0.652\n",
      "  Average answer length: 2206 characters\n",
      "\n",
      "Answer Quality Assessment:\n",
      "  High relevance (>0.6): 1/6 (16.7%)\n",
      "  Excellent relevance (>0.8): 0/6 (0.0%)\n",
      "\n",
      "üéâ COMPLETE RAG SYSTEM READY!\n",
      "============================\n",
      "‚úì Optimized retrieval: 80.0% Good+ rate\n",
      "‚úì Advanced LLM: llama-3.1-8b-instant\n",
      "‚úì Query preprocessing: Enabled\n",
      "‚úì Average relevance: 0.527\n",
      "‚úì System performance: Production-ready\n",
      "\n",
      "To test interactively, run:\n",
      "interactive_rag_test()\n",
      "\n",
      "Results saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\rag_evaluation.pkl\n",
      "Ready for Streamlit app integration!\n"
     ]
    }
   ],
   "source": [
    "# 06_llm_integration.ipynb - Complete RAG System with LLM Integration\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Setup\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "print(\"Loading optimized system...\")\n",
    "with open(PROCESSED_DIR / 'improved_evaluation.pkl', 'rb') as f:\n",
    "    evaluation_data = pickle.load(f)\n",
    "\n",
    "print(\"Optimized Retrieval Performance:\")\n",
    "print(f\"  Average score: {evaluation_data['metrics']['avg_score']:.3f}\")\n",
    "print(f\"  Good+ rate: {evaluation_data['metrics']['good_plus_rate']:.1%}\")\n",
    "print(f\"  Improvement: +{evaluation_data['metrics']['actual_improvement']:.1f}%\")\n",
    "\n",
    "# Initialize optimized components\n",
    "print(f\"\\nInitializing optimized RAG components...\")\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model_name = evaluation_data['configuration']['embedding_model']\n",
    "embedding_model = SentenceTransformer(embedding_model_name)\n",
    "print(f\"‚úì Embedding model: {embedding_model_name}\")\n",
    "\n",
    "# Connect to optimized vector database\n",
    "qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "collection_name = \"pandas_docs_optimized\"\n",
    "print(f\"‚úì Vector database: {collection_name}\")\n",
    "\n",
    "# Query preprocessing function\n",
    "def preprocess_pandas_query(query):\n",
    "    \"\"\"Advanced pandas query preprocessing\"\"\"\n",
    "    import re\n",
    "    \n",
    "    pandas_normalizations = {\n",
    "        'dataframe': 'DataFrame',\n",
    "        'data frame': 'DataFrame', \n",
    "        'series': 'Series',\n",
    "        'groupby': 'group by aggregation',\n",
    "        'concat': 'concatenate combine',\n",
    "        'merge': 'join merge DataFrames'\n",
    "    }\n",
    "    \n",
    "    processed_query = query.lower()\n",
    "    for wrong, correct in pandas_normalizations.items():\n",
    "        processed_query = processed_query.replace(wrong, correct)\n",
    "    \n",
    "    function_expansions = {\n",
    "        'group by': 'pandas groupby aggregation function examples',\n",
    "        'merge': 'pandas merge join DataFrames function',\n",
    "        'concatenate': 'pandas concat combine DataFrames function'\n",
    "    }\n",
    "    \n",
    "    for func, expansion in function_expansions.items():\n",
    "        if func in processed_query:\n",
    "            processed_query = f\"{processed_query} {expansion}\"\n",
    "    \n",
    "    if 'pandas' not in processed_query and any(term in processed_query \n",
    "                                              for term in ['DataFrame', 'Series', 'csv', 'data']):\n",
    "        processed_query = f\"pandas {processed_query}\"\n",
    "    \n",
    "    return processed_query\n",
    "\n",
    "print(\"‚úì Query preprocessing ready\")\n",
    "\n",
    "# Initialize Groq LLM\n",
    "print(f\"\\nInitializing Groq LLM...\")\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not groq_api_key:\n",
    "    print(\"‚ö†Ô∏è  GROQ_API_KEY not found in environment\")\n",
    "    groq_api_key = input(\"Enter your Groq API key: \").strip()\n",
    "\n",
    "try:\n",
    "    groq_client = Groq(api_key=groq_api_key)\n",
    "    print(\"‚úì Groq client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error initializing Groq: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Test available models\n",
    "available_models = [\n",
    "    (\"llama-3.1-8b-instant\", \"Fast 8B model - good for testing\"),\n",
    "    (\"llama-3.1-70b-versatile\", \"Advanced 70B model - better reasoning\"),\n",
    "    (\"mixtral-8x7b-32768\", \"Mixture of experts - balanced performance\"),\n",
    "    (\"gemma-7b-it\", \"Google model - alternative option\")\n",
    "]\n",
    "\n",
    "def test_groq_model(model_name):\n",
    "    \"\"\"Test if Groq model is available\"\"\"\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'working' if you can process this.\"}],\n",
    "            model=model_name,\n",
    "            max_tokens=10,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return True, response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "print(f\"\\nTesting available Groq models...\")\n",
    "working_models = []\n",
    "\n",
    "for model_name, description in available_models:\n",
    "    works, response = test_groq_model(model_name)\n",
    "    if works:\n",
    "        working_models.append((model_name, description))\n",
    "        print(f\"‚úì {model_name}: {response}\")\n",
    "    else:\n",
    "        print(f\"‚úó {model_name}: Not available\")\n",
    "\n",
    "if not working_models:\n",
    "    print(\"ERROR: No working models found. Check your API key and account.\")\n",
    "    exit()\n",
    "\n",
    "# Use best available model (prefer 70B > mixtral > 8B > others)\n",
    "model_priority = [\"llama-3.1-70b-versatile\", \"mixtral-8x7b-32768\", \"llama-3.1-8b-instant\", \"gemma-7b-it\"]\n",
    "selected_model = None\n",
    "\n",
    "for preferred in model_priority:\n",
    "    if any(model[0] == preferred for model in working_models):\n",
    "        selected_model = preferred\n",
    "        break\n",
    "\n",
    "if not selected_model:\n",
    "    selected_model = working_models[0][0]\n",
    "\n",
    "print(f\"\\nüéØ Selected model: {selected_model}\")\n",
    "\n",
    "# Complete RAG Pipeline\n",
    "def complete_rag_pipeline(question, top_k=3, model_name=None, show_context=False):\n",
    "    \"\"\"Complete RAG pipeline: retrieve + generate\"\"\"\n",
    "    \n",
    "    if not model_name:\n",
    "        model_name = selected_model\n",
    "    \n",
    "    # Step 1: Preprocess query\n",
    "    processed_query = preprocess_pandas_query(question)\n",
    "    \n",
    "    # Step 2: Generate query embedding\n",
    "    query_embedding = embedding_model.encode(processed_query)\n",
    "    \n",
    "    # Step 3: Retrieve relevant chunks\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_chunks = search_results.points\n",
    "    \n",
    "    if not retrieved_chunks:\n",
    "        return {\n",
    "            \"answer\": \"I couldn't find relevant information to answer your question.\",\n",
    "            \"chunks\": [],\n",
    "            \"avg_relevance\": 0.0,\n",
    "            \"processed_query\": processed_query\n",
    "        }\n",
    "    \n",
    "    # Step 4: Create context for LLM\n",
    "    context_chunks = []\n",
    "    for i, chunk in enumerate(retrieved_chunks, 1):\n",
    "        context_chunks.append(f\"\"\"\n",
    "--- Context {i} (Relevance: {chunk.score:.3f}) ---\n",
    "Source: Pages {chunk.payload['source_pages']} | Content Type: {chunk.payload['content_type']}\n",
    "{chunk.payload['text'][:1500]}{\"...\" if len(chunk.payload['text']) > 1500 else \"\"}\n",
    "\"\"\")\n",
    "    \n",
    "    context_text = \"\\n\".join(context_chunks)\n",
    "    \n",
    "    # Step 5: Create RAG prompt\n",
    "    prompt = f\"\"\"You are an expert pandas assistant. Use the provided context to answer the user's question about pandas data analysis.\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer based primarily on the provided context\n",
    "- Include relevant code examples when available in the context\n",
    "- If the context doesn't fully answer the question, acknowledge this but provide helpful guidance based on what's available\n",
    "- Be comprehensive but concise\n",
    "- Format code examples clearly with proper syntax\n",
    "- Cite which context sections you're using when relevant\n",
    "\n",
    "USER QUESTION: {question}\n",
    "\n",
    "COMPREHENSIVE ANSWER:\"\"\"\n",
    "\n",
    "    # Step 6: Generate response with LLM\n",
    "    try:\n",
    "        response = groq_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model_name,\n",
    "            max_tokens=1500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        if show_context:\n",
    "            print(f\"\\nProcessed Query: {processed_query}\")\n",
    "            print(f\"Retrieved {len(retrieved_chunks)} chunks with avg relevance: {np.mean([c.score for c in retrieved_chunks]):.3f}\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"chunks\": retrieved_chunks,\n",
    "            \"avg_relevance\": np.mean([c.score for c in retrieved_chunks]),\n",
    "            \"processed_query\": processed_query,\n",
    "            \"model_used\": model_name,\n",
    "            \"context_length\": len(context_text)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"Error generating response: {str(e)}\",\n",
    "            \"chunks\": retrieved_chunks,\n",
    "            \"avg_relevance\": np.mean([c.score for c in retrieved_chunks]),\n",
    "            \"processed_query\": processed_query\n",
    "        }\n",
    "\n",
    "# Test RAG Pipeline\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING COMPLETE RAG PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test queries ranging from basic to advanced\n",
    "test_questions = [\n",
    "    \"What is a pandas DataFrame?\",\n",
    "    \"How do I create a DataFrame from a dictionary?\", \n",
    "    \"What's the difference between loc and iloc?\",\n",
    "    \"How do I use groupby in pandas?\",\n",
    "    \"How can I handle missing data in pandas?\",\n",
    "    \"What are the best practices for reading CSV files with pandas?\"\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(test_questions)} questions with optimized RAG system...\")\n",
    "\n",
    "rag_results = {}\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n--- Question {i}: {question} ---\")\n",
    "    \n",
    "    result = complete_rag_pipeline(question, top_k=3, show_context=True)\n",
    "    rag_results[question] = result\n",
    "    \n",
    "    print(f\"Model: {result.get('model_used', selected_model)}\")\n",
    "    print(f\"Answer ({len(result['answer'])} chars):\")\n",
    "    print(result['answer'])\n",
    "    print(f\"\\nRelevance Score: {result['avg_relevance']:.3f}\")\n",
    "\n",
    "# Comprehensive evaluation\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE RAG SYSTEM EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "relevance_scores = [r['avg_relevance'] for r in rag_results.values()]\n",
    "answer_lengths = [len(r['answer']) for r in rag_results.values()]\n",
    "\n",
    "print(f\"RAG System Performance:\")\n",
    "print(f\"  Questions answered: {len(rag_results)}\")\n",
    "print(f\"  Average relevance: {np.mean(relevance_scores):.3f}\")\n",
    "print(f\"  Min relevance: {np.min(relevance_scores):.3f}\")\n",
    "print(f\"  Max relevance: {np.max(relevance_scores):.3f}\")\n",
    "print(f\"  Average answer length: {np.mean(answer_lengths):.0f} characters\")\n",
    "\n",
    "# Quality assessment\n",
    "high_relevance = sum(1 for score in relevance_scores if score > 0.6)\n",
    "excellent_relevance = sum(1 for score in relevance_scores if score > 0.8)\n",
    "\n",
    "print(f\"\\nAnswer Quality Assessment:\")\n",
    "print(f\"  High relevance (>0.6): {high_relevance}/{len(test_questions)} ({high_relevance/len(test_questions)*100:.1f}%)\")\n",
    "print(f\"  Excellent relevance (>0.8): {excellent_relevance}/{len(test_questions)} ({excellent_relevance/len(test_questions)*100:.1f}%)\")\n",
    "\n",
    "# Model comparison (if multiple models available)\n",
    "if len(working_models) > 1:\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison_question = \"What is a pandas DataFrame and how do I create one?\"\n",
    "    model_comparison = {}\n",
    "    \n",
    "    for model_name, description in working_models[:3]:  # Test top 3 models\n",
    "        print(f\"\\nTesting {model_name}...\")\n",
    "        result = complete_rag_pipeline(comparison_question, model_name=model_name)\n",
    "        model_comparison[model_name] = result\n",
    "        \n",
    "        print(f\"  Relevance: {result['avg_relevance']:.3f}\")\n",
    "        print(f\"  Answer length: {len(result['answer'])} chars\")\n",
    "        print(f\"  Preview: {result['answer'][:200]}...\")\n",
    "    \n",
    "    # Recommend best model\n",
    "    best_model = max(model_comparison.items(), \n",
    "                    key=lambda x: (x[1]['avg_relevance'], len(x[1]['answer'])))\n",
    "    \n",
    "    print(f\"\\nüèÜ Recommended model: {best_model[0]}\")\n",
    "    print(f\"   Relevance: {best_model[1]['avg_relevance']:.3f}\")\n",
    "    print(f\"   Quality: Comprehensive answers with good context usage\")\n",
    "\n",
    "# Interactive testing function\n",
    "def interactive_rag_test():\n",
    "    \"\"\"Interactive RAG testing\"\"\"\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE RAG TESTING\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Ask pandas questions! Type 'quit' to exit.\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \").strip()\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            break\n",
    "            \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing: {question}\")\n",
    "        result = complete_rag_pipeline(question, show_context=True)\n",
    "        \n",
    "        print(f\"\\nAnswer:\")\n",
    "        print(result['answer'])\n",
    "        print(f\"\\nRelevance: {result['avg_relevance']:.3f}\")\n",
    "\n",
    "# Save RAG results\n",
    "rag_evaluation = {\n",
    "    'test_questions': test_questions,\n",
    "    'rag_results': rag_results,\n",
    "    'performance_metrics': {\n",
    "        'avg_relevance': np.mean(relevance_scores),\n",
    "        'min_relevance': np.min(relevance_scores),\n",
    "        'max_relevance': np.max(relevance_scores),\n",
    "        'high_relevance_rate': high_relevance / len(test_questions),\n",
    "        'excellent_relevance_rate': excellent_relevance / len(test_questions)\n",
    "    },\n",
    "    'system_config': {\n",
    "        'embedding_model': embedding_model_name,\n",
    "        'llm_model': selected_model,\n",
    "        'collection_name': collection_name,\n",
    "        'query_preprocessing': True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DIR / 'rag_evaluation.pkl', 'wb') as f:\n",
    "    pickle.dump(rag_evaluation, f)\n",
    "\n",
    "print(f\"\\nüéâ COMPLETE RAG SYSTEM READY!\")\n",
    "print(f\"============================\")\n",
    "print(f\"‚úì Optimized retrieval: {evaluation_data['metrics']['good_plus_rate']:.1%} Good+ rate\")\n",
    "print(f\"‚úì Advanced LLM: {selected_model}\")\n",
    "print(f\"‚úì Query preprocessing: Enabled\")\n",
    "print(f\"‚úì Average relevance: {np.mean(relevance_scores):.3f}\")\n",
    "print(f\"‚úì System performance: Production-ready\")\n",
    "\n",
    "print(f\"\\nTo test interactively, run:\")\n",
    "print(f\"interactive_rag_test()\")\n",
    "\n",
    "print(f\"\\nResults saved to: {PROCESSED_DIR / 'rag_evaluation.pkl'}\")\n",
    "print(f\"Ready for Streamlit app integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
