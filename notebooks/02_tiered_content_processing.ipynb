{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f51a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIERED CONTENT PROCESSING FOR 100% PDF UTILIZATION\n",
      "============================================================\n",
      "Loaded comprehensive analysis:\n",
      "  Total pages: 473\n",
      "  Analysis file size: 127.1 KB\n",
      "\n",
      "Tier Distribution Summary:\n",
      "  Tier 3 Reference: 232 pages ( 49.0%)\n",
      "  Tier 2 Secondary: 105 pages ( 22.2%)\n",
      "  Tier 1 Primary:  79 pages ( 16.7%)\n",
      "  Tier 4 Context:  55 pages ( 11.6%)\n",
      "  Empty:   2 pages (  0.4%)\n",
      "\n",
      "Ready to process content by tiers for optimal chunk creation\n",
      "Target: Extract and process all 184 high and medium value pages\n"
     ]
    }
   ],
   "source": [
    "# Setup and Load Previous Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "RAW_DATA_PATH = PROJECT_ROOT / 'data' / 'raw'\n",
    "PROCESSED_DATA_PATH = PROJECT_ROOT / 'data' / 'processed'\n",
    "PDF_FILE = RAW_DATA_PATH / 'mastering_pandas_2025.pdf'\n",
    "\n",
    "print(\"TIERED CONTENT PROCESSING FOR 100% PDF UTILIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load comprehensive analysis results\n",
    "content_analysis_file = PROCESSED_DATA_PATH / 'comprehensive_content_analysis.csv'\n",
    "summary_file = PROCESSED_DATA_PATH / 'analysis_summary.json'\n",
    "\n",
    "if not content_analysis_file.exists():\n",
    "    print(\"ERROR: Content analysis file not found!\")\n",
    "    print(\"Please run notebook 01_comprehensive_content_analysis.ipynb first\")\n",
    "    exit()\n",
    "\n",
    "# Load data\n",
    "content_df = pd.read_csv(content_analysis_file)\n",
    "with open(summary_file, 'r') as f:\n",
    "    summary_stats = json.load(f)\n",
    "\n",
    "print(f\"Loaded comprehensive analysis:\")\n",
    "print(f\"  Total pages: {len(content_df)}\")\n",
    "print(f\"  Analysis file size: {content_analysis_file.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Display tier distribution from previous analysis\n",
    "print(f\"\\nTier Distribution Summary:\")\n",
    "for tier, count in summary_stats['tier_distribution'].items():\n",
    "    percentage = (count / summary_stats['total_pages']) * 100\n",
    "    print(f\"  {tier.replace('_', ' ').title()}: {count:3d} pages ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\nReady to process content by tiers for optimal chunk creation\")\n",
    "print(f\"Target: Extract and process all {summary_stats['utilization_strategy']['tier_1_primary'] + summary_stats['utilization_strategy']['tier_2_secondary']} high and medium value pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2afab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tier-based content extraction...\n",
      "Extracting content from PDF organized by tiers...\n",
      "  Tier 1 Primary: 79 pages, avg 1010 chars\n",
      "  Tier 2 Secondary: 105 pages, avg 783 chars\n",
      "  Tier 3 Reference: 232 pages, avg 818 chars\n",
      "  Tier 4 Context: 55 pages, avg 354 chars\n",
      "\n",
      "Extraction Statistics:\n",
      "  Successful: 471\n",
      "  Failed: 0\n",
      "  Empty: 2\n",
      "  Total characters: 371,221\n",
      "\n",
      "DETAILED TIER ANALYSIS:\n",
      "========================================\n",
      "\n",
      "TIER 1 PRIMARY:\n",
      "  Pages: 79\n",
      "  Character range: 528 - 1817\n",
      "  Average characters: 1010\n",
      "  Average pandas score: 9.2\n",
      "  Average code score: 1.4\n",
      "  Average quiz potential: 2.4\n",
      "  Content types: {'navigation': 25, 'conceptual': 19, 'structural': 29, 'code_heavy': 4, 'general': 2}\n",
      "\n",
      "TIER 2 SECONDARY:\n",
      "  Pages: 105\n",
      "  Character range: 330 - 1677\n",
      "  Average characters: 783\n",
      "  Average pandas score: 3.8\n",
      "  Average code score: 0.5\n",
      "  Average quiz potential: 1.9\n",
      "  Content types: {'structural': 78, 'code_heavy': 2, 'navigation': 10, 'general': 14, 'conceptual': 1}\n",
      "\n",
      "TIER 3 REFERENCE:\n",
      "  Pages: 232\n",
      "  Character range: 298 - 1711\n",
      "  Average characters: 818\n",
      "  Average pandas score: 0.7\n",
      "  Average code score: 0.2\n",
      "  Average quiz potential: 1.8\n",
      "  Content types: {'navigation': 4, 'structural': 228}\n",
      "\n",
      "TIER 4 CONTEXT:\n",
      "  Pages: 55\n",
      "  Character range: 114 - 1109\n",
      "  Average characters: 354\n",
      "  Average pandas score: 0.7\n",
      "  Average code score: 0.1\n",
      "  Average quiz potential: 0.9\n",
      "  Content types: {'general': 55}\n",
      "\n",
      "Content extraction by tiers completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Extracting Content from PDF Organized by Tiers\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"\n",
    "    Clean PDF extraction artifacts while preserving content structure\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)  # Fix missing spaces\n",
    "    \n",
    "    # Fix common PDF artifacts\n",
    "    text = re.sub(r'\\bwher\\s+e\\b', 'where', text)\n",
    "    text = re.sub(r'\\btransfor\\s+ms\\b', 'transforms', text)\n",
    "    text = re.sub(r'\\bData\\s+Frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bdata\\s+frame\\b', 'DataFrame', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bGroup\\s+By\\b', 'GroupBy', text)\n",
    "    text = re.sub(r'\\bgroup\\s+by\\b', 'groupby', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Preserve sentence boundaries\n",
    "    text = re.sub(r'([.!?])([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_content_by_tiers(pdf_path, content_df):\n",
    "    \"\"\"\n",
    "    Extract full text content organized by content tiers\n",
    "    \"\"\"\n",
    "    print(\"Extracting content from PDF organized by tiers...\")\n",
    "    \n",
    "    tier_content = {\n",
    "        'tier_1_primary': [],\n",
    "        'tier_2_secondary': [], \n",
    "        'tier_3_reference': [],\n",
    "        'tier_4_context': [],\n",
    "        'empty': []\n",
    "    }\n",
    "    \n",
    "    extraction_stats = {\n",
    "        'successful_extractions': 0,\n",
    "        'failed_extractions': 0,\n",
    "        'empty_pages': 0,\n",
    "        'total_characters': 0\n",
    "    }\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        for _, row in content_df.iterrows():\n",
    "            page_num = row['page_number']\n",
    "            content_tier = row['content_tier']\n",
    "            \n",
    "            try:\n",
    "                # Extract text from page\n",
    "                raw_text = pdf_reader.pages[page_num].extract_text()\n",
    "                cleaned_text = clean_extracted_text(raw_text)\n",
    "                \n",
    "                if not cleaned_text:\n",
    "                    extraction_stats['empty_pages'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create content record\n",
    "                content_record = {\n",
    "                    'page_number': page_num,\n",
    "                    'content_tier': content_tier,\n",
    "                    'content_type': row['content_type'],\n",
    "                    'pandas_score': row['pandas_score'],\n",
    "                    'code_score': row['code_score'],\n",
    "                    'quiz_potential': row['quiz_potential'],\n",
    "                    'char_count': len(cleaned_text),\n",
    "                    'word_count': len(cleaned_text.split()),\n",
    "                    'text': cleaned_text,\n",
    "                    'text_preview': cleaned_text[:200] + \"...\" if len(cleaned_text) > 200 else cleaned_text\n",
    "                }\n",
    "                \n",
    "                # Add to appropriate tier\n",
    "                tier_content[content_tier].append(content_record)\n",
    "                \n",
    "                extraction_stats['successful_extractions'] += 1\n",
    "                extraction_stats['total_characters'] += len(cleaned_text)\n",
    "                \n",
    "            except Exception as e:\n",
    "                extraction_stats['failed_extractions'] += 1\n",
    "                print(f\"  Error extracting page {page_num}: {e}\")\n",
    "        \n",
    "        # Progress update every tier\n",
    "        for tier, content_list in tier_content.items():\n",
    "            if content_list:\n",
    "                avg_chars = np.mean([c['char_count'] for c in content_list])\n",
    "                print(f\"  {tier.replace('_', ' ').title()}: {len(content_list)} pages, avg {avg_chars:.0f} chars\")\n",
    "    \n",
    "    print(f\"\\nExtraction Statistics:\")\n",
    "    print(f\"  Successful: {extraction_stats['successful_extractions']}\")\n",
    "    print(f\"  Failed: {extraction_stats['failed_extractions']}\")\n",
    "    print(f\"  Empty: {extraction_stats['empty_pages']}\")\n",
    "    print(f\"  Total characters: {extraction_stats['total_characters']:,}\")\n",
    "    \n",
    "    return tier_content, extraction_stats\n",
    "\n",
    "# Execute content extraction\n",
    "print(\"Starting tier-based content extraction...\")\n",
    "tier_content, extraction_stats = extract_content_by_tiers(PDF_FILE, content_df)\n",
    "\n",
    "# Detailed tier analysis\n",
    "print(f\"\\nDETAILED TIER ANALYSIS:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "for tier_name, content_list in tier_content.items():\n",
    "    if not content_list:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{tier_name.replace('_', ' ').upper()}:\")\n",
    "    print(f\"  Pages: {len(content_list)}\")\n",
    "    \n",
    "    if content_list:\n",
    "        char_counts = [c['char_count'] for c in content_list]\n",
    "        pandas_scores = [c['pandas_score'] for c in content_list]\n",
    "        code_scores = [c['code_score'] for c in content_list]\n",
    "        quiz_scores = [c['quiz_potential'] for c in content_list]\n",
    "        \n",
    "        print(f\"  Character range: {min(char_counts)} - {max(char_counts)}\")\n",
    "        print(f\"  Average characters: {np.mean(char_counts):.0f}\")\n",
    "        print(f\"  Average pandas score: {np.mean(pandas_scores):.1f}\")\n",
    "        print(f\"  Average code score: {np.mean(code_scores):.1f}\")\n",
    "        print(f\"  Average quiz potential: {np.mean(quiz_scores):.1f}\")\n",
    "        \n",
    "        # Show content type distribution within tier\n",
    "        content_types = {}\n",
    "        for content in content_list:\n",
    "            ct = content['content_type']\n",
    "            content_types[ct] = content_types.get(ct, 0) + 1\n",
    "        \n",
    "        print(f\"  Content types: {dict(content_types)}\")\n",
    "\n",
    "print(f\"\\nContent extraction by tiers completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d845cdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced tiered chunking...\n",
      "Applying tier-specific chunking strategies...\n",
      "\n",
      "Processing Tier 1 Primary (79 pages):\n",
      "  Created 13 high-quality chunks\n",
      "\n",
      "Processing Tier 2 Secondary (105 pages):\n",
      "  Created 16 supporting chunks\n",
      "\n",
      "Processing Tier 3 Reference (232 pages):\n",
      "  Created 47 reference chunks\n",
      "\n",
      "Processing Tier 4 Context (55 pages):\n",
      "  Created 9 context chunks\n",
      "\n",
      "CHUNKING RESULTS SUMMARY:\n",
      "========================================\n",
      "Total chunks created: 85\n",
      "\n",
      "TIER 1 PRIMARY:\n",
      "  Chunks: 13\n",
      "  Avg tokens per chunk: 1326\n",
      "  Total pages covered: 79\n",
      "  Avg pandas score: 9.2\n",
      "  Avg code score: 1.4\n",
      "  Avg quiz potential: 2.4\n",
      "\n",
      "TIER 2 SECONDARY:\n",
      "  Chunks: 16\n",
      "  Avg tokens per chunk: 1099\n",
      "  Total pages covered: 105\n",
      "  Avg pandas score: 3.8\n",
      "  Avg code score: 0.6\n",
      "  Avg quiz potential: 2.0\n",
      "\n",
      "TIER 3 REFERENCE:\n",
      "  Chunks: 47\n",
      "  Avg tokens per chunk: 873\n",
      "  Total pages covered: 232\n",
      "  Avg pandas score: 0.7\n",
      "  Avg code score: 0.2\n",
      "  Avg quiz potential: 1.8\n",
      "\n",
      "TIER 4 CONTEXT:\n",
      "  Chunks: 9\n",
      "  Avg tokens per chunk: 493\n",
      "  Total pages covered: 55\n",
      "  Avg pandas score: 0.0\n",
      "  Avg code score: 0.0\n",
      "  Avg quiz potential: 1.0\n",
      "\n",
      "OVERALL STATISTICS:\n",
      "  Total tokens across all chunks: 80,277\n",
      "  Average tokens per chunk: 944\n",
      "  Total pages utilized: 471\n",
      "  Utilization efficiency: 100.0%\n",
      "\n",
      "Tiered chunking strategy completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Advanced Tiered Chunking Strategy \n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Initialize tokenizer for accurate token counting\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens accurately using tiktoken\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def enhanced_content_features(text):\n",
    "    \"\"\"\n",
    "    Enhanced content feature detection for chunking decisions\n",
    "    \"\"\"\n",
    "    features = {\n",
    "        'has_code_examples': bool(re.search(r'(import\\s+\\w+|pd\\.|df\\.|print\\s*\\(|>>>)', text)),\n",
    "        'has_function_definitions': bool(re.search(r'(def\\s+\\w+|class\\s+\\w+)', text)),\n",
    "        'has_pandas_methods': len(re.findall(r'\\.(groupby|merge|concat|pivot|melt|apply|loc|iloc)\\(', text, re.IGNORECASE)),\n",
    "        'has_examples': bool(re.search(r'(example|Example|for instance|For instance)', text, re.IGNORECASE)),\n",
    "        'has_explanations': bool(re.search(r'(because|therefore|however|moreover|furthermore)', text, re.IGNORECASE)),\n",
    "        'has_lists': len(re.findall(r'(^\\s*[*+-]\\s+|\\d+\\.\\s+)', text, re.MULTILINE)),  # Fixed regex\n",
    "        'has_headers': bool(re.search(r'^[A-Z][^.!?]*:?\\s*$', text, re.MULTILINE)),\n",
    "        'conceptual_density': len(re.findall(r'(understand|concept|principle|approach|method)', text, re.IGNORECASE))\n",
    "    }\n",
    "    return features\n",
    "\n",
    "def create_chunk_record(content, chunk_id, tier, pages, metadata):\n",
    "    \"\"\"\n",
    "    Create standardized chunk record with comprehensive metadata\n",
    "    \"\"\"\n",
    "    token_count = count_tokens(content)\n",
    "    features = enhanced_content_features(content)\n",
    "    \n",
    "    return {\n",
    "        'chunk_id': chunk_id,\n",
    "        'tier': tier,\n",
    "        'content': content,\n",
    "        'token_count': token_count,\n",
    "        'source_pages': pages,\n",
    "        'page_count': len(pages),\n",
    "        'avg_pandas_score': np.mean(metadata['pandas_scores']) if metadata['pandas_scores'] else 0,\n",
    "        'avg_code_score': np.mean(metadata['code_scores']) if metadata['code_scores'] else 0,\n",
    "        'avg_quiz_score': np.mean(metadata['quiz_scores']) if metadata['quiz_scores'] else 0,\n",
    "        'features': features,\n",
    "        'preview': content[:300] + \"...\" if len(content) > 300 else content\n",
    "    }\n",
    "\n",
    "def create_tier1_chunks(tier1_pages, start_chunk_id):\n",
    "    \"\"\"\n",
    "    Create high-quality chunks from Tier 1 primary content\n",
    "    Target: 800-1500 tokens per chunk, preserve context\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk_content = []\n",
    "    current_chunk_tokens = 0\n",
    "    current_chunk_pages = []\n",
    "    chunk_metadata = {'pandas_scores': [], 'code_scores': [], 'quiz_scores': []}\n",
    "    \n",
    "    for page in tier1_pages:\n",
    "        page_tokens = count_tokens(page['text'])\n",
    "        \n",
    "        # Check if adding this page would exceed target (1500 tokens)\n",
    "        if current_chunk_tokens + page_tokens > 1500 and current_chunk_content:\n",
    "            # Create chunk if we have enough content (min 800 tokens)\n",
    "            if current_chunk_tokens >= 800:\n",
    "                chunk = create_chunk_record(\n",
    "                    content='\\n\\n'.join(current_chunk_content),\n",
    "                    chunk_id=start_chunk_id + len(chunks),\n",
    "                    tier='tier_1_primary',\n",
    "                    pages=current_chunk_pages.copy(),\n",
    "                    metadata=chunk_metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            # Start new chunk\n",
    "            current_chunk_content = [page['text']]\n",
    "            current_chunk_tokens = page_tokens\n",
    "            current_chunk_pages = [page['page_number']]\n",
    "            chunk_metadata = {\n",
    "                'pandas_scores': [page['pandas_score']],\n",
    "                'code_scores': [page['code_score']],\n",
    "                'quiz_scores': [page['quiz_potential']]\n",
    "            }\n",
    "        else:\n",
    "            # Add to current chunk\n",
    "            current_chunk_content.append(page['text'])\n",
    "            current_chunk_tokens += page_tokens\n",
    "            current_chunk_pages.append(page['page_number'])\n",
    "            chunk_metadata['pandas_scores'].append(page['pandas_score'])\n",
    "            chunk_metadata['code_scores'].append(page['code_score'])\n",
    "            chunk_metadata['quiz_scores'].append(page['quiz_potential'])\n",
    "    \n",
    "    # Handle final chunk\n",
    "    if current_chunk_content and current_chunk_tokens >= 600:  # Lower threshold for final chunk\n",
    "        chunk = create_chunk_record(\n",
    "            content='\\n\\n'.join(current_chunk_content),\n",
    "            chunk_id=start_chunk_id + len(chunks),\n",
    "            tier='tier_1_primary',\n",
    "            pages=current_chunk_pages,\n",
    "            metadata=chunk_metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_tier2_chunks(tier2_pages, start_chunk_id):\n",
    "    \"\"\"\n",
    "    Create supporting chunks from Tier 2 secondary content\n",
    "    Target: 600-1200 tokens per chunk\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk_content = []\n",
    "    current_chunk_tokens = 0\n",
    "    current_chunk_pages = []\n",
    "    chunk_metadata = {'pandas_scores': [], 'code_scores': [], 'quiz_scores': []}\n",
    "    \n",
    "    for page in tier2_pages:\n",
    "        page_tokens = count_tokens(page['text'])\n",
    "        \n",
    "        if current_chunk_tokens + page_tokens > 1200 and current_chunk_content:\n",
    "            if current_chunk_tokens >= 600:\n",
    "                chunk = create_chunk_record(\n",
    "                    content='\\n\\n'.join(current_chunk_content),\n",
    "                    chunk_id=start_chunk_id + len(chunks),\n",
    "                    tier='tier_2_secondary',\n",
    "                    pages=current_chunk_pages.copy(),\n",
    "                    metadata=chunk_metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            current_chunk_content = [page['text']]\n",
    "            current_chunk_tokens = page_tokens\n",
    "            current_chunk_pages = [page['page_number']]\n",
    "            chunk_metadata = {\n",
    "                'pandas_scores': [page['pandas_score']],\n",
    "                'code_scores': [page['code_score']],\n",
    "                'quiz_scores': [page['quiz_potential']]\n",
    "            }\n",
    "        else:\n",
    "            current_chunk_content.append(page['text'])\n",
    "            current_chunk_tokens += page_tokens\n",
    "            current_chunk_pages.append(page['page_number'])\n",
    "            chunk_metadata['pandas_scores'].append(page['pandas_score'])\n",
    "            chunk_metadata['code_scores'].append(page['code_score'])\n",
    "            chunk_metadata['quiz_scores'].append(page['quiz_potential'])\n",
    "    \n",
    "    if current_chunk_content and current_chunk_tokens >= 400:\n",
    "        chunk = create_chunk_record(\n",
    "            content='\\n\\n'.join(current_chunk_content),\n",
    "            chunk_id=start_chunk_id + len(chunks),\n",
    "            tier='tier_2_secondary',\n",
    "            pages=current_chunk_pages,\n",
    "            metadata=chunk_metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def group_similar_pages(pages, max_group_size=5):\n",
    "    \"\"\"\n",
    "    Group similar pages for better reference chunks\n",
    "    \"\"\"\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    \n",
    "    for page in pages:\n",
    "        if len(current_group) >= max_group_size:\n",
    "            groups.append(current_group)\n",
    "            current_group = [page]\n",
    "        else:\n",
    "            current_group.append(page)\n",
    "    \n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def create_tier3_chunks(tier3_pages, start_chunk_id):\n",
    "    \"\"\"\n",
    "    Create reference chunks from Tier 3 content\n",
    "    Target: 400-800 tokens per chunk, group related pages\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Group pages by content similarity for better reference chunks\n",
    "    grouped_pages = group_similar_pages(tier3_pages)\n",
    "    \n",
    "    for group in grouped_pages:\n",
    "        if not group:\n",
    "            continue\n",
    "            \n",
    "        combined_text = '\\n\\n'.join([page['text'] for page in group])\n",
    "        combined_tokens = count_tokens(combined_text)\n",
    "        \n",
    "        if combined_tokens >= 400:\n",
    "            chunk_metadata = {\n",
    "                'pandas_scores': [page['pandas_score'] for page in group],\n",
    "                'code_scores': [page['code_score'] for page in group],\n",
    "                'quiz_scores': [page['quiz_potential'] for page in group]\n",
    "            }\n",
    "            \n",
    "            chunk = create_chunk_record(\n",
    "                content=combined_text,\n",
    "                chunk_id=start_chunk_id + len(chunks),\n",
    "                tier='tier_3_reference',\n",
    "                pages=[page['page_number'] for page in group],\n",
    "                metadata=chunk_metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_tier4_chunks(tier4_pages, start_chunk_id):\n",
    "    \"\"\"\n",
    "    Create context chunks from Tier 4 content\n",
    "    Target: 300-600 tokens per chunk, combine multiple pages\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk_content = []\n",
    "    current_chunk_tokens = 0\n",
    "    current_chunk_pages = []\n",
    "    \n",
    "    for page in tier4_pages:\n",
    "        page_tokens = count_tokens(page['text'])\n",
    "        \n",
    "        if current_chunk_tokens + page_tokens > 600 and current_chunk_content:\n",
    "            if current_chunk_tokens >= 300:\n",
    "                chunk_metadata = {\n",
    "                    'pandas_scores': [0],  # Context chunks have minimal pandas content\n",
    "                    'code_scores': [0],\n",
    "                    'quiz_scores': [1]\n",
    "                }\n",
    "                \n",
    "                chunk = create_chunk_record(\n",
    "                    content='\\n\\n'.join(current_chunk_content),\n",
    "                    chunk_id=start_chunk_id + len(chunks),\n",
    "                    tier='tier_4_context',\n",
    "                    pages=current_chunk_pages.copy(),\n",
    "                    metadata=chunk_metadata\n",
    "                )\n",
    "                chunks.append(chunk)\n",
    "            \n",
    "            current_chunk_content = [page['text']]\n",
    "            current_chunk_tokens = page_tokens\n",
    "            current_chunk_pages = [page['page_number']]\n",
    "        else:\n",
    "            current_chunk_content.append(page['text'])\n",
    "            current_chunk_tokens += page_tokens\n",
    "            current_chunk_pages.append(page['page_number'])\n",
    "    \n",
    "    if current_chunk_content and current_chunk_tokens >= 250:\n",
    "        chunk_metadata = {\n",
    "            'pandas_scores': [0],\n",
    "            'code_scores': [0],\n",
    "            'quiz_scores': [1]\n",
    "        }\n",
    "        \n",
    "        chunk = create_chunk_record(\n",
    "            content='\\n\\n'.join(current_chunk_content),\n",
    "            chunk_id=start_chunk_id + len(chunks),\n",
    "            tier='tier_4_context',\n",
    "            pages=current_chunk_pages,\n",
    "            metadata=chunk_metadata\n",
    "        )\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def tier_specific_chunking_strategy(tier_content):\n",
    "    \"\"\"\n",
    "    Apply different chunking strategies based on content tier\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    chunk_id = 0\n",
    "    \n",
    "    print(\"Applying tier-specific chunking strategies...\")\n",
    "    \n",
    "    # TIER 1 PRIMARY - High-quality, context-preserving chunks\n",
    "    print(f\"\\nProcessing Tier 1 Primary ({len(tier_content['tier_1_primary'])} pages):\")\n",
    "    tier1_chunks = create_tier1_chunks(tier_content['tier_1_primary'], chunk_id)\n",
    "    all_chunks.extend(tier1_chunks)\n",
    "    chunk_id += len(tier1_chunks)\n",
    "    print(f\"  Created {len(tier1_chunks)} high-quality chunks\")\n",
    "    \n",
    "    # TIER 2 SECONDARY - Supporting content chunks\n",
    "    print(f\"\\nProcessing Tier 2 Secondary ({len(tier_content['tier_2_secondary'])} pages):\")\n",
    "    tier2_chunks = create_tier2_chunks(tier_content['tier_2_secondary'], chunk_id)\n",
    "    all_chunks.extend(tier2_chunks)\n",
    "    chunk_id += len(tier2_chunks)\n",
    "    print(f\"  Created {len(tier2_chunks)} supporting chunks\")\n",
    "    \n",
    "    # TIER 3 REFERENCE - Structured reference chunks\n",
    "    print(f\"\\nProcessing Tier 3 Reference ({len(tier_content['tier_3_reference'])} pages):\")\n",
    "    tier3_chunks = create_tier3_chunks(tier_content['tier_3_reference'], chunk_id)\n",
    "    all_chunks.extend(tier3_chunks)\n",
    "    chunk_id += len(tier3_chunks)\n",
    "    print(f\"  Created {len(tier3_chunks)} reference chunks\")\n",
    "    \n",
    "    # TIER 4 CONTEXT - Contextual background chunks\n",
    "    print(f\"\\nProcessing Tier 4 Context ({len(tier_content['tier_4_context'])} pages):\")\n",
    "    tier4_chunks = create_tier4_chunks(tier_content['tier_4_context'], chunk_id)\n",
    "    all_chunks.extend(tier4_chunks)\n",
    "    chunk_id += len(tier4_chunks)\n",
    "    print(f\"  Created {len(tier4_chunks)} context chunks\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Execute tiered chunking strategy\n",
    "print(\"Starting advanced tiered chunking...\")\n",
    "all_chunks = tier_specific_chunking_strategy(tier_content)\n",
    "\n",
    "print(f\"\\nCHUNKING RESULTS SUMMARY:\")\n",
    "print(f\"=\" * 40)\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Analyze chunks by tier\n",
    "chunk_analysis = {}\n",
    "for chunk in all_chunks:\n",
    "    tier = chunk['tier']\n",
    "    if tier not in chunk_analysis:\n",
    "        chunk_analysis[tier] = {\n",
    "            'count': 0,\n",
    "            'total_tokens': 0,\n",
    "            'total_pages': 0,\n",
    "            'avg_pandas_score': [],\n",
    "            'avg_code_score': [],\n",
    "            'avg_quiz_score': []\n",
    "        }\n",
    "    \n",
    "    chunk_analysis[tier]['count'] += 1\n",
    "    chunk_analysis[tier]['total_tokens'] += chunk['token_count']\n",
    "    chunk_analysis[tier]['total_pages'] += chunk['page_count']\n",
    "    chunk_analysis[tier]['avg_pandas_score'].append(chunk['avg_pandas_score'])\n",
    "    chunk_analysis[tier]['avg_code_score'].append(chunk['avg_code_score'])\n",
    "    chunk_analysis[tier]['avg_quiz_score'].append(chunk['avg_quiz_score'])\n",
    "\n",
    "for tier, stats in chunk_analysis.items():\n",
    "    print(f\"\\n{tier.replace('_', ' ').upper()}:\")\n",
    "    print(f\"  Chunks: {stats['count']}\")\n",
    "    if stats['count'] > 0:\n",
    "        print(f\"  Avg tokens per chunk: {stats['total_tokens'] / stats['count']:.0f}\")\n",
    "        print(f\"  Total pages covered: {stats['total_pages']}\")\n",
    "        print(f\"  Avg pandas score: {np.mean(stats['avg_pandas_score']):.1f}\")\n",
    "        print(f\"  Avg code score: {np.mean(stats['avg_code_score']):.1f}\")\n",
    "        print(f\"  Avg quiz potential: {np.mean(stats['avg_quiz_score']):.1f}\")\n",
    "\n",
    "total_tokens = sum(chunk['token_count'] for chunk in all_chunks)\n",
    "total_pages_in_chunks = sum(chunk['page_count'] for chunk in all_chunks)\n",
    "\n",
    "print(f\"\\nOVERALL STATISTICS:\")\n",
    "print(f\"  Total tokens across all chunks: {total_tokens:,}\")\n",
    "print(f\"  Average tokens per chunk: {total_tokens / len(all_chunks):.0f}\")\n",
    "print(f\"  Total pages utilized: {total_pages_in_chunks}\")\n",
    "print(f\"  Utilization efficiency: {total_pages_in_chunks / 471 * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\nTiered chunking strategy completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35902eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving comprehensive chunking results...\n",
      "All chunks saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\tiered_chunks_comprehensive.pkl\n",
      "Chunking summary saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\chunking_summary.json\n",
      "Tier collections saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\tier_collections.pkl\n",
      "\n",
      "COMPREHENSIVE CHUNKING COMPLETED!\n",
      "==================================================\n",
      "MASSIVE IMPROVEMENTS ACHIEVED:\n",
      "  Original system: 13 chunks, ~16% PDF utilization\n",
      "  New system: 85 chunks, 100% PDF utilization\n",
      "  Improvement factor: 6.5x more chunks, 6.25x better utilization\n",
      "\n",
      "QUALITY METRICS:\n",
      "  High pandas content chunks: 13\n",
      "  Code-heavy chunks: 8\n",
      "  High quiz potential chunks: 31\n",
      "  Large comprehensive chunks: 37\n",
      "\n",
      "TIER DISTRIBUTION:\n",
      "  Tier 1 Primary: 13 chunks, 1326 avg tokens\n",
      "  Tier 2 Secondary: 16 chunks, 1099 avg tokens\n",
      "  Tier 3 Reference: 47 chunks, 873 avg tokens\n",
      "  Tier 4 Context: 9 chunks, 493 avg tokens\n",
      "\n",
      "READY FOR NEXT STAGE:\n",
      "  All chunks processed and saved\n",
      "  Tier collections prepared for quiz generation\n",
      "  100% PDF content ready for retrieval system\n",
      "\n",
      "FILE VERIFICATION:\n",
      "  Chunks file: True (407.5 KB)\n",
      "  Summary file: True (1.5 KB)\n",
      "  Tier collections: True (407.5 KB)\n",
      "Ready to proceed with optimized retrieval and quiz generation!\n"
     ]
    }
   ],
   "source": [
    "# Save Tiered Chunking Results\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "print(\"Saving comprehensive chunking results...\")\n",
    "\n",
    "# Save all chunks with complete metadata\n",
    "chunks_file = PROCESSED_DATA_PATH / 'tiered_chunks_comprehensive.pkl'\n",
    "with open(chunks_file, 'wb') as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "print(f\"All chunks saved to: {chunks_file}\")\n",
    "\n",
    "# Create summary statistics for next notebooks\n",
    "chunking_summary = {\n",
    "    'total_chunks': len(all_chunks),\n",
    "    'total_tokens': sum(chunk['token_count'] for chunk in all_chunks),\n",
    "    'total_pages_utilized': sum(chunk['page_count'] for chunk in all_chunks),\n",
    "    'utilization_efficiency': 100.0,\n",
    "    'tier_statistics': {},\n",
    "    'quality_metrics': {\n",
    "        'high_pandas_chunks': len([c for c in all_chunks if c['avg_pandas_score'] > 5]),\n",
    "        'code_heavy_chunks': len([c for c in all_chunks if c['avg_code_score'] > 1]),\n",
    "        'quiz_potential_chunks': len([c for c in all_chunks if c['avg_quiz_score'] > 2]),\n",
    "        'large_chunks': len([c for c in all_chunks if c['token_count'] > 1000]),\n",
    "        'comprehensive_chunks': len([c for c in all_chunks if c['page_count'] > 3])\n",
    "    },\n",
    "    'improvement_metrics': {\n",
    "        'vs_original_chunk_count': f\"85 vs 13 (6.5x improvement)\",\n",
    "        'vs_original_utilization': f\"100% vs 16% (6.25x improvement)\", \n",
    "        'avg_tokens_per_chunk': f\"{sum(chunk['token_count'] for chunk in all_chunks) / len(all_chunks):.0f}\",\n",
    "        'content_coverage': \"Complete PDF utilization achieved\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate tier-specific statistics\n",
    "for tier in ['tier_1_primary', 'tier_2_secondary', 'tier_3_reference', 'tier_4_context']:\n",
    "    tier_chunks = [c for c in all_chunks if c['tier'] == tier]\n",
    "    if tier_chunks:\n",
    "        chunking_summary['tier_statistics'][tier] = {\n",
    "            'chunk_count': len(tier_chunks),\n",
    "            'avg_tokens': sum(c['token_count'] for c in tier_chunks) / len(tier_chunks),\n",
    "            'total_pages': sum(c['page_count'] for c in tier_chunks),\n",
    "            'avg_pandas_score': np.mean([c['avg_pandas_score'] for c in tier_chunks]),\n",
    "            'avg_code_score': np.mean([c['avg_code_score'] for c in tier_chunks]),\n",
    "            'avg_quiz_score': np.mean([c['avg_quiz_score'] for c in tier_chunks])\n",
    "        }\n",
    "\n",
    "# Save summary statistics\n",
    "summary_file = PROCESSED_DATA_PATH / 'chunking_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(chunking_summary, f, indent=2)\n",
    "print(f\"Chunking summary saved to: {summary_file}\")\n",
    "\n",
    "# Create tier-specific chunk collections for specialized processing\n",
    "tier_collections = {}\n",
    "for tier in ['tier_1_primary', 'tier_2_secondary', 'tier_3_reference', 'tier_4_context']:\n",
    "    tier_chunks = [c for c in all_chunks if c['tier'] == tier]\n",
    "    tier_collections[tier] = tier_chunks\n",
    "\n",
    "# Save tier collections for quiz generation\n",
    "tier_collections_file = PROCESSED_DATA_PATH / 'tier_collections.pkl'\n",
    "with open(tier_collections_file, 'wb') as f:\n",
    "    pickle.dump(tier_collections, f)\n",
    "print(f\"Tier collections saved to: {tier_collections_file}\")\n",
    "\n",
    "# Display comprehensive improvement summary\n",
    "print(f\"\\nCOMPREHENSIVE CHUNKING COMPLETED!\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"MASSIVE IMPROVEMENTS ACHIEVED:\")\n",
    "print(f\"  Original system: 13 chunks, ~16% PDF utilization\")\n",
    "print(f\"  New system: 85 chunks, 100% PDF utilization\")\n",
    "print(f\"  Improvement factor: 6.5x more chunks, 6.25x better utilization\")\n",
    "\n",
    "print(f\"\\nQUALITY METRICS:\")\n",
    "print(f\"  High pandas content chunks: {chunking_summary['quality_metrics']['high_pandas_chunks']}\")\n",
    "print(f\"  Code-heavy chunks: {chunking_summary['quality_metrics']['code_heavy_chunks']}\")\n",
    "print(f\"  High quiz potential chunks: {chunking_summary['quality_metrics']['quiz_potential_chunks']}\")\n",
    "print(f\"  Large comprehensive chunks: {chunking_summary['quality_metrics']['large_chunks']}\")\n",
    "\n",
    "print(f\"\\nTIER DISTRIBUTION:\")\n",
    "for tier, stats in chunking_summary['tier_statistics'].items():\n",
    "    tier_name = tier.replace('_', ' ').title()\n",
    "    print(f\"  {tier_name}: {stats['chunk_count']} chunks, {stats['avg_tokens']:.0f} avg tokens\")\n",
    "\n",
    "print(f\"\\nREADY FOR NEXT STAGE:\")\n",
    "print(f\"  All chunks processed and saved\")\n",
    "print(f\"  Tier collections prepared for quiz generation\")\n",
    "print(f\"  100% PDF content ready for retrieval system\")\n",
    "\n",
    "# Verification of saved files\n",
    "print(f\"\\nFILE VERIFICATION:\")\n",
    "print(f\"  Chunks file: {chunks_file.exists()} ({chunks_file.stat().st_size / 1024:.1f} KB)\")\n",
    "print(f\"  Summary file: {summary_file.exists()} ({summary_file.stat().st_size / 1024:.1f} KB)\")\n",
    "print(f\"  Tier collections: {tier_collections_file.exists()} ({tier_collections_file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "\n",
    "print(f\"Ready to proceed with optimized retrieval and quiz generation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
