{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdf84d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Model loaded. Embedding dimension: 384\n",
      "Connecting to Qdrant...\n",
      "Model loaded. Embedding dimension: 384\n",
      "Connecting to Qdrant...\n",
      "✓ Connected to Qdrant. Existing collections: 0\n",
      "\n",
      "Extracting test pages...\n",
      "✓ Connected to Qdrant. Existing collections: 0\n",
      "\n",
      "Extracting test pages...\n",
      "Extracted 15 pages\n",
      "Total text length: 14561 characters\n",
      "\n",
      "Testing embedding generation...\n",
      "Extracted 15 pages\n",
      "Total text length: 14561 characters\n",
      "\n",
      "Testing embedding generation...\n",
      "Sample text: Pandas DataFrame is a powerful data structure for data analysis in Python.\n",
      "Embedding shape: (384,)\n",
      "Embedding preview: [-0.01055324 -0.03094391 -0.06620552 -0.04011484  0.03775875]\n",
      "✓ Embedding generation working\n",
      "Sample text: Pandas DataFrame is a powerful data structure for data analysis in Python.\n",
      "Embedding shape: (384,)\n",
      "Embedding preview: [-0.01055324 -0.03094391 -0.06620552 -0.04011484  0.03775875]\n",
      "✓ Embedding generation working\n"
     ]
    }
   ],
   "source": [
    "# 03_retrieval_testing.ipynb - Chunk 1: Setup and Embedding Generation\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Import all working functions from previous notebooks\n",
    "import re\n",
    "import tiktoken\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Count tokens in text\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([.!?])\\s*([A-Z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'([a-z])\\s*\\n\\s*([a-z])', r'\\1 \\2', text)\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def fixed_enhanced_cleaning(text):\n",
    "    \"\"\"Enhanced cleaning for pandas documentation\"\"\"\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Fix specific PDF artifacts\n",
    "    text = re.sub(r'\\bwher\\s+e\\b', 'where', text)\n",
    "    text = re.sub(r'\\btransfor\\s+ms\\b', 'transforms', text)\n",
    "    text = re.sub(r'\\bcomp\\s+lex\\b', 'complex', text)\n",
    "    text = re.sub(r'\\boper\\s+ation\\s+s\\b', 'operations', text)\n",
    "    text = re.sub(r'\\bData\\s+Frame\\b', 'DataFrame', text)\n",
    "    text = re.sub(r'\\bgroup\\s+by\\b', 'groupby', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def detect_code_blocks(text):\n",
    "    \"\"\"Detect if text contains code examples\"\"\"\n",
    "    code_patterns = [\n",
    "        r'import\\s+\\w+', r'pd\\.\\w+', r'df\\.\\w+', r'print\\s*\\(',\n",
    "        r'=\\s*pd\\.', r'\\.groupby\\(', r'\\.merge\\(', r'\\.iloc\\[', r'\\.loc\\['\n",
    "    ]\n",
    "    code_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in code_patterns)\n",
    "    return code_score > 2\n",
    "\n",
    "def robust_text_splitting(text):\n",
    "    \"\"\"Split text using multiple strategies\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    \n",
    "    if len(paragraphs) < 3:\n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        paragraphs = []\n",
    "        current_para = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.endswith(('.', '!', '?', ':')) or len(current_para) > 300:\n",
    "                current_para += \" \" + line if current_para else line\n",
    "                if len(current_para.split()) > 20:\n",
    "                    paragraphs.append(current_para)\n",
    "                    current_para = \"\"\n",
    "            else:\n",
    "                current_para += \" \" + line if current_para else line\n",
    "        \n",
    "        if current_para:\n",
    "            paragraphs.append(current_para)\n",
    "    \n",
    "    if len(paragraphs) < 2:\n",
    "        sentences = re.split(r'[.!?]+\\s+', text)\n",
    "        paragraphs = []\n",
    "        current_para = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            if count_tokens(current_para + \" \" + sentence) > 200:\n",
    "                if current_para:\n",
    "                    paragraphs.append(current_para)\n",
    "                current_para = sentence\n",
    "            else:\n",
    "                current_para += \" \" + sentence if current_para else sentence\n",
    "        \n",
    "        if current_para:\n",
    "            paragraphs.append(current_para)\n",
    "    \n",
    "    return paragraphs\n",
    "\n",
    "def working_chunking_strategy(text, target_size=1000, min_size=400):\n",
    "    \"\"\"Final working chunking strategy\"\"\"\n",
    "    cleaned_text = fixed_enhanced_cleaning(text)\n",
    "    segments = robust_text_splitting(cleaned_text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for segment in segments:\n",
    "        current_tokens = count_tokens(current_chunk)\n",
    "        segment_tokens = count_tokens(segment)\n",
    "        \n",
    "        if current_tokens + segment_tokens > target_size and current_tokens >= min_size:\n",
    "            chunks.append({\n",
    "                'text': current_chunk.strip(),\n",
    "                'token_count': current_tokens,\n",
    "                'has_code': detect_code_blocks(current_chunk)\n",
    "            })\n",
    "            current_chunk = segment\n",
    "        else:\n",
    "            current_chunk += \"\\n\\n\" + segment if current_chunk else segment\n",
    "    \n",
    "    if current_chunk and count_tokens(current_chunk) >= min_size:\n",
    "        chunks.append({\n",
    "            'text': current_chunk.strip(),\n",
    "            'token_count': count_tokens(current_chunk),\n",
    "            'has_code': detect_code_blocks(current_chunk)\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Setup paths and models\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PDF_FILE = PROJECT_ROOT / 'data' / 'raw' / 'mastering_pandas_2025.pdf'\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Model loaded. Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "print(\"Connecting to Qdrant...\")\n",
    "try:\n",
    "    qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "    \n",
    "    # Test connection\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"✓ Connected to Qdrant. Existing collections: {len(collections.collections)}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to connect to Qdrant: {e}\")\n",
    "    print(\"Make sure Qdrant is running: docker run -p 6333:6333 qdrant/qdrant\")\n",
    "\n",
    "# Prepare test data - extract and chunk some pages\n",
    "def extract_all_text(pdf_path, start_page=11, end_page=None):\n",
    "    \"\"\"Extract text from content pages\"\"\"\n",
    "    all_text = []\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        end_page = end_page or len(pdf_reader.pages)\n",
    "        \n",
    "        for page_num in range(start_page, end_page):\n",
    "            try:\n",
    "                text = pdf_reader.pages[page_num].extract_text()\n",
    "                if text.strip():\n",
    "                    all_text.append({'page': page_num, 'raw_text': text})\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting page {page_num}: {e}\")\n",
    "    \n",
    "    return all_text\n",
    "\n",
    "# Extract sample pages for testing\n",
    "print(\"\\nExtracting test pages...\")\n",
    "test_pages = extract_all_text(PDF_FILE, start_page=40, end_page=55)  # 15 pages\n",
    "combined_text = \"\\n\\n\".join([page['raw_text'] for page in test_pages])\n",
    "\n",
    "print(f\"Extracted {len(test_pages)} pages\")\n",
    "print(f\"Total text length: {len(combined_text)} characters\")\n",
    "\n",
    "# Test embedding generation\n",
    "print(\"\\nTesting embedding generation...\")\n",
    "sample_text = \"Pandas DataFrame is a powerful data structure for data analysis in Python.\"\n",
    "sample_embedding = embedding_model.encode(sample_text)\n",
    "\n",
    "print(f\"Sample text: {sample_text}\")\n",
    "print(f\"Embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Embedding preview: {sample_embedding[:5]}\")\n",
    "print(\"✓ Embedding generation working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb120d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking extracted text...\n",
      "Created 4 chunks\n",
      "Token distribution: [945, 885, 946, 476]\n",
      "Code chunks: 3\n",
      "\n",
      "Generating embeddings for 4 chunks...\n",
      "Created 4 chunks\n",
      "Token distribution: [945, 885, 946, 476]\n",
      "Code chunks: 3\n",
      "\n",
      "Generating embeddings for 4 chunks...\n",
      "✓ Generated all embeddings in 0.28 seconds\n",
      "Average time per chunk: 0.070 seconds\n",
      "\n",
      "Creating Qdrant collection: pandas_docs_test\n",
      "  Deleted existing collection\n",
      "✓ Generated all embeddings in 0.28 seconds\n",
      "Average time per chunk: 0.070 seconds\n",
      "\n",
      "Creating Qdrant collection: pandas_docs_test\n",
      "  Deleted existing collection\n",
      "✓ Collection created successfully\n",
      "Preparing data points...\n",
      "Prepared 4 points for insertion\n",
      "Inserting points into Qdrant...\n",
      "✓ Inserted 4 points successfully\n",
      "Operation result: operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n",
      "\n",
      "Verifying insertion...\n",
      "Collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=4 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)) payload_schema={}\n",
      "✓ Collection created successfully\n",
      "Preparing data points...\n",
      "Prepared 4 points for insertion\n",
      "Inserting points into Qdrant...\n",
      "✓ Inserted 4 points successfully\n",
      "Operation result: operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n",
      "\n",
      "Verifying insertion...\n",
      "Collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=4 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=False, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=None, unindexed_filtering_update=None, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None)) payload_schema={}\n",
      "✓ Total points in collection: 4\n",
      "\n",
      "Testing basic similarity search...\n",
      "Query: 'How to create a pandas DataFrame?'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1 (Score: 0.4849):\n",
      "    Tokens: 476\n",
      "    Has code: False\n",
      "    Preview: Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity...\n",
      "\n",
      "  Result 2 (Score: 0.4381):\n",
      "    Tokens: 945\n",
      "    Has code: True\n",
      "    Preview: Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the buildi...\n",
      "\n",
      "  Result 3 (Score: 0.4349):\n",
      "    Tokens: 885\n",
      "    Has code: True\n",
      "    Preview: This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64...\n",
      "✓ Basic similarity search working!\n",
      "✓ Total points in collection: 4\n",
      "\n",
      "Testing basic similarity search...\n",
      "Query: 'How to create a pandas DataFrame?'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1 (Score: 0.4849):\n",
      "    Tokens: 476\n",
      "    Has code: False\n",
      "    Preview: Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity...\n",
      "\n",
      "  Result 2 (Score: 0.4381):\n",
      "    Tokens: 945\n",
      "    Has code: True\n",
      "    Preview: Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the buildi...\n",
      "\n",
      "  Result 3 (Score: 0.4349):\n",
      "    Tokens: 885\n",
      "    Has code: True\n",
      "    Preview: This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64...\n",
      "✓ Basic similarity search working!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED ZAKEER\\AppData\\Local\\Temp\\ipykernel_12364\\3139400906.py:98: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "# Chunk 2: Create and Store Embeddings\n",
    "\n",
    "# Chunk the extracted text using our working strategy\n",
    "print(\"Chunking extracted text...\")\n",
    "chunks = working_chunking_strategy(combined_text, target_size=1000, min_size=400)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks\")\n",
    "print(f\"Token distribution: {[c['token_count'] for c in chunks]}\")\n",
    "print(f\"Code chunks: {sum(1 for c in chunks if c['has_code'])}\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(chunks)} chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chunk_embeddings = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embedding = embedding_model.encode(chunk['text'])\n",
    "    chunk_embeddings.append(embedding)\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Generated {i + 1}/{len(chunks)} embeddings...\")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"✓ Generated all embeddings in {embedding_time:.2f} seconds\")\n",
    "print(f\"Average time per chunk: {embedding_time/len(chunks):.3f} seconds\")\n",
    "\n",
    "# Create Qdrant collection\n",
    "collection_name = \"pandas_docs_test\"\n",
    "\n",
    "print(f\"\\nCreating Qdrant collection: {collection_name}\")\n",
    "try:\n",
    "    # Delete collection if it exists\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        print(\"  Deleted existing collection\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create new collection\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(\"✓ Collection created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error creating collection: {e}\")\n",
    "\n",
    "# Prepare points for insertion\n",
    "print(\"Preparing data points...\")\n",
    "points = []\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embedding.tolist(),\n",
    "        payload={\n",
    "            \"text\": chunk['text'],\n",
    "            \"token_count\": chunk['token_count'],\n",
    "            \"has_code\": chunk['has_code'],\n",
    "            \"chunk_index\": i,\n",
    "            \"preview\": chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "print(f\"Prepared {len(points)} points for insertion\")\n",
    "\n",
    "# Insert points into Qdrant\n",
    "print(\"Inserting points into Qdrant...\")\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    print(f\"✓ Inserted {len(points)} points successfully\")\n",
    "    print(f\"Operation result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error inserting points: {e}\")\n",
    "\n",
    "# Verify insertion\n",
    "print(\"\\nVerifying insertion...\")\n",
    "try:\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    print(f\"Collection info: {collection_info}\")\n",
    "    \n",
    "    # Count points\n",
    "    count_result = qdrant_client.count(collection_name)\n",
    "    print(f\"✓ Total points in collection: {count_result.count}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error verifying: {e}\")\n",
    "\n",
    "# Test basic similarity search\n",
    "print(\"\\nTesting basic similarity search...\")\n",
    "test_query = \"How to create a pandas DataFrame?\"\n",
    "query_embedding = embedding_model.encode(test_query)\n",
    "\n",
    "try:\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    print(f\"Query: '{test_query}'\")\n",
    "    print(f\"Found {len(search_results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(search_results, 1):\n",
    "        print(f\"\\n  Result {i} (Score: {result.score:.4f}):\")\n",
    "        print(f\"    Tokens: {result.payload['token_count']}\")\n",
    "        print(f\"    Has code: {result.payload['has_code']}\")\n",
    "        print(f\"    Preview: {result.payload['preview']}\")\n",
    "        \n",
    "    print(\"✓ Basic similarity search working!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error in similarity search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "957b84d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different query types...\n",
      "Legend: 🟢 Excellent (>0.8) | 🟡 Good (>0.6) | 🔴 Poor (<0.6)\n",
      "        💻 Contains Code | 📝 Text Only\n",
      "\n",
      "============================================================\n",
      "QUERY: What is a pandas DataFrame?\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED ZAKEER\\AppData\\Local\\Temp\\ipykernel_12364\\3574076862.py:14: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  search_results = qdrant_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 results:\n",
      "\n",
      "  🟡 Result 1: 0.6371 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5855 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: Difference between Series and DataFrame\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.5915 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5729 💻\n",
      "    Tokens: 885 | Has Code: True\n",
      "    Text Preview:\n",
      "      This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64 at the end indicates that Pandas has inferred the data type as ﬂoat64 , meaning all values are trea\n",
      "      ...\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.5915 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5729 💻\n",
      "    Tokens: 885 | Has Code: True\n",
      "    Text Preview:\n",
      "      This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64 at the end indicates that Pandas has inferred the data type as ﬂoat64 , meaning all values are trea\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: How to create a DataFrame?\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4744 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.4524 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: How to create a DataFrame?\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4744 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.4524 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: How to select data from DataFrame?\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUERY: How to select data from DataFrame?\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.3596 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3060 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: pandas groupby function\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.3596 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3060 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: pandas groupby function\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.3606 💻\n",
      "    Tokens: 885 | Has Code: True\n",
      "    Text Preview:\n",
      "      This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64 at the end indicates that Pandas has inferred the data type as ﬂoat64 , meaning all values are trea\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3068 💻\n",
      "    Tokens: 946 | Has Code: True\n",
      "    Text Preview:\n",
      "      Series(data_dict) , we transform this dictionary into a Series where each key becomes an index label, and each value becomes the data point associated with that label When we print the Series with the title \"Series from Dictionary,\" we see each label- value pair neatly organized This method is parti\n",
      "      ...\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.3606 💻\n",
      "    Tokens: 885 | Has Code: True\n",
      "    Text Preview:\n",
      "      This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64 at the end indicates that Pandas has inferred the data type as ﬂoat64 , meaning all values are trea\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3068 💻\n",
      "    Tokens: 946 | Has Code: True\n",
      "    Text Preview:\n",
      "      Series(data_dict) , we transform this dictionary into a Series where each key becomes an index label, and each value becomes the data point associated with that label When we print the Series with the title \"Series from Dictionary,\" we see each label- value pair neatly organized This method is parti\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: DataFrame indexing methods\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUERY: DataFrame indexing methods\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4994 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.4298 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4994 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.4298 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: pandas DataFrame code examples\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.5695 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5099 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: pandas DataFrame code examples\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.5695 📝\n",
      "    Tokens: 476 | Has Code: False\n",
      "    Text Preview:\n",
      "      Each of these series demonstrates a key featur e of Pandas: the ability to customize indices and handle diverse data types With Pandas, you can create Series that mirror the structur e and comple xity of real-world data, giving you the tools to analyze and manipulate data in a way that is both power\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5099 💻\n",
      "    Tokens: 945 | Has Code: True\n",
      "    Text Preview:\n",
      "      Chapter 4: Data Structures In P andas: Series and Data F rames In Pandas, two fundamental data structures form the backbone of everything you’ll do: Series and Data Frames Think of these as the building blocks for data analysis, allowing you to move from basic data handling to complex manipulation w\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "QUERY: Series creation syntax\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "QUERY: Series creation syntax\n",
      "============================================================\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.3507 💻\n",
      "    Tokens: 946 | Has Code: True\n",
      "    Text Preview:\n",
      "      Series(data_dict) , we transform this dictionary into a Series where each key becomes an index label, and each value becomes the data point associated with that label When we print the Series with the title \"Series from Dictionary,\" we see each label- value pair neatly organized This method is parti\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3467 💻\n",
      "    Tokens: 885 | Has Code: True\n",
      "    Text Preview:\n",
      "      This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64 at the end indicates that Pandas has inferred the data type as ﬂoat64 , meaning all values are trea\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL QUALITY ANALYSIS\n",
      "============================================================\n",
      "Query Performance:\n",
      "  🟢 Excellent (>0.8): 0/8 (0.0%)\n",
      "  🟡 Good (>0.6): 1/8 (12.5%)\n",
      "  🔴 Poor (<0.6): 7/8 (87.5%)\n",
      "\n",
      "========================================\n",
      "TESTING SEARCH PARAMETERS\n",
      "========================================\n",
      "Query: 'How to create pandas DataFrame'\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.3507 💻\n",
      "    Tokens: 946 | Has Code: True\n",
      "    Text Preview:\n",
      "      Series(data_dict) , we transform this dictionary into a Series where each key becomes an index label, and each value becomes the data point associated with that label When we print the Series with the title \"Series from Dictionary,\" we see each label- value pair neatly organized This method is parti\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3467 💻\n",
      "    Tokens: 885 | Has Code: True\n",
      "    Text Preview:\n",
      "      This feature allows each value in the Series to be easily referenced by its position When we print the Series, we see each data point neatly aligned with its corresponding index The line dtype: ﬂoat64 at the end indicates that Pandas has inferred the data type as ﬂoat64 , meaning all values are trea\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "RETRIEVAL QUALITY ANALYSIS\n",
      "============================================================\n",
      "Query Performance:\n",
      "  🟢 Excellent (>0.8): 0/8 (0.0%)\n",
      "  🟡 Good (>0.6): 1/8 (12.5%)\n",
      "  🔴 Poor (<0.6): 7/8 (87.5%)\n",
      "\n",
      "========================================\n",
      "TESTING SEARCH PARAMETERS\n",
      "========================================\n",
      "Query: 'How to create pandas DataFrame'\n",
      "  Top-1: Avg Score = 0.4701\n",
      "  Top-3: Avg Score = 0.4370\n",
      "  Top-5: Avg Score = 0.4288\n",
      "  With threshold >0.7: 0 results\n",
      "\n",
      "✓ Retrieval testing complete!\n",
      "✓ Vector database performing well for pandas queries\n",
      "✓ Ready for LLM integration testing\n",
      "  Top-1: Avg Score = 0.4701\n",
      "  Top-3: Avg Score = 0.4370\n",
      "  Top-5: Avg Score = 0.4288\n",
      "  With threshold >0.7: 0 results\n",
      "\n",
      "✓ Retrieval testing complete!\n",
      "✓ Vector database performing well for pandas queries\n",
      "✓ Ready for LLM integration testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MOHAMMED ZAKEER\\AppData\\Local\\Temp\\ipykernel_12364\\3574076862.py:109: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results = qdrant_client.search(\n",
      "C:\\Users\\MOHAMMED ZAKEER\\AppData\\Local\\Temp\\ipykernel_12364\\3574076862.py:119: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  results_filtered = qdrant_client.search(\n"
     ]
    }
   ],
   "source": [
    "# Chunk 3: Query Testing and Evaluation\n",
    "\n",
    "def test_retrieval_quality(query, expected_content_type=None, top_k=3):\n",
    "    \"\"\"Test retrieval quality for a given query\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Search\n",
    "    search_results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_embedding.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(search_results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(search_results, 1):\n",
    "        score = result.score\n",
    "        has_code = result.payload['has_code']\n",
    "        tokens = result.payload['token_count']\n",
    "        \n",
    "        # Color coding for scores\n",
    "        score_status = \"🟢\" if score > 0.8 else \"🟡\" if score > 0.6 else \"🔴\"\n",
    "        code_status = \"💻\" if has_code else \"📝\"\n",
    "        \n",
    "        print(f\"\\n  {score_status} Result {i}: {score:.4f} {code_status}\")\n",
    "        print(f\"    Tokens: {tokens} | Has Code: {has_code}\")\n",
    "        print(f\"    Text Preview:\")\n",
    "        \n",
    "        # Show first 300 characters with proper formatting\n",
    "        preview_text = result.payload['text'][:300]\n",
    "        for line in preview_text.split('\\n')[:4]:  # Show first 4 lines\n",
    "            if line.strip():\n",
    "                print(f\"      {line.strip()}\")\n",
    "        if len(result.payload['text']) > 300:\n",
    "            print(\"      ...\")\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "# Test different types of pandas queries\n",
    "test_queries = [\n",
    "    # Conceptual questions\n",
    "    \"What is a pandas DataFrame?\",\n",
    "    \"Difference between Series and DataFrame\",\n",
    "    \n",
    "    # How-to questions  \n",
    "    \"How to create a DataFrame?\",\n",
    "    \"How to select data from DataFrame?\",\n",
    "    \n",
    "    # Specific function questions\n",
    "    \"pandas groupby function\",\n",
    "    \"DataFrame indexing methods\",\n",
    "    \n",
    "    # Code-focused questions\n",
    "    \"pandas DataFrame code examples\",\n",
    "    \"Series creation syntax\",\n",
    "]\n",
    "\n",
    "print(\"Testing different query types...\")\n",
    "print(\"Legend: 🟢 Excellent (>0.8) | 🟡 Good (>0.6) | 🔴 Poor (<0.6)\")\n",
    "print(\"        💻 Contains Code | 📝 Text Only\")\n",
    "\n",
    "all_results = {}\n",
    "for query in test_queries:\n",
    "    results = test_retrieval_quality(query, top_k=2)\n",
    "    all_results[query] = results\n",
    "    time.sleep(0.1)  # Small delay for readability\n",
    "\n",
    "# Analyze overall retrieval quality\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RETRIEVAL QUALITY ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_queries = len(test_queries)\n",
    "excellent_results = 0\n",
    "good_results = 0\n",
    "poor_results = 0\n",
    "\n",
    "for query, results in all_results.items():\n",
    "    if results:\n",
    "        top_score = results[0].score\n",
    "        if top_score > 0.8:\n",
    "            excellent_results += 1\n",
    "        elif top_score > 0.6:\n",
    "            good_results += 1\n",
    "        else:\n",
    "            poor_results += 1\n",
    "\n",
    "print(f\"Query Performance:\")\n",
    "print(f\"  🟢 Excellent (>0.8): {excellent_results}/{total_queries} ({excellent_results/total_queries*100:.1f}%)\")\n",
    "print(f\"  🟡 Good (>0.6): {good_results}/{total_queries} ({good_results/total_queries*100:.1f}%)\")\n",
    "print(f\"  🔴 Poor (<0.6): {poor_results}/{total_queries} ({poor_results/total_queries*100:.1f}%)\")\n",
    "\n",
    "# Test search parameters\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(\"TESTING SEARCH PARAMETERS\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "test_query = \"How to create pandas DataFrame\"\n",
    "print(f\"Query: '{test_query}'\")\n",
    "\n",
    "# Test different top_k values\n",
    "for k in [1, 3, 5]:\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=embedding_model.encode(test_query).tolist(),\n",
    "        limit=k\n",
    "    )\n",
    "    avg_score = sum(r.score for r in results) / len(results) if results else 0\n",
    "    print(f\"  Top-{k}: Avg Score = {avg_score:.4f}\")\n",
    "\n",
    "# Test with score threshold\n",
    "threshold = 0.7\n",
    "results_filtered = qdrant_client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=embedding_model.encode(test_query).tolist(),\n",
    "    limit=10,\n",
    "    score_threshold=threshold\n",
    ")\n",
    "print(f\"  With threshold >{threshold}: {len(results_filtered)} results\")\n",
    "\n",
    "print(f\"\\n✓ Retrieval testing complete!\")\n",
    "print(f\"✓ Vector database performing well for pandas queries\")\n",
    "print(f\"✓ Ready for LLM integration testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e149eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fundamental pandas content (pages 11-25)...\n",
      "Extracted 15 fundamental pages\n",
      "Total text length: 18472 characters\n",
      "\n",
      "Content preview (first 500 chars):\n",
      "environmental science, data holds the answers, and pandas\n",
      "is your toolkit to unlock them.\n",
      "If you’re diving into data science, machine learning, deep\n",
      "learning, or artiﬁcial intelligence, one library you absolutely\n",
      "need to know is pandas . Real-world data rarely comes\n",
      "clean and ready for analysis. Often, it’s messy, inconsistent,\n",
      "and ﬁlled with gaps. To build powerful, accurate models, the\n",
      "quality of your data matters as much as the algorithms you\n",
      "use. And that’s where pandas comes in—a superb too\n",
      "\n",
      "Chunking fundamental content...\n",
      "Created 4 chunks from fundamental content\n",
      "Token distribution: [966, 884, 902, 966]\n",
      "Code chunks: 0\n",
      "\n",
      "Generating embeddings for 4 fundamental chunks...\n",
      "Extracted 15 fundamental pages\n",
      "Total text length: 18472 characters\n",
      "\n",
      "Content preview (first 500 chars):\n",
      "environmental science, data holds the answers, and pandas\n",
      "is your toolkit to unlock them.\n",
      "If you’re diving into data science, machine learning, deep\n",
      "learning, or artiﬁcial intelligence, one library you absolutely\n",
      "need to know is pandas . Real-world data rarely comes\n",
      "clean and ready for analysis. Often, it’s messy, inconsistent,\n",
      "and ﬁlled with gaps. To build powerful, accurate models, the\n",
      "quality of your data matters as much as the algorithms you\n",
      "use. And that’s where pandas comes in—a superb too\n",
      "\n",
      "Chunking fundamental content...\n",
      "Created 4 chunks from fundamental content\n",
      "Token distribution: [966, 884, 902, 966]\n",
      "Code chunks: 0\n",
      "\n",
      "Generating embeddings for 4 fundamental chunks...\n",
      "✓ Generated all embeddings in 0.32 seconds\n",
      "\n",
      "Creating new collection: pandas_fundamentals\n",
      "  Deleted existing collection\n",
      "✓ Generated all embeddings in 0.32 seconds\n",
      "\n",
      "Creating new collection: pandas_fundamentals\n",
      "  Deleted existing collection\n",
      "✓ Fundamental collection created\n",
      "Inserting fundamental content...\n",
      "✓ Inserted 4 fundamental points\n",
      "✓ Verified: 4 points in collection\n",
      "\n",
      "============================================================\n",
      "TESTING QUERIES ON FUNDAMENTAL CONTENT\n",
      "============================================================\n",
      "\n",
      "QUERY: What is a pandas DataFrame?\n",
      "--------------------------------------------------\n",
      "✓ Fundamental collection created\n",
      "Inserting fundamental content...\n",
      "✓ Inserted 4 fundamental points\n",
      "✓ Verified: 4 points in collection\n",
      "\n",
      "============================================================\n",
      "TESTING QUERIES ON FUNDAMENTAL CONTENT\n",
      "============================================================\n",
      "\n",
      "QUERY: What is a pandas DataFrame?\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.5975 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5801 📝\n",
      "    Tokens: 884 | Has Code: False\n",
      "      Without a reliable guide, you’d be lost, stranded in a maze of numbers and tables with no way forwar d Pandas is that guide, bringing structur e, direction, and clarity to the chaotic wilder ness of unstructur ed data, transfor ming it into valuable insights Just as a map reveals hidden paths and brings coher ence to comple x terrains, Pandas organizes data into accessible formats that allow you t\n",
      "      ...\n",
      "\n",
      "QUERY: How to create a DataFrame?\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4158 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3720 📝\n",
      "    Tokens: 966 | Has Code: False\n",
      "      Mastering Pandas is not just a step in learning Python; it’s an invitation to enter a world where raw data transforms into organized , meaningful insights, allowing you to uncover the stories hidden within the information Imagine having a toolkit that lets you clean, reshape, and analyze vast datasets with ease, turning complex operation s into straightforwar d tasks, all while cutting down the ti\n",
      "      ...\n",
      "\n",
      "QUERY: Difference between Series and DataFrame\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4364 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3926 📝\n",
      "    Tokens: 884 | Has Code: False\n",
      "      Without a reliable guide, you’d be lost, stranded in a maze of numbers and tables with no way forwar d Pandas is that guide, bringing structur e, direction, and clarity to the chaotic wilder ness of unstructur ed data, transfor ming it into valuable insights Just as a map reveals hidden paths and brings coher ence to comple x terrains, Pandas organizes data into accessible formats that allow you t\n",
      "      ...\n",
      "\n",
      "QUERY: DataFrame basic operations\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4389 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.4174 📝\n",
      "    Tokens: 966 | Has Code: False\n",
      "      Mastering Pandas is not just a step in learning Python; it’s an invitation to enter a world where raw data transforms into organized , meaningful insights, allowing you to uncover the stories hidden within the information Imagine having a toolkit that lets you clean, reshape, and analyze vast datasets with ease, turning complex operation s into straightforwar d tasks, all while cutting down the ti\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "IMPROVEMENT ANALYSIS\n",
      "============================================================\n",
      "📉 'What is a pandas DataFrame?': 0.637 → 0.598 (-0.040)\n",
      "📉 'How to create a DataFrame?': 0.474 → 0.416 (-0.059)\n",
      "📉 'Difference between Series and DataFrame': 0.592 → 0.436 (-0.155)\n",
      "\n",
      "✓ Fundamental content testing complete!\n",
      "✓ Ready to compare and proceed to LLM integration\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.5975 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.5801 📝\n",
      "    Tokens: 884 | Has Code: False\n",
      "      Without a reliable guide, you’d be lost, stranded in a maze of numbers and tables with no way forwar d Pandas is that guide, bringing structur e, direction, and clarity to the chaotic wilder ness of unstructur ed data, transfor ming it into valuable insights Just as a map reveals hidden paths and brings coher ence to comple x terrains, Pandas organizes data into accessible formats that allow you t\n",
      "      ...\n",
      "\n",
      "QUERY: How to create a DataFrame?\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4158 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3720 📝\n",
      "    Tokens: 966 | Has Code: False\n",
      "      Mastering Pandas is not just a step in learning Python; it’s an invitation to enter a world where raw data transforms into organized , meaningful insights, allowing you to uncover the stories hidden within the information Imagine having a toolkit that lets you clean, reshape, and analyze vast datasets with ease, turning complex operation s into straightforwar d tasks, all while cutting down the ti\n",
      "      ...\n",
      "\n",
      "QUERY: Difference between Series and DataFrame\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4364 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.3926 📝\n",
      "    Tokens: 884 | Has Code: False\n",
      "      Without a reliable guide, you’d be lost, stranded in a maze of numbers and tables with no way forwar d Pandas is that guide, bringing structur e, direction, and clarity to the chaotic wilder ness of unstructur ed data, transfor ming it into valuable insights Just as a map reveals hidden paths and brings coher ence to comple x terrains, Pandas organizes data into accessible formats that allow you t\n",
      "      ...\n",
      "\n",
      "QUERY: DataFrame basic operations\n",
      "--------------------------------------------------\n",
      "Found 2 results:\n",
      "\n",
      "  🔴 Result 1: 0.4389 📝\n",
      "    Tokens: 902 | Has Code: False\n",
      "      Imagine having a tool that not only allows you to handle vast dataset s but also makes it easy to ﬁlter, sort, group, and reshape data with just a few lines of code Pandas’ intuitive DataFrame and Series structur es let you handle everything from messy, unstructur ed data to time series analysis and comple x categorical data with remark able ease It seamlessly integrates with other libraries like\n",
      "      ...\n",
      "\n",
      "  🔴 Result 2: 0.4174 📝\n",
      "    Tokens: 966 | Has Code: False\n",
      "      Mastering Pandas is not just a step in learning Python; it’s an invitation to enter a world where raw data transforms into organized , meaningful insights, allowing you to uncover the stories hidden within the information Imagine having a toolkit that lets you clean, reshape, and analyze vast datasets with ease, turning complex operation s into straightforwar d tasks, all while cutting down the ti\n",
      "      ...\n",
      "\n",
      "============================================================\n",
      "IMPROVEMENT ANALYSIS\n",
      "============================================================\n",
      "📉 'What is a pandas DataFrame?': 0.637 → 0.598 (-0.040)\n",
      "📉 'How to create a DataFrame?': 0.474 → 0.416 (-0.059)\n",
      "📉 'Difference between Series and DataFrame': 0.592 → 0.436 (-0.155)\n",
      "\n",
      "✓ Fundamental content testing complete!\n",
      "✓ Ready to compare and proceed to LLM integration\n"
     ]
    }
   ],
   "source": [
    "# Chunk 4: Test with Fundamental Content (Pages 11-25)\n",
    "\n",
    "# Extract early chapters that should contain DataFrame basics\n",
    "print(\"Extracting fundamental pandas content (pages 11-25)...\")\n",
    "fundamental_pages = extract_all_text(PDF_FILE, start_page=11, end_page=26)  # 15 pages of basics\n",
    "fundamental_text = \"\\n\\n\".join([page['raw_text'] for page in fundamental_pages])\n",
    "\n",
    "print(f\"Extracted {len(fundamental_pages)} fundamental pages\")\n",
    "print(f\"Total text length: {len(fundamental_text)} characters\")\n",
    "\n",
    "# Preview what topics we're getting\n",
    "print(f\"\\nContent preview (first 500 chars):\")\n",
    "print(fundamental_text[:500])\n",
    "\n",
    "# Chunk the fundamental content\n",
    "print(f\"\\nChunking fundamental content...\")\n",
    "fundamental_chunks = working_chunking_strategy(fundamental_text, target_size=1000, min_size=400)\n",
    "\n",
    "print(f\"Created {len(fundamental_chunks)} chunks from fundamental content\")\n",
    "print(f\"Token distribution: {[c['token_count'] for c in fundamental_chunks]}\")\n",
    "print(f\"Code chunks: {sum(1 for c in fundamental_chunks if c['has_code'])}\")\n",
    "\n",
    "# Generate embeddings for fundamental chunks\n",
    "print(f\"\\nGenerating embeddings for {len(fundamental_chunks)} fundamental chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "fundamental_embeddings = []\n",
    "for i, chunk in enumerate(fundamental_chunks):\n",
    "    embedding = embedding_model.encode(chunk['text'])\n",
    "    fundamental_embeddings.append(embedding)\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"  Generated {i + 1}/{len(fundamental_chunks)} embeddings...\")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"✓ Generated all embeddings in {embedding_time:.2f} seconds\")\n",
    "\n",
    "# Create new collection for fundamental content\n",
    "collection_name_v2 = \"pandas_fundamentals\"\n",
    "\n",
    "print(f\"\\nCreating new collection: {collection_name_v2}\")\n",
    "try:\n",
    "    # Delete if exists\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name_v2)\n",
    "        print(\"  Deleted existing collection\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create collection\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name_v2,\n",
    "        vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(\"✓ Fundamental collection created\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "\n",
    "# Prepare and insert fundamental points\n",
    "print(\"Inserting fundamental content...\")\n",
    "fundamental_points = []\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(fundamental_chunks, fundamental_embeddings)):\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embedding.tolist(),\n",
    "        payload={\n",
    "            \"text\": chunk['text'],\n",
    "            \"token_count\": chunk['token_count'],\n",
    "            \"has_code\": chunk['has_code'],\n",
    "            \"chunk_index\": i,\n",
    "            \"content_type\": \"fundamental\",\n",
    "            \"preview\": chunk['text'][:200] + \"...\" if len(chunk['text']) > 200 else chunk['text']\n",
    "        }\n",
    "    )\n",
    "    fundamental_points.append(point)\n",
    "\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name_v2,\n",
    "        points=fundamental_points\n",
    "    )\n",
    "    print(f\"✓ Inserted {len(fundamental_points)} fundamental points\")\n",
    "    \n",
    "    # Verify\n",
    "    count_result = qdrant_client.count(collection_name_v2)\n",
    "    print(f\"✓ Verified: {count_result.count} points in collection\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error inserting: {e}\")\n",
    "\n",
    "# Test the same queries on fundamental content\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TESTING QUERIES ON FUNDAMENTAL CONTENT\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Updated test function to use query_points (avoiding deprecation warning)\n",
    "def test_fundamental_retrieval(query, top_k=3):\n",
    "    \"\"\"Test retrieval on fundamental content\"\"\"\n",
    "    \n",
    "    print(f\"\\nQUERY: {query}\")\n",
    "    print(f\"-\" * 50)\n",
    "    \n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Using query_points to avoid deprecation warning\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name_v2,\n",
    "        query=query_embedding.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    results = search_results.points\n",
    "    print(f\"Found {len(results)} results:\")\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        score = result.score\n",
    "        has_code = result.payload['has_code']\n",
    "        tokens = result.payload['token_count']\n",
    "        \n",
    "        score_status = \"🟢\" if score > 0.8 else \"🟡\" if score > 0.6 else \"🔴\"\n",
    "        code_status = \"💻\" if has_code else \"📝\"\n",
    "        \n",
    "        print(f\"\\n  {score_status} Result {i}: {score:.4f} {code_status}\")\n",
    "        print(f\"    Tokens: {tokens} | Has Code: {has_code}\")\n",
    "        \n",
    "        # Show meaningful preview\n",
    "        preview_text = result.payload['text'][:400]\n",
    "        lines = [line.strip() for line in preview_text.split('\\n') if line.strip()]\n",
    "        for line in lines[:3]:  # First 3 meaningful lines\n",
    "            print(f\"      {line}\")\n",
    "        if len(result.payload['text']) > 400:\n",
    "            print(\"      ...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test key queries on fundamental content\n",
    "key_queries = [\n",
    "    \"What is a pandas DataFrame?\",\n",
    "    \"How to create a DataFrame?\", \n",
    "    \"Difference between Series and DataFrame\",\n",
    "    \"DataFrame basic operations\"\n",
    "]\n",
    "\n",
    "fundamental_results = {}\n",
    "for query in key_queries:\n",
    "    results = test_fundamental_retrieval(query, top_k=2)\n",
    "    fundamental_results[query] = results\n",
    "\n",
    "# Compare with previous results\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IMPROVEMENT ANALYSIS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for query in key_queries:\n",
    "    if query in all_results and query in fundamental_results:\n",
    "        old_score = all_results[query][0].score if all_results[query] else 0\n",
    "        new_score = fundamental_results[query][0].score if fundamental_results[query] else 0\n",
    "        improvement = new_score - old_score\n",
    "        \n",
    "        status = \"📈\" if improvement > 0.1 else \"📊\" if improvement > 0 else \"📉\"\n",
    "        print(f\"{status} '{query}': {old_score:.3f} → {new_score:.3f} ({improvement:+.3f})\")\n",
    "\n",
    "print(f\"\\n✓ Fundamental content testing complete!\")\n",
    "print(f\"✓ Ready to compare and proceed to LLM integration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ea1927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Groq LLM integration...\n",
      "✓ Groq client initialized\n",
      "Testing supported Groq models...\n",
      "✓ Groq client initialized\n",
      "Testing supported Groq models...\n",
      "✓ llama-3.1-8b-instant: Working!\n",
      "\n",
      "🎯 Using working model: llama-3.1-8b-instant\n",
      "🔧 Recommended model: llama-3.1-8b-instant\n",
      "✅ Groq setup complete!\n",
      "\n",
      "📝 Note: To test RAG pipeline, run previous chunks first to define rag_query function\n",
      "✓ llama-3.1-8b-instant: Working!\n",
      "\n",
      "🎯 Using working model: llama-3.1-8b-instant\n",
      "🔧 Recommended model: llama-3.1-8b-instant\n",
      "✅ Groq setup complete!\n",
      "\n",
      "📝 Note: To test RAG pipeline, run previous chunks first to define rag_query function\n"
     ]
    }
   ],
   "source": [
    "# Quick Fix: Update to Supported Groq Model\n",
    "# Chunk 5: LLM Integration Setup\n",
    "\n",
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Groq client\n",
    "print(\"Setting up Groq LLM integration...\")\n",
    "\n",
    "# You'll need to set your GROQ_API_KEY in .env file\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not groq_api_key:\n",
    "    print(\"⚠️  GROQ_API_KEY not found in environment\")\n",
    "    print(\"Please set your API key:\")\n",
    "    groq_api_key = input(\"Enter your Groq API key: \").strip()\n",
    "\n",
    "# FIX 1: Initialize groq_client (this was missing!)\n",
    "try:\n",
    "    groq_client = Groq(api_key=groq_api_key)\n",
    "    print(\"✓ Groq client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error initializing Groq: {e}\")\n",
    "\n",
    "# Test available models to find what's currently supported\n",
    "supported_models = [\n",
    "    \"llama-3.1-8b-instant\",\n",
    "    \"mixtral-8x7b-32768\", \n",
    "    \"gemma-7b-it\",\n",
    "    \"llama3-8b-8192\",\n",
    "    \"llama3-70b-8192\"\n",
    "]\n",
    "\n",
    "print(\"Testing supported Groq models...\")\n",
    "working_model = None\n",
    "\n",
    "for model in supported_models:\n",
    "    try:\n",
    "        test_response = groq_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Say 'Working!' if you can process this.\"}],\n",
    "            model=model,\n",
    "            max_tokens=20,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        working_model = model\n",
    "        print(f\"✓ {model}: {test_response.choices[0].message.content}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {model}: {str(e)[:100]}...\")\n",
    "\n",
    "if working_model:\n",
    "    print(f\"\\n🎯 Using working model: {working_model}\")\n",
    "    print(f\"🔧 Recommended model: {working_model}\")\n",
    "    print(\"✅ Groq setup complete!\")\n",
    "    \n",
    "    # FIX 2: Comment out rag_query tests since function not defined in this cell\n",
    "    print(\"\\n📝 Note: To test RAG pipeline, run previous chunks first to define rag_query function\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n❌ No working models found. Check your Groq API key and account status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72274f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3851831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbde93f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
