{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd3291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\mygame\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed chunks...\n",
      "Loaded 13 chunks\n",
      "Average tokens per chunk: 1160.5\n",
      "\n",
      "Initializing embedding model...\n",
      "Model loaded. Embedding dimension: 384\n",
      "\n",
      "Generating embeddings for 13 chunks...\n",
      "  Generated 5/13 embeddings\n",
      "  Generated 10/13 embeddings\n",
      "  Generated 13/13 embeddings\n",
      "Embeddings generated in 1.29 seconds\n",
      "Average time per chunk: 0.099 seconds\n",
      "\n",
      "Connecting to Qdrant...\n",
      "Connected to Qdrant. Existing collections: 0\n",
      "\n",
      "Creating collection: pandas_docs_improved\n",
      "  Deleted existing collection\n",
      "  Collection created successfully\n",
      "\n",
      "Preparing data points...\n",
      "Prepared 13 points for insertion\n",
      "Inserting points into Qdrant...\n",
      "  Inserted 13 points successfully\n",
      "  Operation result: operation_id=0 status=<UpdateStatus.COMPLETED: 'completed'>\n",
      "\n",
      "Verifying insertion...\n",
      "  Total points in collection: 13\n",
      "  Verification successful\n",
      "\n",
      "Running comprehensive retrieval evaluation...\n",
      "Testing 18 queries with improved chunks:\n",
      "   1. Good      (0.615) - What is a pandas DataFrame?\n",
      "   2. Fair      (0.578) - What is a pandas Series?\n",
      "   3. Fair      (0.439) - Difference between Series and DataFrame\n",
      "   4. Fair      (0.476) - How to create a DataFrame?\n",
      "   5. Fair      (0.538) - How to read CSV files in pandas?\n",
      "   6. Fair      (0.455) - How to select data from DataFrame?\n",
      "   7. Fair      (0.534) - How to filter DataFrame rows?\n",
      "   8. Poor      (0.378) - pandas groupby function\n",
      "   9. Fair      (0.452) - pandas merge function\n",
      "  10. Fair      (0.423) - pandas concat function\n",
      "  11. Fair      (0.508) - DataFrame indexing methods\n",
      "  12. Fair      (0.598) - pandas DataFrame code examples\n",
      "  13. Poor      (0.264) - Series creation syntax\n",
      "  14. Good      (0.616) - pandas data manipulation examples\n",
      "  15. Good      (0.617) - pandas data cleaning techniques\n",
      "  16. Fair      (0.488) - handling missing data in pandas\n",
      "  17. Fair      (0.496) - pandas performance optimization\n",
      "  18. Good      (0.689) - pandas date and time handling\n",
      "\n",
      "Retrieval Quality Analysis:\n",
      "Total queries tested: 18\n",
      "Average top score: 0.509\n",
      "Median top score: 0.502\n",
      "Standard deviation: 0.100\n",
      "\n",
      "Quality Distribution:\n",
      "  Excellent:  0 queries ( 0.0%)\n",
      "  Good     :  4 queries (22.2%)\n",
      "  Fair     : 12 queries (66.7%)\n",
      "  Poor     :  2 queries (11.1%)\n",
      "\n",
      "Performance Metrics:\n",
      "  Excellent retrieval rate: 0.0%\n",
      "  Good+ retrieval rate: 22.2%\n",
      "  Average relevance score: 0.509\n",
      "\n",
      "Best Performing Queries:\n",
      "  0.689 - pandas date and time handling\n",
      "  0.617 - pandas data cleaning techniques\n",
      "  0.616 - pandas data manipulation examples\n",
      "\n",
      "Worst Performing Queries:\n",
      "  0.264 - Series creation syntax\n",
      "  0.378 - pandas groupby function\n",
      "  0.423 - pandas concat function\n",
      "\n",
      "Detailed Analysis for Sample Queries:\n",
      "\n",
      "Query: 'What is a pandas DataFrame?'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1: 0.6155 (Good)\n",
      "    Content: conceptual | Tokens: 1423 | Pages: 7\n",
      "    Code Score: 2 | Concept Score: 58\n",
      "    Preview: intuitive data manipulation and analysis. This example highlights the versatility of Pandas Data Frames, as they can seamlessly integrate diﬀerent data types and organize them into a single, cohesive structure. In the following exampl e, we will use this structur e to further explore the power of Da...\n",
      "\n",
      "  Result 2: 0.5064 (Fair)\n",
      "    Content: conceptual | Tokens: 1447 | Pages: 7\n",
      "    Code Score: 0 | Concept Score: 18\n",
      "    Preview: PREF ACE Welcome to Mastering Pandas: A Comprehensive Guide to Data Analysis in Python , a journey into the heart of modern data science. This book is not just a guide; it’s your gateway to the world of data exploration, where powerful insights lie hidden within raw numbers and text. Here, Pandas tr...\n",
      "\n",
      "  Result 3: 0.4908 (Fair)\n",
      "    Content: general | Tokens: 1149 | Pages: 6\n",
      "    Code Score: 0 | Concept Score: 21\n",
      "    Preview: This initial DataFrame provides each student’s score in various subjects. The following examples demonstrate how to use pivot() and pivot_table() to reshape and analyze the data. Example 1: Reshaping Data with pivot() The pivot() function in Pandas is a powerful tool for reorganizing data without pe...\n",
      "\n",
      "Query: 'How to create a DataFrame?'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1: 0.4764 (Fair)\n",
      "    Content: conceptual | Tokens: 1423 | Pages: 7\n",
      "    Code Score: 2 | Concept Score: 58\n",
      "    Preview: intuitive data manipulation and analysis. This example highlights the versatility of Pandas Data Frames, as they can seamlessly integrate diﬀerent data types and organize them into a single, cohesive structure. In the following exampl e, we will use this structur e to further explore the power of Da...\n",
      "\n",
      "  Result 2: 0.4478 (Fair)\n",
      "    Content: general | Tokens: 1149 | Pages: 6\n",
      "    Code Score: 0 | Concept Score: 21\n",
      "    Preview: This initial DataFrame provides each student’s score in various subjects. The following examples demonstrate how to use pivot() and pivot_table() to reshape and analyze the data. Example 1: Reshaping Data with pivot() The pivot() function in Pandas is a powerful tool for reorganizing data without pe...\n",
      "\n",
      "  Result 3: 0.3945 (Poor)\n",
      "    Content: code_heavy | Tokens: 1456 | Pages: 7\n",
      "    Code Score: 25 | Concept Score: 43\n",
      "    Preview: \"Getting started with Pandas is straightforwar d, and you can set it up easily in both your ter minal or Jupyter Notebook. First, to install Pandas, open your terminal or, if you’re using Jupyter Notebook, simply run this command in a cell: pip install pandas # for terminal !pip install pandas # for...\n",
      "\n",
      "Query: 'pandas groupby function'\n",
      "Found 3 results:\n",
      "\n",
      "  Result 1: 0.3776 (Poor)\n",
      "    Content: general | Tokens: 1120 | Pages: 6\n",
      "    Code Score: 0 | Concept Score: 29\n",
      "    Preview: Exploring Advanced Grouping T echniques: Custom Aggregations and Hierarchical Indexing in Pandas In data analysis, there’s often a need to go beyond basic aggregations like sum, mean, and count, and dive into more nuanced calculations that provide deeper insights. Pandas oﬀers powerful tools for thi...\n",
      "\n",
      "  Result 2: 0.3720 (Poor)\n",
      "    Content: general | Tokens: 1149 | Pages: 6\n",
      "    Code Score: 0 | Concept Score: 21\n",
      "    Preview: This initial DataFrame provides each student’s score in various subjects. The following examples demonstrate how to use pivot() and pivot_table() to reshape and analyze the data. Example 1: Reshaping Data with pivot() The pivot() function in Pandas is a powerful tool for reorganizing data without pe...\n",
      "\n",
      "  Result 3: 0.3586 (Poor)\n",
      "    Content: conceptual | Tokens: 1423 | Pages: 7\n",
      "    Code Score: 2 | Concept Score: 58\n",
      "    Preview: intuitive data manipulation and analysis. This example highlights the versatility of Pandas Data Frames, as they can seamlessly integrate diﬀerent data types and organize them into a single, cohesive structure. In the following exampl e, we will use this structur e to further explore the power of Da...\n",
      "\n",
      "Retrieval testing complete!\n",
      "Collection 'pandas_docs_improved' ready with 13 high-quality chunks.\n",
      "\n",
      "Evaluation results saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\retrieval_evaluation.pkl\n",
      "Ready for LLM integration testing!\n"
     ]
    }
   ],
   "source": [
    "# 03_retrieval_testing.ipynb - Vector Database and Retrieval Evaluation\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "print(\"Loading processed chunks...\")\n",
    "chunks_file = PROCESSED_DIR / 'processed_chunks.pkl'\n",
    "\n",
    "if not chunks_file.exists():\n",
    "    print(\"ERROR: Processed chunks not found. Please run 02_chunking_strategy.ipynb first.\")\n",
    "    exit()\n",
    "\n",
    "with open(chunks_file, 'rb') as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks\")\n",
    "print(f\"Average tokens per chunk: {np.mean([c['token_count'] for c in chunks]):.1f}\")\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"\\nInitializing embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"Model loaded. Embedding dimension: {embedding_dimension}\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"\\nGenerating embeddings for {len(chunks)} chunks...\")\n",
    "start_time = time.time()\n",
    "\n",
    "chunk_embeddings = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    embedding = embedding_model.encode(chunk['text'])\n",
    "    chunk_embeddings.append(embedding)\n",
    "    \n",
    "    if (i + 1) % 5 == 0 or i == len(chunks) - 1:\n",
    "        print(f\"  Generated {i + 1}/{len(chunks)} embeddings\")\n",
    "\n",
    "embedding_time = time.time() - start_time\n",
    "print(f\"Embeddings generated in {embedding_time:.2f} seconds\")\n",
    "print(f\"Average time per chunk: {embedding_time/len(chunks):.3f} seconds\")\n",
    "\n",
    "# Initialize Qdrant client\n",
    "print(f\"\\nConnecting to Qdrant...\")\n",
    "try:\n",
    "    qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "    collections = qdrant_client.get_collections()\n",
    "    print(f\"Connected to Qdrant. Existing collections: {len(collections.collections)}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to connect to Qdrant: {e}\")\n",
    "    print(\"Make sure Qdrant is running: docker run -p 6333:6333 qdrant/qdrant\")\n",
    "    exit()\n",
    "\n",
    "# Create collection\n",
    "collection_name = \"pandas_docs_improved\"\n",
    "print(f\"\\nCreating collection: {collection_name}\")\n",
    "\n",
    "try:\n",
    "    # Delete if exists\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        print(\"  Deleted existing collection\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Create new collection\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=embedding_dimension, distance=Distance.COSINE)\n",
    "    )\n",
    "    print(\"  Collection created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"  ERROR creating collection: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Prepare and insert points\n",
    "print(f\"\\nPreparing data points...\")\n",
    "points = []\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "    \n",
    "    # Create rich metadata\n",
    "    point = PointStruct(\n",
    "        id=str(uuid.uuid4()),\n",
    "        vector=embedding.tolist(),\n",
    "        payload={\n",
    "            \"text\": chunk['text'],\n",
    "            \"token_count\": chunk['token_count'],\n",
    "            \"content_type\": chunk['content_type'],\n",
    "            \"chunk_index\": chunk['chunk_index'],\n",
    "            \"global_chunk_id\": chunk['global_chunk_id'],\n",
    "            \"source_pages\": chunk['source_pages'],\n",
    "            \"page_count\": chunk['page_count'],\n",
    "            \"code_score\": chunk['features']['code_score'],\n",
    "            \"concept_score\": chunk['features']['concept_score'],\n",
    "            \"is_code_heavy\": chunk['features']['is_code_heavy'],\n",
    "            \"is_concept_heavy\": chunk['features']['is_concept_heavy'],\n",
    "            \"preview\": chunk['text'][:300] + \"...\" if len(chunk['text']) > 300 else chunk['text']\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "\n",
    "print(f\"Prepared {len(points)} points for insertion\")\n",
    "\n",
    "# Insert points\n",
    "print(f\"Inserting points into Qdrant...\")\n",
    "try:\n",
    "    result = qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        points=points\n",
    "    )\n",
    "    print(f\"  Inserted {len(points)} points successfully\")\n",
    "    print(f\"  Operation result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ERROR inserting points: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Verify insertion\n",
    "print(f\"\\nVerifying insertion...\")\n",
    "try:\n",
    "    count_result = qdrant_client.count(collection_name)\n",
    "    print(f\"  Total points in collection: {count_result.count}\")\n",
    "    \n",
    "    if count_result.count != len(chunks):\n",
    "        print(f\"  WARNING: Expected {len(chunks)} points, found {count_result.count}\")\n",
    "    else:\n",
    "        print(f\"  Verification successful\")\n",
    "except Exception as e:\n",
    "    print(f\"  ERROR during verification: {e}\")\n",
    "\n",
    "# Create comprehensive test queries\n",
    "test_queries = [\n",
    "    # Basic concepts\n",
    "    \"What is a pandas DataFrame?\",\n",
    "    \"What is a pandas Series?\", \n",
    "    \"Difference between Series and DataFrame\",\n",
    "    \n",
    "    # How-to questions\n",
    "    \"How to create a DataFrame?\",\n",
    "    \"How to read CSV files in pandas?\",\n",
    "    \"How to select data from DataFrame?\",\n",
    "    \"How to filter DataFrame rows?\",\n",
    "    \n",
    "    # Specific functions\n",
    "    \"pandas groupby function\",\n",
    "    \"pandas merge function\", \n",
    "    \"pandas concat function\",\n",
    "    \"DataFrame indexing methods\",\n",
    "    \n",
    "    # Code examples\n",
    "    \"pandas DataFrame code examples\",\n",
    "    \"Series creation syntax\",\n",
    "    \"pandas data manipulation examples\",\n",
    "    \n",
    "    # Advanced topics\n",
    "    \"pandas data cleaning techniques\",\n",
    "    \"handling missing data in pandas\",\n",
    "    \"pandas performance optimization\",\n",
    "    \"pandas date and time handling\"\n",
    "]\n",
    "\n",
    "def evaluate_retrieval(query, collection_name, top_k=3, show_results=False):\n",
    "    \"\"\"Evaluate retrieval quality for a single query\"\"\"\n",
    "    \n",
    "    # Generate query embedding\n",
    "    query_embedding = embedding_model.encode(query)\n",
    "    \n",
    "    # Search\n",
    "    search_results = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding.tolist(),\n",
    "        limit=top_k\n",
    "    )\n",
    "    \n",
    "    results = search_results.points\n",
    "    \n",
    "    if show_results:\n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Found {len(results)} results:\")\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            score = result.score\n",
    "            content_type = result.payload['content_type']\n",
    "            tokens = result.payload['token_count']\n",
    "            pages = result.payload['page_count']\n",
    "            code_score = result.payload['code_score']\n",
    "            concept_score = result.payload['concept_score']\n",
    "            \n",
    "            # Score quality indicator\n",
    "            quality = \"Excellent\" if score > 0.8 else \"Good\" if score > 0.6 else \"Fair\" if score > 0.4 else \"Poor\"\n",
    "            \n",
    "            print(f\"\\n  Result {i}: {score:.4f} ({quality})\")\n",
    "            print(f\"    Content: {content_type} | Tokens: {tokens} | Pages: {pages}\")\n",
    "            print(f\"    Code Score: {code_score} | Concept Score: {concept_score}\")\n",
    "            print(f\"    Preview: {result.payload['preview']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(f\"\\nRunning comprehensive retrieval evaluation...\")\n",
    "print(f\"Testing {len(test_queries)} queries with improved chunks:\")\n",
    "\n",
    "evaluation_results = {}\n",
    "total_scores = []\n",
    "quality_distribution = {\"Excellent\": 0, \"Good\": 0, \"Fair\": 0, \"Poor\": 0}\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    results = evaluate_retrieval(query, collection_name, top_k=3, show_results=False)\n",
    "    \n",
    "    if results:\n",
    "        top_score = results[0].score\n",
    "        avg_score = np.mean([r.score for r in results])\n",
    "        \n",
    "        # Categorize quality\n",
    "        if top_score > 0.8:\n",
    "            quality = \"Excellent\"\n",
    "        elif top_score > 0.6:\n",
    "            quality = \"Good\"\n",
    "        elif top_score > 0.4:\n",
    "            quality = \"Fair\"\n",
    "        else:\n",
    "            quality = \"Poor\"\n",
    "        \n",
    "        quality_distribution[quality] += 1\n",
    "        total_scores.append(top_score)\n",
    "        \n",
    "        evaluation_results[query] = {\n",
    "            'top_score': top_score,\n",
    "            'avg_score': avg_score,\n",
    "            'quality': quality,\n",
    "            'results_count': len(results)\n",
    "        }\n",
    "        \n",
    "        print(f\"  {i+1:2d}. {quality:9s} ({top_score:.3f}) - {query}\")\n",
    "    else:\n",
    "        print(f\"  {i+1:2d}. No Results - {query}\")\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(f\"\\nRetrieval Quality Analysis:\")\n",
    "print(f\"Total queries tested: {len(test_queries)}\")\n",
    "print(f\"Average top score: {np.mean(total_scores):.3f}\")\n",
    "print(f\"Median top score: {np.median(total_scores):.3f}\")\n",
    "print(f\"Standard deviation: {np.std(total_scores):.3f}\")\n",
    "\n",
    "print(f\"\\nQuality Distribution:\")\n",
    "for quality, count in quality_distribution.items():\n",
    "    percentage = (count / len(test_queries)) * 100\n",
    "    print(f\"  {quality:9s}: {count:2d} queries ({percentage:4.1f}%)\")\n",
    "\n",
    "# Improvement analysis (compare with baseline expectations)\n",
    "excellent_rate = quality_distribution[\"Excellent\"] / len(test_queries)\n",
    "good_plus_rate = (quality_distribution[\"Excellent\"] + quality_distribution[\"Good\"]) / len(test_queries)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Excellent retrieval rate: {excellent_rate:.1%}\")\n",
    "print(f\"  Good+ retrieval rate: {good_plus_rate:.1%}\")\n",
    "print(f\"  Average relevance score: {np.mean(total_scores):.3f}\")\n",
    "\n",
    "# Show best and worst performing queries\n",
    "if evaluation_results:\n",
    "    best_queries = sorted(evaluation_results.items(), key=lambda x: x[1]['top_score'], reverse=True)[:3]\n",
    "    worst_queries = sorted(evaluation_results.items(), key=lambda x: x[1]['top_score'])[:3]\n",
    "    \n",
    "    print(f\"\\nBest Performing Queries:\")\n",
    "    for query, results in best_queries:\n",
    "        print(f\"  {results['top_score']:.3f} - {query}\")\n",
    "    \n",
    "    print(f\"\\nWorst Performing Queries:\")\n",
    "    for query, results in worst_queries:\n",
    "        print(f\"  {results['top_score']:.3f} - {query}\")\n",
    "\n",
    "# Detailed analysis for a few sample queries\n",
    "print(f\"\\nDetailed Analysis for Sample Queries:\")\n",
    "\n",
    "sample_queries = [\n",
    "    \"What is a pandas DataFrame?\",\n",
    "    \"How to create a DataFrame?\", \n",
    "    \"pandas groupby function\"\n",
    "]\n",
    "\n",
    "for query in sample_queries:\n",
    "    results = evaluate_retrieval(query, collection_name, top_k=3, show_results=True)\n",
    "\n",
    "print(f\"\\nRetrieval testing complete!\")\n",
    "print(f\"Collection '{collection_name}' ready with {len(chunks)} high-quality chunks.\")\n",
    "\n",
    "# Save evaluation results\n",
    "eval_file = PROCESSED_DIR / 'retrieval_evaluation.pkl'\n",
    "with open(eval_file, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'test_queries': test_queries,\n",
    "        'evaluation_results': evaluation_results,\n",
    "        'quality_distribution': quality_distribution,\n",
    "        'metrics': {\n",
    "            'avg_score': np.mean(total_scores),\n",
    "            'median_score': np.median(total_scores),\n",
    "            'std_score': np.std(total_scores),\n",
    "            'excellent_rate': excellent_rate,\n",
    "            'good_plus_rate': good_plus_rate\n",
    "        }\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nEvaluation results saved to: {eval_file}\")\n",
    "print(f\"Ready for LLM integration testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e0a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
