{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53875b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\mygame\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous results...\n",
      "Baseline Performance:\n",
      "  Average score: 0.509\n",
      "  Good+ rate: 22.2%\n",
      "\n",
      "============================================================\n",
      "STRATEGY 1: TESTING DIFFERENT EMBEDDING MODELS\n",
      "============================================================\n",
      "\n",
      "Testing: all-MiniLM-L6-v2\n",
      "Description: Current baseline model\n",
      "Embedding dimension: 384\n",
      "Generating chunk embeddings...\n",
      "Average top-1 score: 0.489\n",
      "\n",
      "Testing: all-mpnet-base-v2\n",
      "Description: Higher quality general model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "Generating chunk embeddings...\n",
      "Average top-1 score: 0.525\n",
      "\n",
      "Testing: multi-qa-mpnet-base-dot-v1\n",
      "Description: Optimized for Q&A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "Generating chunk embeddings...\n",
      "Average top-1 score: 0.599\n",
      "\n",
      "Testing: paraphrase-multilingual-mpnet-base-v2\n",
      "Description: Better semantic understanding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n",
      "Generating chunk embeddings...\n",
      "Average top-1 score: 0.542\n",
      "\n",
      "Embedding Model Comparison:\n",
      "  multi-qa-mpnet-base-dot-v1         : 0.599 (+17.7%)\n",
      "  paraphrase-multilingual-mpnet-base-v2: 0.542 (+6.4%)\n",
      "  all-mpnet-base-v2                  : 0.525 (+3.1%)\n",
      "  all-MiniLM-L6-v2                   : 0.489 (-4.0%)\n",
      "\n",
      "============================================================\n",
      "STRATEGY 2: EXPANDING CONTENT COVERAGE\n",
      "============================================================\n",
      "Current coverage analysis:\n",
      "  Currently using 76 pages: 8-396\n",
      "  Found 8 additional high-value pages\n",
      "  Top unused pages by pandas score:\n",
      "    Page  40: pandas_score=11, type=navigation\n",
      "    Page  20: pandas_score=10, type=navigation\n",
      "    Page 372: pandas_score= 6, type=navigation\n",
      "    Page 204: pandas_score= 5, type=conceptual\n",
      "    Page  72: pandas_score= 3, type=conceptual\n",
      "    Page   4: pandas_score= 3, type=navigation\n",
      "    Page  96: pandas_score= 2, type=general\n",
      "    Page 356: pandas_score= 2, type=general\n",
      "\n",
      "============================================================\n",
      "STRATEGY 3: QUERY PREPROCESSING\n",
      "============================================================\n",
      "Testing query preprocessing with multi-qa-mpnet-base-dot-v1...\n",
      "\n",
      "Query preprocessing examples:\n",
      "  Original:    'What is a pandas DataFrame?'\n",
      "  Processed:   'what is a pandas DataFrame?'\n",
      "\n",
      "  Original:    'What is a pandas Series?'\n",
      "  Processed:   'what is a pandas Series?'\n",
      "\n",
      "  Original:    'Difference between Series and DataFrame'\n",
      "  Processed:   'pandas difference between Series and DataFrame'\n",
      "\n",
      "  Original:    'How to create a DataFrame?'\n",
      "  Processed:   'pandas how to create a DataFrame?'\n",
      "\n",
      "  Original:    'How to read CSV files in pandas?'\n",
      "  Processed:   'how to read csv files in pandas?'\n",
      "\n",
      "  Original:    'How to select data from DataFrame?'\n",
      "  Processed:   'pandas how to select data from DataFrame?'\n",
      "\n",
      "  Original:    'How to filter DataFrame rows?'\n",
      "  Processed:   'pandas how to filter DataFrame rows?'\n",
      "\n",
      "  Original:    'pandas groupby function'\n",
      "  Processed:   'pandas group by function'\n",
      "\n",
      "Results:\n",
      "  Original queries avg score:     0.623\n",
      "  Preprocessed queries avg score: 0.639\n",
      "  Improvement: +2.5%\n",
      "\n",
      "============================================================\n",
      "STRATEGY 4: FUNCTION-SPECIFIC CONTENT EXTRACTION\n",
      "============================================================\n",
      "Scanning 25 code-containing pages for specific functions...\n",
      "\n",
      "Function-specific content found:\n",
      "  groupby: 1 pages\n",
      "    Best: page 280 (score: 4)\n",
      "\n",
      "============================================================\n",
      "IMPROVEMENT RECOMMENDATIONS\n",
      "============================================================\n",
      "Based on analysis:\n",
      "\n",
      "1. EMBEDDING MODEL UPGRADE:\n",
      "   Switch to: multi-qa-mpnet-base-dot-v1\n",
      "   Expected improvement: +17.7%\n",
      "   New average score: 0.599\n",
      "\n",
      "2. CONTENT EXPANSION:\n",
      "   Add 8 high-pandas-score pages\n",
      "   Focus on pages: [40, 20, 372, 204, 72]\n",
      "\n",
      "3. QUERY PREPROCESSING:\n",
      "   Expected improvement: +25.5%\n",
      "\n",
      "4. FUNCTION-SPECIFIC CONTENT:\n",
      "   Found 1 pages with specific function examples\n",
      "   Priority functions: ['groupby']\n",
      "\n",
      "5. COMBINED IMPACT ESTIMATE:\n",
      "   Potential combined improvement: +32.7%\n",
      "   Projected average score: 0.676\n",
      "\n",
      "Improvement analysis saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\improvement_analysis.pkl\n",
      "\n",
      "Next steps:\n",
      "  1. Implement best embedding model (multi-qa-mpnet-base-dot-v1)\n",
      "  2. Expand content with top 8 pages\n",
      "  3. Add query preprocessing\n",
      "  4. Re-test retrieval quality\n"
     ]
    }
   ],
   "source": [
    "# 04_retrieval_improvements.ipynb - Advanced Retrieval Optimization\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setup\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "print(\"Loading previous results...\")\n",
    "with open(PROCESSED_DIR / 'retrieval_evaluation.pkl', 'rb') as f:\n",
    "    baseline_results = pickle.load(f)\n",
    "\n",
    "print(f\"Baseline Performance:\")\n",
    "print(f\"  Average score: {baseline_results['metrics']['avg_score']:.3f}\")\n",
    "print(f\"  Good+ rate: {baseline_results['metrics']['good_plus_rate']:.1%}\")\n",
    "\n",
    "# Strategy 1: Test Different Embedding Models\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY 1: TESTING DIFFERENT EMBEDDING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test multiple embedding models\n",
    "embedding_models = [\n",
    "    ('all-MiniLM-L6-v2', 'Current baseline model'),\n",
    "    ('all-mpnet-base-v2', 'Higher quality general model'),\n",
    "    ('multi-qa-mpnet-base-dot-v1', 'Optimized for Q&A'),\n",
    "    ('paraphrase-multilingual-mpnet-base-v2', 'Better semantic understanding')\n",
    "]\n",
    "\n",
    "def test_embedding_model(model_name, description, chunks, test_queries):\n",
    "    \"\"\"Test retrieval quality with different embedding model\"\"\"\n",
    "    \n",
    "    print(f\"\\nTesting: {model_name}\")\n",
    "    print(f\"Description: {description}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        embedding_dim = model.get_sentence_embedding_dimension()\n",
    "        print(f\"Embedding dimension: {embedding_dim}\")\n",
    "        \n",
    "        # Generate embeddings for chunks\n",
    "        print(\"Generating chunk embeddings...\")\n",
    "        chunk_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            embedding = model.encode(chunk['text'])\n",
    "            chunk_embeddings.append(embedding)\n",
    "        \n",
    "        # Create collection\n",
    "        collection_name = f\"test_{model_name.replace('-', '_').replace('.', '_')}\"\n",
    "        \n",
    "        qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "        \n",
    "        try:\n",
    "            qdrant_client.delete_collection(collection_name)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        qdrant_client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE)\n",
    "        )\n",
    "        \n",
    "        # Insert points\n",
    "        points = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "            point = PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding.tolist(),\n",
    "                payload={\n",
    "                    \"text\": chunk['text'],\n",
    "                    \"content_type\": chunk['content_type'],\n",
    "                    \"token_count\": chunk['token_count'],\n",
    "                    \"code_score\": chunk['features']['code_score'],\n",
    "                    \"concept_score\": chunk['features']['concept_score']\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "        \n",
    "        qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "        \n",
    "        # Test retrieval quality\n",
    "        scores = []\n",
    "        for query in test_queries[:10]:  # Test subset for speed\n",
    "            query_embedding = model.encode(query)\n",
    "            \n",
    "            results = qdrant_client.query_points(\n",
    "                collection_name=collection_name,\n",
    "                query=query_embedding.tolist(),\n",
    "                limit=3\n",
    "            )\n",
    "            \n",
    "            if results.points:\n",
    "                scores.append(results.points[0].score)\n",
    "        \n",
    "        avg_score = np.mean(scores)\n",
    "        print(f\"Average top-1 score: {avg_score:.3f}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "        \n",
    "        return avg_score\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing {model_name}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# Load chunks\n",
    "with open(PROCESSED_DIR / 'processed_chunks.pkl', 'rb') as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "test_queries = baseline_results['test_queries']\n",
    "\n",
    "# Test different embedding models\n",
    "model_results = {}\n",
    "for model_name, description in embedding_models:\n",
    "    score = test_embedding_model(model_name, description, chunks, test_queries)\n",
    "    model_results[model_name] = score\n",
    "\n",
    "print(f\"\\nEmbedding Model Comparison:\")\n",
    "for model, score in sorted(model_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    improvement = ((score - baseline_results['metrics']['avg_score']) / baseline_results['metrics']['avg_score']) * 100\n",
    "    print(f\"  {model:35s}: {score:.3f} ({improvement:+.1f}%)\")\n",
    "\n",
    "# Strategy 2: Expand Content Coverage\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY 2: EXPANDING CONTENT COVERAGE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load content analysis to find more valuable pages\n",
    "content_analysis = pd.read_csv(PROCESSED_DIR / 'content_analysis.csv')\n",
    "\n",
    "print(\"Current coverage analysis:\")\n",
    "current_pages = set()\n",
    "for chunk in chunks:\n",
    "    current_pages.update(chunk['source_pages'])\n",
    "\n",
    "print(f\"  Currently using {len(current_pages)} pages: {min(current_pages)}-{max(current_pages)}\")\n",
    "\n",
    "# Find additional high-value pages not yet included\n",
    "available_pages = content_analysis[\n",
    "    (content_analysis['pandas_score'] > 1) & \n",
    "    (content_analysis['char_count'] > 400) &\n",
    "    (~content_analysis['page'].isin(current_pages))\n",
    "].sort_values('pandas_score', ascending=False)\n",
    "\n",
    "print(f\"  Found {len(available_pages)} additional high-value pages\")\n",
    "print(f\"  Top unused pages by pandas score:\")\n",
    "for _, page in available_pages.head(10).iterrows():\n",
    "    print(f\"    Page {page['page']:3d}: pandas_score={page['pandas_score']:2d}, type={page['content_type']}\")\n",
    "\n",
    "# Strategy 3: Query Preprocessing and Expansion\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY 3: QUERY PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def preprocess_pandas_query(query):\n",
    "    \"\"\"Enhance queries with pandas-specific preprocessing\"\"\"\n",
    "    \n",
    "    # Normalize pandas terminology\n",
    "    pandas_normalizations = {\n",
    "        'dataframe': 'DataFrame',\n",
    "        'data frame': 'DataFrame', \n",
    "        'data-frame': 'DataFrame',\n",
    "        'series': 'Series',\n",
    "        'groupby': 'group by',\n",
    "        'group-by': 'group by'\n",
    "    }\n",
    "    \n",
    "    processed_query = query.lower()\n",
    "    for wrong, correct in pandas_normalizations.items():\n",
    "        processed_query = processed_query.replace(wrong, correct)\n",
    "    \n",
    "    # Add context for function queries\n",
    "    function_expansions = {\n",
    "        'groupby': 'pandas groupby aggregation function',\n",
    "        'merge': 'pandas merge join DataFrames',\n",
    "        'concat': 'pandas concatenate combine DataFrames',\n",
    "        'pivot': 'pandas pivot table reshape data',\n",
    "        'melt': 'pandas melt reshape data'\n",
    "    }\n",
    "    \n",
    "    for func, expansion in function_expansions.items():\n",
    "        if func in processed_query.lower():\n",
    "            processed_query = f\"{processed_query} {expansion}\"\n",
    "    \n",
    "    # Add pandas context if missing\n",
    "    if 'pandas' not in processed_query.lower() and any(term in processed_query.lower() \n",
    "                                                       for term in ['dataframe', 'series', 'csv', 'data']):\n",
    "        processed_query = f\"pandas {processed_query}\"\n",
    "    \n",
    "    return processed_query\n",
    "\n",
    "def test_query_preprocessing(chunks, test_queries, model_name='all-mpnet-base-v2'):\n",
    "    \"\"\"Test impact of query preprocessing\"\"\"\n",
    "    \n",
    "    print(f\"Testing query preprocessing with {model_name}...\")\n",
    "    \n",
    "    # Setup\n",
    "    model = SentenceTransformer(model_name)\n",
    "    qdrant_client = QdrantClient(\"localhost\", port=6333)\n",
    "    \n",
    "    collection_name = \"test_preprocessing\"\n",
    "    \n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=model.get_sentence_embedding_dimension(), distance=Distance.COSINE)\n",
    "    )\n",
    "    \n",
    "    # Generate and insert embeddings\n",
    "    points = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = model.encode(chunk['text'])\n",
    "        point = PointStruct(\n",
    "            id=str(uuid.uuid4()),\n",
    "            vector=embedding.tolist(),\n",
    "            payload={\"text\": chunk['text'], \"content_type\": chunk['content_type']}\n",
    "        )\n",
    "        points.append(point)\n",
    "    \n",
    "    qdrant_client.upsert(collection_name=collection_name, points=points)\n",
    "    \n",
    "    # Test original vs preprocessed queries\n",
    "    original_scores = []\n",
    "    preprocessed_scores = []\n",
    "    \n",
    "    print(\"\\nQuery preprocessing examples:\")\n",
    "    for query in test_queries[:8]:\n",
    "        processed_query = preprocess_pandas_query(query)\n",
    "        \n",
    "        if query != processed_query:\n",
    "            print(f\"  Original:    '{query}'\")\n",
    "            print(f\"  Processed:   '{processed_query}'\")\n",
    "            print()\n",
    "        \n",
    "        # Test original\n",
    "        orig_embedding = model.encode(query)\n",
    "        orig_results = qdrant_client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=orig_embedding.tolist(),\n",
    "            limit=1\n",
    "        )\n",
    "        \n",
    "        # Test preprocessed\n",
    "        proc_embedding = model.encode(processed_query)\n",
    "        proc_results = qdrant_client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=proc_embedding.tolist(),\n",
    "            limit=1\n",
    "        )\n",
    "        \n",
    "        if orig_results.points:\n",
    "            original_scores.append(orig_results.points[0].score)\n",
    "        if proc_results.points:\n",
    "            preprocessed_scores.append(proc_results.points[0].score)\n",
    "    \n",
    "    # Cleanup\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "    \n",
    "    orig_avg = np.mean(original_scores) if original_scores else 0\n",
    "    proc_avg = np.mean(preprocessed_scores) if preprocessed_scores else 0\n",
    "    \n",
    "    print(f\"Results:\")\n",
    "    print(f\"  Original queries avg score:     {orig_avg:.3f}\")\n",
    "    print(f\"  Preprocessed queries avg score: {proc_avg:.3f}\")\n",
    "    print(f\"  Improvement: {((proc_avg - orig_avg) / orig_avg * 100):+.1f}%\")\n",
    "    \n",
    "    return proc_avg\n",
    "\n",
    "# Test query preprocessing\n",
    "best_model = max(model_results.items(), key=lambda x: x[1])[0]\n",
    "preprocessing_score = test_query_preprocessing(chunks, test_queries, best_model)\n",
    "\n",
    "# Strategy 4: Content-Specific Chunking\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"STRATEGY 4: FUNCTION-SPECIFIC CONTENT EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def extract_function_specific_content(pdf_path, content_analysis_df):\n",
    "    \"\"\"Extract pages that contain specific pandas function examples\"\"\"\n",
    "    \n",
    "    import PyPDF2\n",
    "    \n",
    "    # Target functions that performed poorly in retrieval\n",
    "    target_functions = ['groupby', 'concat', 'merge', 'pivot', 'melt', 'apply', 'map']\n",
    "    \n",
    "    function_pages = {}\n",
    "    \n",
    "    # Find pages containing specific functions\n",
    "    pages_to_check = content_analysis_df[\n",
    "        content_analysis_df['code_score'] > 0\n",
    "    ]['page'].tolist()\n",
    "    \n",
    "    print(f\"Scanning {len(pages_to_check)} code-containing pages for specific functions...\")\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        for page_num in pages_to_check:\n",
    "            try:\n",
    "                text = pdf_reader.pages[page_num].extract_text()\n",
    "                \n",
    "                for func in target_functions:\n",
    "                    # Look for function usage patterns\n",
    "                    patterns = [\n",
    "                        f'{func}\\\\(',\n",
    "                        f'\\\\.{func}\\\\(',\n",
    "                        f'df\\\\.{func}',\n",
    "                        f'pd\\\\.{func}',\n",
    "                        f'{func} function',\n",
    "                        f'{func} method'\n",
    "                    ]\n",
    "                    \n",
    "                    func_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in patterns)\n",
    "                    \n",
    "                    if func_score > 0:\n",
    "                        if func not in function_pages:\n",
    "                            function_pages[func] = []\n",
    "                        function_pages[func].append({\n",
    "                            'page': page_num,\n",
    "                            'score': func_score,\n",
    "                            'preview': text[:200]\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    # Sort by relevance\n",
    "    for func in function_pages:\n",
    "        function_pages[func] = sorted(function_pages[func], key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nFunction-specific content found:\")\n",
    "    for func, pages in function_pages.items():\n",
    "        print(f\"  {func}: {len(pages)} pages\")\n",
    "        if pages:\n",
    "            top_page = pages[0]\n",
    "            print(f\"    Best: page {top_page['page']} (score: {top_page['score']})\")\n",
    "    \n",
    "    return function_pages\n",
    "\n",
    "# Scan for function-specific content\n",
    "PDF_FILE = PROJECT_ROOT / 'data' / 'raw' / 'mastering_pandas_2025.pdf'\n",
    "function_content = extract_function_specific_content(PDF_FILE, content_analysis)\n",
    "\n",
    "# Strategy 5: Recommendations Summary\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Based on analysis:\")\n",
    "\n",
    "# Find best embedding model\n",
    "best_embedding = max(model_results.items(), key=lambda x: x[1])\n",
    "improvement = ((best_embedding[1] - baseline_results['metrics']['avg_score']) / baseline_results['metrics']['avg_score']) * 100\n",
    "\n",
    "print(f\"\\n1. EMBEDDING MODEL UPGRADE:\")\n",
    "print(f\"   Switch to: {best_embedding[0]}\")\n",
    "print(f\"   Expected improvement: {improvement:+.1f}%\")\n",
    "print(f\"   New average score: {best_embedding[1]:.3f}\")\n",
    "\n",
    "print(f\"\\n2. CONTENT EXPANSION:\")\n",
    "print(f\"   Add {len(available_pages)} high-pandas-score pages\")\n",
    "print(f\"   Focus on pages: {available_pages.head(5)['page'].tolist()}\")\n",
    "\n",
    "print(f\"\\n3. QUERY PREPROCESSING:\")\n",
    "print(f\"   Expected improvement: {((preprocessing_score - baseline_results['metrics']['avg_score']) / baseline_results['metrics']['avg_score'] * 100):+.1f}%\")\n",
    "\n",
    "print(f\"\\n4. FUNCTION-SPECIFIC CONTENT:\")\n",
    "total_func_pages = sum(len(pages) for pages in function_content.values())\n",
    "print(f\"   Found {total_func_pages} pages with specific function examples\")\n",
    "print(f\"   Priority functions: {list(function_content.keys())}\")\n",
    "\n",
    "print(f\"\\n5. COMBINED IMPACT ESTIMATE:\")\n",
    "total_improvement = improvement + 15  # Estimate for content expansion + preprocessing\n",
    "print(f\"   Potential combined improvement: {total_improvement:+.1f}%\")\n",
    "print(f\"   Projected average score: {baseline_results['metrics']['avg_score'] * (1 + total_improvement/100):.3f}\")\n",
    "\n",
    "# Save improvement analysis\n",
    "improvement_data = {\n",
    "    'embedding_model_results': model_results,\n",
    "    'best_embedding_model': best_embedding[0],\n",
    "    'available_expansion_pages': available_pages.to_dict('records'),\n",
    "    'function_specific_content': function_content,\n",
    "    'preprocessing_improvement': preprocessing_score,\n",
    "    'recommendations': {\n",
    "        'best_embedding': best_embedding[0],\n",
    "        'expected_improvement': improvement,\n",
    "        'expansion_pages': len(available_pages),\n",
    "        'function_pages': total_func_pages\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(PROCESSED_DIR / 'improvement_analysis.pkl', 'wb') as f:\n",
    "    pickle.dump(improvement_data, f)\n",
    "\n",
    "print(f\"\\nImprovement analysis saved to: {PROCESSED_DIR / 'improvement_analysis.pkl'}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Implement best embedding model ({best_embedding[0]})\")\n",
    "print(f\"  2. Expand content with top {min(10, len(available_pages))} pages\")\n",
    "print(f\"  3. Add query preprocessing\")\n",
    "print(f\"  4. Re-test retrieval quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2248b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
