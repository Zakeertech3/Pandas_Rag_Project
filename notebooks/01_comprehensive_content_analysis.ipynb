{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1940a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e67554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Structure Verification:\n",
      "Project Root: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\n",
      "Raw Data Path: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\raw\n",
      "Processed Data Path: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\n",
      "PDF File Path: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\raw\\mastering_pandas_2025.pdf\n",
      "PDF File Exists: True\n",
      "File Size: 29.36 MB\n"
     ]
    }
   ],
   "source": [
    "# Path Setup and Verification\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
    "RAW_DATA_PATH = PROJECT_ROOT / 'data' / 'raw'\n",
    "PROCESSED_DATA_PATH = PROJECT_ROOT / 'data' / 'processed'\n",
    "PDF_FILE = RAW_DATA_PATH / 'mastering_pandas_2025.pdf'\n",
    "\n",
    "# Create processed directory if it doesn't exist\n",
    "PROCESSED_DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Project Structure Verification:\")\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Raw Data Path: {RAW_DATA_PATH}\")\n",
    "print(f\"Processed Data Path: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"PDF File Path: {PDF_FILE}\")\n",
    "print(f\"PDF File Exists: {PDF_FILE.exists()}\")\n",
    "\n",
    "if PDF_FILE.exists():\n",
    "    file_size = PDF_FILE.stat().st_size\n",
    "    print(f\"File Size: {file_size / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(\"ERROR: PDF file not found!\")\n",
    "    print(\"Please ensure 'mastering_pandas_2025.pdf' is in the data/raw/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90214e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing PDF structure...\n",
      "\n",
      "PDF Analysis Results:\n",
      "Total Pages: 473\n",
      "\n",
      "Document Metadata:\n",
      "  /Author: Yildiz, Muslum\n",
      "  /CreationDate: D:20250520051054+00'00'\n",
      "  /Creator: calibre 7.16.0\n",
      "  /ModDate: D:20250520051054+00'00'\n",
      "  /Producer: calibre 7.16.0\n",
      "  /Title: MASTERING PANDAS: A Comprehensive Guide to Data Analysis in Python\n",
      "\n",
      "Testing text extraction on first 3 pages...\n",
      "  Page 0: 0 characters, 0 words\n",
      "  Page 1: 121 characters, 22 words\n",
      "  Page 2: 452 characters, 83 words\n"
     ]
    }
   ],
   "source": [
    "# Basic PDF Structure Analysis\n",
    "\n",
    "print(\"Analyzing PDF structure...\")\n",
    "\n",
    "try:\n",
    "    with open(PDF_FILE, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        total_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        print(f\"\\nPDF Analysis Results:\")\n",
    "        print(f\"Total Pages: {total_pages}\")\n",
    "        \n",
    "        # Check metadata\n",
    "        if pdf_reader.metadata:\n",
    "            print(f\"\\nDocument Metadata:\")\n",
    "            for key, value in pdf_reader.metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Quick sample of first few pages to verify reading\n",
    "        print(f\"\\nTesting text extraction on first 3 pages...\")\n",
    "        for i in range(min(3, total_pages)):\n",
    "            try:\n",
    "                text = pdf_reader.pages[i].extract_text()\n",
    "                char_count = len(text.strip())\n",
    "                word_count = len(text.split()) if text.strip() else 0\n",
    "                print(f\"  Page {i}: {char_count} characters, {word_count} words\")\n",
    "                \n",
    "                if i == 0 and text.strip():\n",
    "                    print(f\"  Sample text: {text[:100].replace('\\n', ' ')}...\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  Page {i}: Error extracting text - {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error opening PDF: {e}\")\n",
    "    print(\"Please check if the PDF file is not corrupted and accessible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61a98fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content classification functions defined successfully\n",
      "Ready to analyze all pages for comprehensive content extraction\n"
     ]
    }
   ],
   "source": [
    "# Content Classification Functions\n",
    "\n",
    "def analyze_content_characteristics(text):\n",
    "    \"\"\"\n",
    "    Comprehensive content analysis for tiered classification\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            'char_count': 0,\n",
    "            'word_count': 0,\n",
    "            'line_count': 0,\n",
    "            'content_tier': 'empty',\n",
    "            'content_type': 'empty',\n",
    "            'pandas_score': 0,\n",
    "            'code_score': 0,\n",
    "            'structural_score': 0,\n",
    "            'quiz_potential': 0\n",
    "        }\n",
    "    \n",
    "    text = text.strip()\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    line_count = len(text.split('\\n'))\n",
    "    \n",
    "    # Content type detection patterns\n",
    "    navigation_patterns = [\n",
    "        r'table\\s+of\\s+contents', r'chapter\\s+\\d+', r'page\\s+\\d+',\n",
    "        r'section\\s+\\d+', r'appendix', r'index', r'bibliography'\n",
    "    ]\n",
    "    \n",
    "    code_patterns = [\n",
    "        r'import\\s+\\w+', r'from\\s+\\w+\\s+import', r'pd\\.', r'df\\.',\n",
    "        r'print\\s*\\(', r'=\\s*pd\\.', r'\\.groupby\\(', r'\\.merge\\(',\n",
    "        r'\\.iloc\\[', r'\\.loc\\[', r'def\\s+\\w+', r'class\\s+\\w+',\n",
    "        r'>>>', r'```', r'#.*', r'pandas\\.'\n",
    "    ]\n",
    "    \n",
    "    pandas_concepts = [\n",
    "        r'dataframe', r'series', r'index', r'groupby', r'merge', r'concat',\n",
    "        r'pivot', r'melt', r'apply', r'lambda', r'iloc', r'loc', r'query',\n",
    "        r'fillna', r'dropna', r'read_csv', r'to_csv', r'head\\(', r'tail\\(',\n",
    "        r'describe\\(', r'info\\(', r'value_counts', r'unique\\('\n",
    "    ]\n",
    "    \n",
    "    structural_patterns = [\n",
    "        r'^[A-Z][^.!?]*:?\\s*$',  # Headers\n",
    "        r'example\\s*\\d*[:\\-]?', r'note[:\\-]?', r'important[:\\-]?',\n",
    "        r'figure\\s+\\d+', r'table\\s+\\d+', r'listing\\s+\\d+',\n",
    "        r'step\\s+\\d+', r'method\\s+\\d+', r'approach\\s+\\d+'\n",
    "    ]\n",
    "    \n",
    "    quiz_indicators = [\n",
    "        r'example', r'exercise', r'practice', r'try', r'test',\n",
    "        r'solution', r'answer', r'result', r'output', r'returns?',\n",
    "        r'what\\s+is', r'how\\s+to', r'why', r'when', r'where'\n",
    "    ]\n",
    "    \n",
    "    # Score calculations\n",
    "    navigation_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in navigation_patterns)\n",
    "    code_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in code_patterns)\n",
    "    pandas_score = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in pandas_concepts)\n",
    "    structural_score = sum(len(re.findall(pattern, text, re.MULTILINE | re.IGNORECASE)) for pattern in structural_patterns)\n",
    "    quiz_potential = sum(len(re.findall(pattern, text, re.IGNORECASE)) for pattern in quiz_indicators)\n",
    "    \n",
    "    # Content type classification\n",
    "    if char_count < 100:\n",
    "        content_type = 'minimal'\n",
    "    elif navigation_score > 2:\n",
    "        content_type = 'navigation'\n",
    "    elif code_score > 5:\n",
    "        content_type = 'code_heavy'\n",
    "    elif pandas_score > 8:\n",
    "        content_type = 'conceptual'\n",
    "    elif structural_score > 3:\n",
    "        content_type = 'structural'\n",
    "    else:\n",
    "        content_type = 'general'\n",
    "    \n",
    "    # Content tier assignment (for 100% utilization strategy)\n",
    "    if pandas_score > 5 and char_count > 500:\n",
    "        content_tier = 'tier_1_primary'      # High-value content\n",
    "    elif (pandas_score > 2 or code_score > 3) and char_count > 300:\n",
    "        content_tier = 'tier_2_secondary'    # Supporting content\n",
    "    elif content_type in ['navigation', 'structural'] and char_count > 200:\n",
    "        content_tier = 'tier_3_reference'    # Reference content\n",
    "    elif char_count > 100:\n",
    "        content_tier = 'tier_4_context'      # Background context\n",
    "    else:\n",
    "        content_tier = 'tier_5_minimal'      # Minimal content\n",
    "    \n",
    "    return {\n",
    "        'char_count': char_count,\n",
    "        'word_count': word_count,\n",
    "        'line_count': line_count,\n",
    "        'content_tier': content_tier,\n",
    "        'content_type': content_type,\n",
    "        'pandas_score': pandas_score,\n",
    "        'code_score': code_score,\n",
    "        'structural_score': structural_score,\n",
    "        'quiz_potential': quiz_potential,\n",
    "        'navigation_score': navigation_score\n",
    "    }\n",
    "\n",
    "def detect_quiz_opportunities(text):\n",
    "    \"\"\"\n",
    "    Identify content suitable for quiz generation\n",
    "    \"\"\"\n",
    "    quiz_types = {\n",
    "        'multiple_choice': 0,\n",
    "        'code_completion': 0,\n",
    "        'true_false': 0,\n",
    "        'fill_blank': 0,\n",
    "        'scenario': 0\n",
    "    }\n",
    "    \n",
    "    # Multiple choice indicators\n",
    "    if re.search(r'(what\\s+is|which\\s+of|select\\s+the|choose\\s+the)', text, re.IGNORECASE):\n",
    "        quiz_types['multiple_choice'] += 2\n",
    "    \n",
    "    # Code completion opportunities\n",
    "    if re.search(r'(example|code|syntax|function)', text, re.IGNORECASE) and re.search(r'(pd\\.|df\\.|\\(|\\))', text):\n",
    "        quiz_types['code_completion'] += 3\n",
    "    \n",
    "    # True/false opportunities\n",
    "    if re.search(r'(always|never|only|must|cannot|should)', text, re.IGNORECASE):\n",
    "        quiz_types['true_false'] += 1\n",
    "    \n",
    "    # Fill in the blank\n",
    "    if re.search(r'(parameter|argument|method|attribute)', text, re.IGNORECASE):\n",
    "        quiz_types['fill_blank'] += 1\n",
    "    \n",
    "    # Scenario-based\n",
    "    if re.search(r'(dataset|data|analysis|problem|task)', text, re.IGNORECASE):\n",
    "        quiz_types['scenario'] += 1\n",
    "    \n",
    "    return quiz_types\n",
    "\n",
    "print(\"Content classification functions defined successfully\")\n",
    "print(\"Ready to analyze all pages for comprehensive content extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "125023d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive analysis of ALL pages...\n",
      "Processing 473 pages...\n",
      "  Processed 50/473 pages\n",
      "  Processed 100/473 pages\n",
      "  Processed 150/473 pages\n",
      "  Processed 200/473 pages\n",
      "  Processed 250/473 pages\n",
      "  Processed 300/473 pages\n",
      "  Processed 350/473 pages\n",
      "  Processed 400/473 pages\n",
      "  Processed 450/473 pages\n",
      "  Processed 473/473 pages\n",
      "\n",
      "Analysis Complete!\n",
      "Successfully processed: 473 pages\n",
      "Error pages: 0\n",
      "\n",
      "Processing completed in 9.24 seconds\n",
      "Average time per page: 0.020 seconds\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Page Analysis - Processing ALL Pages\n",
    "\n",
    "def analyze_complete_document(pdf_path):\n",
    "    \"\"\"\n",
    "    Analyze every single page of the PDF for 100% content utilization\n",
    "    \"\"\"\n",
    "    print(\"Starting comprehensive analysis of ALL pages...\")\n",
    "    \n",
    "    page_analysis = []\n",
    "    error_pages = []\n",
    "    \n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        total_pages = len(pdf_reader.pages)\n",
    "        \n",
    "        print(f\"Processing {total_pages} pages...\")\n",
    "        \n",
    "        for page_num in range(total_pages):\n",
    "            try:\n",
    "                # Extract text\n",
    "                text = pdf_reader.pages[page_num].extract_text()\n",
    "                \n",
    "                # Analyze content characteristics\n",
    "                analysis = analyze_content_characteristics(text)\n",
    "                \n",
    "                # Detect quiz opportunities\n",
    "                quiz_types = detect_quiz_opportunities(text)\n",
    "                \n",
    "                # Create text preview\n",
    "                clean_text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "                text_preview = clean_text[:200] if clean_text else \"\"\n",
    "                \n",
    "                # Combine all analysis\n",
    "                page_data = {\n",
    "                    'page_number': page_num,\n",
    "                    'text_preview': text_preview,\n",
    "                    'full_text_length': len(text),\n",
    "                    **analysis,\n",
    "                    **{f'quiz_{k}': v for k, v in quiz_types.items()}\n",
    "                }\n",
    "                \n",
    "                page_analysis.append(page_data)\n",
    "                \n",
    "                # Progress indicator\n",
    "                if (page_num + 1) % 50 == 0 or page_num == total_pages - 1:\n",
    "                    print(f\"  Processed {page_num + 1}/{total_pages} pages\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                error_pages.append({'page_number': page_num, 'error': str(e)})\n",
    "                print(f\"  Error on page {page_num}: {e}\")\n",
    "    \n",
    "    print(f\"\\nAnalysis Complete!\")\n",
    "    print(f\"Successfully processed: {len(page_analysis)} pages\")\n",
    "    print(f\"Error pages: {len(error_pages)}\")\n",
    "    \n",
    "    return page_analysis, error_pages\n",
    "\n",
    "# Run the comprehensive analysis\n",
    "start_time = time.time()\n",
    "all_pages_analysis, error_pages = analyze_complete_document(PDF_FILE)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nProcessing completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Average time per page: {(end_time - start_time) / len(all_pages_analysis):.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "366aff08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE CONTENT ANALYSIS RESULTS\n",
      "============================================================\n",
      "Total pages analyzed: 473\n",
      "Pages with errors: 0\n",
      "\n",
      "CONTENT TIER DISTRIBUTION (100% Utilization Strategy):\n",
      "  Empty:   2 pages (  0.4%)\n",
      "  Tier 1 Primary:  79 pages ( 16.7%)\n",
      "  Tier 2 Secondary: 105 pages ( 22.2%)\n",
      "  Tier 3 Reference: 232 pages ( 49.0%)\n",
      "  Tier 4 Context:  55 pages ( 11.6%)\n",
      "\n",
      "CONTENT TYPE DISTRIBUTION:\n",
      "  Structural: 335 pages ( 70.8%)\n",
      "  General:  71 pages ( 15.0%)\n",
      "  Navigation:  39 pages (  8.2%)\n",
      "  Conceptual:  20 pages (  4.2%)\n",
      "  Code_Heavy:   6 pages (  1.3%)\n",
      "  Empty:   2 pages (  0.4%)\n",
      "\n",
      "CONTENT QUALITY METRICS:\n",
      "  Pages with substantial content (>500 chars): 369\n",
      "  Pages with high pandas content (score >5): 85\n",
      "  Pages with code examples (score >3): 19\n",
      "  Pages with quiz potential (score >2): 133\n",
      "\n",
      "STATISTICAL SUMMARY:\n",
      "  Average characters per page: 787\n",
      "  Average words per page: 125\n",
      "  Average pandas score: 2.8\n",
      "  Average code score: 0.5\n",
      "\n",
      "TOP 10 PAGES BY PANDAS CONTENT:\n",
      "  Page  84: Score 22 | tier_1_primary | navigation\n",
      "  Page  55: Score 21 | tier_1_primary | conceptual\n",
      "  Page  67: Score 20 | tier_1_primary | navigation\n",
      "  Page  61: Score 15 | tier_1_primary | navigation\n",
      "  Page  80: Score 15 | tier_1_primary | conceptual\n",
      "  Page  51: Score 14 | tier_1_primary | navigation\n",
      "  Page  74: Score 14 | tier_1_primary | conceptual\n",
      "  Page 102: Score 14 | tier_1_primary | code_heavy\n",
      "  Page  45: Score 13 | tier_1_primary | navigation\n",
      "  Page  54: Score 13 | tier_1_primary | conceptual\n",
      "\n",
      "TOP 10 PAGES BY CODE CONTENT:\n",
      "  Page 102: Score 16 | tier_1_primary | code_heavy\n",
      "  Page 105: Score 12 | tier_1_primary | code_heavy\n",
      "  Page 104: Score 11 | tier_1_primary | code_heavy\n",
      "  Page 115: Score  7 | tier_2_secondary | code_heavy\n",
      "  Page  36: Score  6 | tier_2_secondary | code_heavy\n",
      "  Page  45: Score  6 | tier_1_primary | navigation\n",
      "  Page  46: Score  6 | tier_1_primary | navigation\n",
      "  Page  48: Score  6 | tier_1_primary | code_heavy\n",
      "  Page  19: Score  5 | tier_1_primary | navigation\n",
      "  Page  43: Score  5 | tier_1_primary | conceptual\n",
      "\n",
      "TOP 10 PAGES BY QUIZ POTENTIAL:\n",
      "  Page 105: Score  8 | tier_1_primary | code_heavy\n",
      "  Page 136: Score  8 | tier_2_secondary | structural\n",
      "  Page 152: Score  8 | tier_3_reference | structural\n",
      "  Page 153: Score  8 | tier_3_reference | structural\n",
      "  Page 104: Score  7 | tier_1_primary | code_heavy\n",
      "  Page 269: Score  7 | tier_2_secondary | structural\n",
      "  Page  66: Score  6 | tier_1_primary | conceptual\n",
      "  Page 103: Score  6 | tier_3_reference | structural\n",
      "  Page 107: Score  6 | tier_3_reference | structural\n",
      "  Page 222: Score  6 | tier_2_secondary | structural\n",
      "\n",
      "100% CONTENT UTILIZATION STRATEGY:\n",
      "  Tier 1 (Primary): 79 pages - High-value pandas content\n",
      "  Tier 2 (Secondary): 105 pages - Supporting concepts and code\n",
      "  Tier 3 (Reference): 232 pages - Navigation and structure\n",
      "  Tier 4 (Context): 55 pages - Background information\n",
      "  Tier 5 (Minimal): 0 pages - Minimal content\n",
      "  Total: 471 pages\n",
      "\n",
      "Utilizable content: 473/473 pages (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Results Analysis and Content Distribution\n",
    "\n",
    "# Create comprehensive DataFrame\n",
    "content_df = pd.DataFrame(all_pages_analysis)\n",
    "\n",
    "print(\"COMPREHENSIVE CONTENT ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total pages analyzed: {len(content_df)}\")\n",
    "print(f\"Pages with errors: {len(error_pages)}\")\n",
    "\n",
    "# Content tier distribution\n",
    "print(f\"\\nCONTENT TIER DISTRIBUTION (100% Utilization Strategy):\")\n",
    "tier_distribution = content_df['content_tier'].value_counts().sort_index()\n",
    "for tier, count in tier_distribution.items():\n",
    "    percentage = (count / len(content_df)) * 100\n",
    "    print(f\"  {tier.replace('_', ' ').title()}: {count:3d} pages ({percentage:5.1f}%)\")\n",
    "\n",
    "# Content type distribution\n",
    "print(f\"\\nCONTENT TYPE DISTRIBUTION:\")\n",
    "type_distribution = content_df['content_type'].value_counts()\n",
    "for content_type, count in type_distribution.items():\n",
    "    percentage = (count / len(content_df)) * 100\n",
    "    print(f\"  {content_type.title()}: {count:3d} pages ({percentage:5.1f}%)\")\n",
    "\n",
    "# Quality metrics\n",
    "print(f\"\\nCONTENT QUALITY METRICS:\")\n",
    "print(f\"  Pages with substantial content (>500 chars): {(content_df['char_count'] > 500).sum()}\")\n",
    "print(f\"  Pages with high pandas content (score >5): {(content_df['pandas_score'] > 5).sum()}\")\n",
    "print(f\"  Pages with code examples (score >3): {(content_df['code_score'] > 3).sum()}\")\n",
    "print(f\"  Pages with quiz potential (score >2): {(content_df['quiz_potential'] > 2).sum()}\")\n",
    "\n",
    "# Statistical summary\n",
    "print(f\"\\nSTATISTICAL SUMMARY:\")\n",
    "print(f\"  Average characters per page: {content_df['char_count'].mean():.0f}\")\n",
    "print(f\"  Average words per page: {content_df['word_count'].mean():.0f}\")\n",
    "print(f\"  Average pandas score: {content_df['pandas_score'].mean():.1f}\")\n",
    "print(f\"  Average code score: {content_df['code_score'].mean():.1f}\")\n",
    "\n",
    "# Top content by different criteria\n",
    "print(f\"\\nTOP 10 PAGES BY PANDAS CONTENT:\")\n",
    "top_pandas = content_df.nlargest(10, 'pandas_score')[['page_number', 'pandas_score', 'content_tier', 'content_type']]\n",
    "for _, row in top_pandas.iterrows():\n",
    "    print(f\"  Page {row['page_number']:3d}: Score {row['pandas_score']:2d} | {row['content_tier']} | {row['content_type']}\")\n",
    "\n",
    "print(f\"\\nTOP 10 PAGES BY CODE CONTENT:\")\n",
    "top_code = content_df.nlargest(10, 'code_score')[['page_number', 'code_score', 'content_tier', 'content_type']]\n",
    "for _, row in top_code.iterrows():\n",
    "    print(f\"  Page {row['page_number']:3d}: Score {row['code_score']:2d} | {row['content_tier']} | {row['content_type']}\")\n",
    "\n",
    "print(f\"\\nTOP 10 PAGES BY QUIZ POTENTIAL:\")\n",
    "top_quiz = content_df.nlargest(10, 'quiz_potential')[['page_number', 'quiz_potential', 'content_tier', 'content_type']]\n",
    "for _, row in top_quiz.iterrows():\n",
    "    print(f\"  Page {row['page_number']:3d}: Score {row['quiz_potential']:2d} | {row['content_tier']} | {row['content_type']}\")\n",
    "\n",
    "# Content utilization summary\n",
    "tier_1_pages = len(content_df[content_df['content_tier'] == 'tier_1_primary'])\n",
    "tier_2_pages = len(content_df[content_df['content_tier'] == 'tier_2_secondary'])\n",
    "tier_3_pages = len(content_df[content_df['content_tier'] == 'tier_3_reference'])\n",
    "tier_4_pages = len(content_df[content_df['content_tier'] == 'tier_4_context'])\n",
    "tier_5_pages = len(content_df[content_df['content_tier'] == 'tier_5_minimal'])\n",
    "\n",
    "print(f\"\\n100% CONTENT UTILIZATION STRATEGY:\")\n",
    "print(f\"  Tier 1 (Primary): {tier_1_pages} pages - High-value pandas content\")\n",
    "print(f\"  Tier 2 (Secondary): {tier_2_pages} pages - Supporting concepts and code\")\n",
    "print(f\"  Tier 3 (Reference): {tier_3_pages} pages - Navigation and structure\")\n",
    "print(f\"  Tier 4 (Context): {tier_4_pages} pages - Background information\")\n",
    "print(f\"  Tier 5 (Minimal): {tier_5_pages} pages - Minimal content\")\n",
    "print(f\"  Total: {tier_1_pages + tier_2_pages + tier_3_pages + tier_4_pages + tier_5_pages} pages\")\n",
    "\n",
    "utilizable_pages = len(content_df[content_df['content_tier'] != 'tier_5_minimal'])\n",
    "print(f\"\\nUtilizable content: {utilizable_pages}/{len(content_df)} pages ({utilizable_pages/len(content_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17021411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive content analysis saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\comprehensive_content_analysis.csv\n",
      "Analysis summary saved to: c:\\Users\\MOHAMMED ZAKEER\\Downloads\\pandas_rag_project\\data\\processed\\analysis_summary.json\n",
      "\n",
      "COMPREHENSIVE ANALYSIS COMPLETE!\n",
      "==================================================\n",
      "Results saved and ready for next stage processing\n",
      "\n",
      "Saved files verification:\n",
      "  Content analysis: True (127.1 KB)\n",
      "  Summary stats: True (0.9 KB)\n"
     ]
    }
   ],
   "source": [
    "# Save Comprehensive Analysis Results\n",
    "\n",
    "# Save the complete content analysis\n",
    "content_analysis_file = PROCESSED_DATA_PATH / 'comprehensive_content_analysis.csv'\n",
    "content_df.to_csv(content_analysis_file, index=False)\n",
    "print(f\"Comprehensive content analysis saved to: {content_analysis_file}\")\n",
    "\n",
    "# Save error log if any errors occurred\n",
    "if error_pages:\n",
    "    error_df = pd.DataFrame(error_pages)\n",
    "    error_file = PROCESSED_DATA_PATH / 'extraction_errors.csv'\n",
    "    error_df.to_csv(error_file, index=False)\n",
    "    print(f\"Error log saved to: {error_file}\")\n",
    "\n",
    "# Create summary statistics for next notebooks\n",
    "summary_stats = {\n",
    "    'total_pages': len(content_df),\n",
    "    'error_pages': len(error_pages),\n",
    "    'tier_distribution': content_df['content_tier'].value_counts().to_dict(),\n",
    "    'type_distribution': content_df['content_type'].value_counts().to_dict(),\n",
    "    'quality_metrics': {\n",
    "        'substantial_content_pages': int((content_df['char_count'] > 500).sum()),\n",
    "        'high_pandas_pages': int((content_df['pandas_score'] > 5).sum()),\n",
    "        'code_heavy_pages': int((content_df['code_score'] > 3).sum()),\n",
    "        'quiz_potential_pages': int((content_df['quiz_potential'] > 2).sum())\n",
    "    },\n",
    "    'statistical_summary': {\n",
    "        'avg_chars_per_page': float(content_df['char_count'].mean()),\n",
    "        'avg_words_per_page': float(content_df['word_count'].mean()),\n",
    "        'avg_pandas_score': float(content_df['pandas_score'].mean()),\n",
    "        'avg_code_score': float(content_df['code_score'].mean())\n",
    "    },\n",
    "    'utilization_strategy': {\n",
    "        'tier_1_primary': int(len(content_df[content_df['content_tier'] == 'tier_1_primary'])),\n",
    "        'tier_2_secondary': int(len(content_df[content_df['content_tier'] == 'tier_2_secondary'])),\n",
    "        'tier_3_reference': int(len(content_df[content_df['content_tier'] == 'tier_3_reference'])),\n",
    "        'tier_4_context': int(len(content_df[content_df['content_tier'] == 'tier_4_context'])),\n",
    "        'tier_5_minimal': int(len(content_df[content_df['content_tier'] == 'tier_5_minimal']))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary statistics\n",
    "summary_file = PROCESSED_DATA_PATH / 'analysis_summary.json'\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "print(f\"Analysis summary saved to: {summary_file}\")\n",
    "\n",
    "# Display final summary\n",
    "print(f\"\\nCOMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Results saved and ready for next stage processing\")\n",
    "\n",
    "\n",
    "# Quick verification of saved data\n",
    "print(f\"\\nSaved files verification:\")\n",
    "print(f\"  Content analysis: {content_analysis_file.exists()} ({content_analysis_file.stat().st_size / 1024:.1f} KB)\")\n",
    "print(f\"  Summary stats: {summary_file.exists()} ({summary_file.stat().st_size / 1024:.1f} KB)\")\n",
    "if error_pages:\n",
    "    print(f\"  Error log: {error_file.exists()} ({error_file.stat().st_size / 1024:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mygame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
